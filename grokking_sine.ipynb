{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBTfiQFVs0GI"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Grokking_Demo.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B58KqSBps0GK"
      },
      "source": [
        "# Grokking Demo Notebook\n",
        "\n",
        "<b style=\"color: red\">To use this notebook, go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wU3VZxxs0GK"
      },
      "source": [
        "# Setup\n",
        "(No need to read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYEoHvphs0GK"
      },
      "outputs": [],
      "source": [
        "TRAIN_MODEL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVqsonBas0GL"
      },
      "outputs": [],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "import os\n",
        "\n",
        "DEVELOPMENT_MODE = True\n",
        "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
        "    # # Install another version of node that makes PySvelte work way faster\n",
        "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")\n",
        "\n",
        "if IN_COLAB or IN_GITHUB:\n",
        "    %pip install transformer_lens\n",
        "    %pip install circuitsvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5Zu_bOxs0GL"
      },
      "outputs": [],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCQeYT1Ys0GM"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95rrdZOxs0GM"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import os\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaWkHpV8s0GM"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms-9EWghs0GM"
      },
      "source": [
        "Plotting helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHddh5X0s0GM"
      },
      "outputs": [],
      "source": [
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBdM6Xa6s0GN"
      },
      "outputs": [],
      "source": [
        "# Define the location to save the model, using a relative path\n",
        "PTH_LOCATION = \"workspace/_scratch/grokking_demo.pth\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FbCNqRNs0GN"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr1ajHuWs0GN"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRhNvT8vs0GN"
      },
      "outputs": [],
      "source": [
        "frac_train = 0.72\n",
        "\n",
        "# Optimizer config\n",
        "lr = 1e-4\n",
        "wd = 0.1\n",
        "betas = (0.9, 0.98)\n",
        "\n",
        "num_epochs = 10000\n",
        "checkpoint_every = 100\n",
        "\n",
        "data_seed = 598"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVV5pB7Es0GN"
      },
      "source": [
        "## Define Task\n",
        "We want the model to learn to compute $sin(k x)$ given $k$. Hence, the input is $k$ and the desired output is a discretized version of $sin(k x)$ with $x$ ranging from $[0, 2\\pi]$ with 100 data points.\n",
        "\n",
        "We use 1000 samples for $k$, drawn uniformly at random from the $[0.1, 10.]$ interval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7RcAWSfs0GN"
      },
      "outputs": [],
      "source": [
        "num_samples = 1000\n",
        "num_x_points = 100\n",
        "k_min = 0.1\n",
        "k_max = 10.0\n",
        "\n",
        "# Create the k values that will be the input to the model\n",
        "k_vector = (k_max - k_min) * torch.rand(num_samples, generator=torch.manual_seed(data_seed)) + k_min\n",
        "dataset = k_vector.unsqueeze(1).to(device) # Shape: [num_samples, 1]\n",
        "\n",
        "# Create the corresponding sin(kx) curves which are the labels\n",
        "x_points = torch.linspace(0, 2 * torch.pi, num_x_points, device=device) # Shape: [num_x_points]\n",
        "# Use broadcasting to compute all sine curves at once\n",
        "# (num_samples, 1) * (num_x_points,) -> (num_samples, num_x_points)\n",
        "labels = torch.sin(dataset * x_points)\n",
        "\n",
        "# --- Split into Training and Test Sets ---\n",
        "torch.manual_seed(data_seed)\n",
        "indices = torch.randperm(num_samples)\n",
        "cutoff = int(num_samples * frac_train)\n",
        "train_indices = indices[:cutoff]\n",
        "test_indices = indices[cutoff:]\n",
        "\n",
        "train_data = dataset[train_indices]\n",
        "train_labels = labels[train_indices]\n",
        "test_data = dataset[test_indices]\n",
        "test_labels = labels[test_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIztxc8Is0GO"
      },
      "source": [
        "## Define Model\n",
        "\n",
        "The standard hooked transformer config from TransformerLens is set up for discrete tokens. We are however considering continuous inputs and outputs in this regression task. Hence, in order to keep all the useful features of HookedTransformer, we will make a new class that inherits from it and adjust the input and output layers to allow for continuous inputs and outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWa9jdIps0GO"
      },
      "outputs": [],
      "source": [
        "class SineTransformer(HookedTransformer):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__(cfg)\n",
        "\n",
        "        # The parent __init__ created self.embed and self.unembed, which are for\n",
        "        # discrete tokens. We don't need them. It's good practice to delete them\n",
        "        # to avoid confusion.\n",
        "        del self.embed\n",
        "        del self.unembed\n",
        "\n",
        "        # --- Create New Layers for our Regression Task ---\n",
        "\n",
        "        # 1. An input layer to project our continuous k value into d_model\n",
        "        self.k_input_projection = nn.Linear(1, self.cfg.d_model)\n",
        "\n",
        "        # 2. An output layer to project from d_model back to a single float value\n",
        "        # at each position in the output sequence.\n",
        "        self.output_projection = nn.Linear(self.cfg.d_model, 1)\n",
        "\n",
        "    def forward(self, k_values):\n",
        "        # k_values shape: [batch_size, 1]\n",
        "\n",
        "        # 1. Project k into the model's dimension using our new layer\n",
        "        k_embedding = self.k_input_projection(k_values) # [batch_size, d_model]\n",
        "\n",
        "        # 2. Get the positional embeddings for our 100 x-points\n",
        "        # The 'n_ctx' in our config now represents the number of output points.\n",
        "        # W_pos is the learned positional embedding matrix of shape [n_ctx, d_model]\n",
        "        positional_embeddings = self.pos_embed.W_pos[:self.cfg.n_ctx, :] # [n_ctx, d_model]\n",
        "\n",
        "        residual_stream = k_embedding.unsqueeze(1) + positional_embeddings\n",
        "\n",
        "        # --- THE FIX IS HERE ---\n",
        "        # Instead of calling self.blocks directly, we loop through the modules inside it.\n",
        "        for block in self.blocks:\n",
        "            residual_stream = block(residual_stream)\n",
        "\n",
        "        # Now we use the final state of the residual stream\n",
        "        output_values = self.output_projection(residual_stream)\n",
        "\n",
        "        return output_values.squeeze(-1)\n",
        "\n",
        "# This is the updated configuration for the Sine task\n",
        "cfg = HookedTransformerConfig(\n",
        "    n_layers=1,\n",
        "    n_heads=4,\n",
        "    d_model=128,\n",
        "    d_head=32,\n",
        "    d_mlp=512,\n",
        "    act_fn=\"relu\",\n",
        "    normalization_type=\"LN\", # LayerNorm is generally helpful. You can set to None.\n",
        "    n_ctx=num_x_points,\n",
        "    # d_vocab and d_vocab_out are no longer used by our new model,\n",
        "    # but the config object requires them. We can set them to a dummy value.\n",
        "    d_vocab=10,\n",
        "    d_vocab_out=10,\n",
        "    init_weights=True,\n",
        "    device=device,\n",
        "    seed=999,\n",
        ")\n",
        "\n",
        "# Instantiate our NEW model class with the updated config\n",
        "model = SineTransformer(cfg).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpV0nvhAs0GO"
      },
      "source": [
        "Disable the biases, as we don't need them for this task and it makes things easier to interpret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2PLgqTzs0GO"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if \"b_\" in name:\n",
        "        param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9xFcKkJs0GO"
      },
      "source": [
        "## Define Optimizer + Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i4SmHnLs0GO"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1iogkzys0GO"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "train_logits = model(train_data)\n",
        "train_loss = loss_fn(train_logits, train_labels)\n",
        "print(f\"Initial Training Loss: {train_loss.item()}\")\n",
        "\n",
        "test_logits = model(test_data)\n",
        "test_loss = loss_fn(test_logits, test_labels)\n",
        "print(f\"Initial Test Loss: {test_loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbGSVZFMs0GP"
      },
      "source": [
        "## Actually Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlgKog0-s0GP"
      },
      "source": [
        "**Weird Decision:** Training the model with full batch training rather than stochastic gradient descent. We do this so to make training smoother and reduce the number of slingshots."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# --- New, more powerful optimizer and scheduler config ---\n",
        "lr = 1e-3           # Start high again\n",
        "wd = 0.01           # A much more reasonable weight decay\n",
        "betas = (0.9, 0.98)\n",
        "\n",
        "# Re-initialize the optimizer with the new parameters\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)\n",
        "\n",
        "# Initialize the scheduler\n",
        "# T_max is the total number of epochs you plan to run this new training session for.\n",
        "# It will decay the LR from 1e-3 down to 0 over this period.\n",
        "new_num_epochs = 20000\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=new_num_epochs)\n",
        "\n",
        "# --- New Training Loop (in a new cell) ---\n",
        "# We can start new lists for this run to see the effect clearly\n",
        "new_train_losses = []\n",
        "new_test_losses = []\n",
        "\n",
        "for epoch in tqdm.tqdm(range(new_num_epochs)):\n",
        "    train_logits = model(train_data)\n",
        "    train_loss = loss_fn(train_logits, train_labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Crucially, step the scheduler after each epoch\n",
        "    scheduler.step()\n",
        "\n",
        "    new_train_losses.append(train_loss.item())\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        test_logits = model(test_data)\n",
        "        test_loss = loss_fn(test_logits, test_labels)\n",
        "        new_test_losses.append(test_loss.item())\n",
        "\n",
        "    if ((epoch+1)%checkpoint_every)==0:\n",
        "        # You can get the current learning rate from the scheduler to see it decrease\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        print(f\"Epoch {epoch} | Test Loss {test_loss.item():.6f} | LR {current_lr:.6f}\")"
      ],
      "metadata": {
        "id": "JVwAegH56Vdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYn9svOss0GP"
      },
      "outputs": [],
      "source": [
        "torch.save(\n",
        "    {\n",
        "        \"model\":model.state_dict(),\n",
        "        \"config\": model.cfg,\n",
        "        \"checkpoints\": model_checkpoints,\n",
        "        \"checkpoint_epochs\": checkpoint_epochs,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"train_indices\": train_indices,\n",
        "        \"test_indices\": test_indices,\n",
        "    },\n",
        "    PTH_LOCATION)\n",
        "\n",
        "if not TRAIN_MODEL:\n",
        "    cached_data = torch.load(PTH_LOCATION)\n",
        "    model.load_state_dict(cached_data['model'])\n",
        "    model_checkpoints = cached_data[\"checkpoints\"]\n",
        "    checkpoint_epochs = cached_data[\"checkpoint_epochs\"]\n",
        "    test_losses = cached_data['test_losses']\n",
        "    train_losses = cached_data['train_losses']\n",
        "    train_indices = cached_data[\"train_indices\"]\n",
        "    test_indices = cached_data[\"test_indices\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V77TDhwos0GP"
      },
      "source": [
        "## Show Model Training Statistics, Check that it groks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhaZN_rns0GP"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
        "from neel_plotly.plot import line"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "line([train_losses[::100], test_losses[::100]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Training Curve for Fibonacci\", line_labels=['train', 'test'], toggle_x=True, toggle_y=True)"
      ],
      "metadata": {
        "id": "y_jB5NKa1FIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l7ceOyOs0GP"
      },
      "source": [
        "# Analysing the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get key weight matrices:"
      ],
      "metadata": {
        "id": "mXH1_cYfFsTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume 'model' is your trained instance\n",
        "# and 'device' is set correctly\n",
        "\n",
        "# Choose some k values to inspect\n",
        "k_to_inspect = torch.tensor([[1.0], [5.0], [10.0]], device=device)\n",
        "\n",
        "# 1. Compute the k-embedding\n",
        "k_embedding = model.k_input_projection(k_to_inspect)\n",
        "print(\"Shape of k_embedding for 3 sample k's:\", k_embedding.shape)\n",
        "\n",
        "# 2. Get the positional embedding\n",
        "positional_embedding = model.pos_embed.W_pos[:num_x_points, :]\n",
        "print(\"\\nShape of positional_embedding:\", positional_embedding.shape) # Expected: [100, 128]\n",
        "\n",
        "# 3. Compute the full initial residual stream\n",
        "k_broadcast = k_embedding.unsqueeze(1).expand(-1, num_x_points, -1)\n",
        "\n",
        "# Now the addition will work because PyTorch can broadcast [100, 128] to match [3, 100, 128]\n",
        "initial_residual_stream = k_broadcast + positional_embedding\n",
        "\n",
        "print(\"\\nShape of the full initial embedding:\", initial_residual_stream.shape)\n",
        "print(\"This is the 'effective embedding' that the first transformer block sees.\")\n"
      ],
      "metadata": {
        "id": "NKyFWQJmFr2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# --- 1. Generate a set of k values for analysis ---\n",
        "k_for_pca = torch.linspace(k_min, k_max, 1000, device=device).unsqueeze(-1)\n",
        "\n",
        "# --- 2. Get the embeddings for these k values ---\n",
        "model.eval() # Put the model in evaluation mode\n",
        "with torch.no_grad(): # We don't need to compute gradients\n",
        "    k_embeddings = model.k_input_projection(k_for_pca)\n",
        "\n",
        "# Move embeddings to CPU and convert to numpy for sklearn\n",
        "k_embeddings_np = k_embeddings.cpu().numpy()\n",
        "\n",
        "# --- 3. Perform PCA ---\n",
        "pca = PCA() # By default, finds all components\n",
        "pca.fit(k_embeddings_np)\n",
        "\n",
        "# --- 4. Plot the explained variance ---\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance)\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.title(\"Scree Plot\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.title(\"Cumulative Variance Plot\")\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.axhline(y=0.99, color='r', linestyle=':', label='99% Variance')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Interpretation ---\n",
        "num_components_for_99_variance = np.argmax(cumulative_variance >= 0.99) + 1\n",
        "print(f\"Number of components to explain 99% of variance: {num_components_for_99_variance}\")\n"
      ],
      "metadata": {
        "id": "13gxX4tjKLGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of the variance is explained by a single component of the 128-dimensional vector. We now investigate what this component looks like."
      ],
      "metadata": {
        "id": "m-oqYtnsKrvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Project the embeddings onto the first principal component ---\n",
        "# The pca.transform() method projects the data onto the components.\n",
        "# We only want the first column, which corresponds to PC1.\n",
        "projected_data_pc1 = pca.transform(k_embeddings_np)[:, 0]\n",
        "\n",
        "# --- 2. Prepare the original k values for plotting ---\n",
        "# We need them as a flat numpy array on the CPU.\n",
        "k_values_for_plot = k_for_pca.cpu().numpy().flatten()\n",
        "\n",
        "# --- 3. Create the plot ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(k_values_for_plot, projected_data_pc1)\n",
        "plt.xlabel(\"Input k value\")\n",
        "plt.ylabel(\"Projection onto Principal Component 1\")\n",
        "plt.title(\"Learned Representation of k (Projected onto PC1)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2GTpoE19KLb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we want to look at the positional embedding: this gives us an idea of what the model thinks 'space' looks like. We do PCA again:"
      ],
      "metadata": {
        "id": "x0Wo_yLqfRue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- 1. Get the positional embedding matrix ---\n",
        "# W_pos is the parameter we want to analyze.\n",
        "# It has shape [num_x_points, d_model], e.g., [100, 128].\n",
        "w_pos = model.pos_embed.W_pos.detach().cpu().numpy()\n",
        "\n",
        "# --- 2. Perform and Plot PCA ---\n",
        "pca = PCA()\n",
        "pca.fit(w_pos)\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(range(1, 11), explained_variance[:10]) # Plot first 10 components\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.title(\"PCA of Positional Embeddings (W_pos)\")\n",
        "plt.show()\n",
        "\n",
        "# --- 3. Compute and Plot the Cosine Similarity Matrix ---\n",
        "# This shows the similarity between the vector for position i and position j\n",
        "w_pos_tensor = torch.from_numpy(w_pos)\n",
        "cosine_sim = F.cosine_similarity(w_pos_tensor.unsqueeze(1), w_pos_tensor.unsqueeze(0), dim=-1)\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.imshow(cosine_sim.numpy(), cmap='viridis')\n",
        "plt.colorbar(label=\"Cosine Similarity\")\n",
        "plt.xlabel(\"Position j\")\n",
        "plt.ylabel(\"Position i\")\n",
        "plt.title(\"Cosine Similarity of Positional Embeddings\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zLoj7tVpK0t-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8859a5491331dba93123a91c2831400aced845b502848170e05fcb48b2c144be"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}