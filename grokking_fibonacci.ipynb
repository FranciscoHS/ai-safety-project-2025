{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBTfiQFVs0GI"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Grokking_Demo.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B58KqSBps0GK"
      },
      "source": [
        "# Grokking Demo Notebook\n",
        "\n",
        "<b style=\"color: red\">To use this notebook, go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wU3VZxxs0GK"
      },
      "source": [
        "# Setup\n",
        "(No need to read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYEoHvphs0GK"
      },
      "outputs": [],
      "source": [
        "TRAIN_MODEL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVqsonBas0GL"
      },
      "outputs": [],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "import os\n",
        "\n",
        "DEVELOPMENT_MODE = True\n",
        "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
        "    # # Install another version of node that makes PySvelte work way faster\n",
        "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")\n",
        "\n",
        "if IN_COLAB or IN_GITHUB:\n",
        "    %pip install transformer_lens\n",
        "    %pip install circuitsvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5Zu_bOxs0GL"
      },
      "outputs": [],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCQeYT1Ys0GM"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95rrdZOxs0GM"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import os\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaWkHpV8s0GM"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms-9EWghs0GM"
      },
      "source": [
        "Plotting helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHddh5X0s0GM"
      },
      "outputs": [],
      "source": [
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBdM6Xa6s0GN"
      },
      "outputs": [],
      "source": [
        "# Define the location to save the model, using a relative path\n",
        "PTH_LOCATION = \"workspace/_scratch/grokking_demo.pth\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FbCNqRNs0GN"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr1ajHuWs0GN"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRhNvT8vs0GN"
      },
      "outputs": [],
      "source": [
        "p = 10\n",
        "frac_train = 0.72\n",
        "\n",
        "# Optimizer config\n",
        "lr = 1e-3\n",
        "wd = 1.\n",
        "betas = (0.9, 0.98)\n",
        "\n",
        "num_epochs = 10000\n",
        "checkpoint_every = 100\n",
        "\n",
        "DATA_SEED = 598"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVV5pB7Es0GN"
      },
      "source": [
        "## Define Task\n",
        "* Define generalized Fibonacci\n",
        "* Define the dataset & labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0CzC5Rfs0GN"
      },
      "source": [
        "Input format:\n",
        "|a|b|=|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7RcAWSfs0GN"
      },
      "outputs": [],
      "source": [
        "a_vector = einops.repeat(torch.arange(p), \"i -> (i j)\", j=p)\n",
        "b_vector = einops.repeat(torch.arange(p), \"j -> (i j)\", i=p)\n",
        "equals_vector = einops.repeat(torch.tensor(p), \" -> (i j)\", i=p, j=p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8usqEh7Ss0GN"
      },
      "outputs": [],
      "source": [
        "dataset = torch.stack([a_vector, b_vector, equals_vector], dim=1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spCsQ8_ks0GO"
      },
      "outputs": [],
      "source": [
        "labels = (dataset[:, 0] + dataset[:, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KawgPijs0GO"
      },
      "source": [
        "Convert this to a train + test set - 30% in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9oFB_m3s0GO"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(DATA_SEED)\n",
        "indices = torch.randperm(p*p)\n",
        "cutoff = int(p*p*frac_train)\n",
        "train_indices = indices[:cutoff]\n",
        "test_indices = indices[cutoff:]\n",
        "\n",
        "train_data = dataset[train_indices]\n",
        "train_labels = labels[train_indices]\n",
        "test_data = dataset[test_indices]\n",
        "test_labels = labels[test_indices]\n",
        "print(train_data[:5])\n",
        "print(train_labels[:5])\n",
        "print(train_data.shape)\n",
        "print(test_data[:5])\n",
        "print(test_labels[:5])\n",
        "print(test_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIztxc8Is0GO"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWa9jdIps0GO"
      },
      "outputs": [],
      "source": [
        "\n",
        "cfg = HookedTransformerConfig(\n",
        "    n_layers = 1,\n",
        "    n_heads = 4,\n",
        "    d_model = 128,\n",
        "    d_head = 32,\n",
        "    d_mlp = 512,\n",
        "    act_fn = \"relu\",\n",
        "    normalization_type=None,\n",
        "    d_vocab=p+1,\n",
        "    d_vocab_out=2*p,\n",
        "    n_ctx=3,\n",
        "    init_weights=True,\n",
        "    device=device,\n",
        "    seed = 999,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxdHppAIs0GO"
      },
      "outputs": [],
      "source": [
        "model = HookedTransformer(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpV0nvhAs0GO"
      },
      "source": [
        "Disable the biases, as we don't need them for this task and it makes things easier to interpret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2PLgqTzs0GO"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if \"b_\" in name:\n",
        "        param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9xFcKkJs0GO"
      },
      "source": [
        "## Define Optimizer + Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i4SmHnLs0GO"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1iogkzys0GO"
      },
      "outputs": [],
      "source": [
        "def loss_fn(logits, labels):\n",
        "    if len(logits.shape)==3:\n",
        "        logits = logits[:, -1]\n",
        "    logits = logits.to(torch.float64)\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
        "    return -correct_log_probs.mean()\n",
        "train_logits = model(train_data)\n",
        "train_loss = loss_fn(train_logits, train_labels)\n",
        "print(train_loss)\n",
        "test_logits = model(test_data)\n",
        "test_loss = loss_fn(test_logits, test_labels)\n",
        "print(test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhyxMx3os0GP"
      },
      "outputs": [],
      "source": [
        "print(\"Uniform loss:\")\n",
        "print(np.log(2*p))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbGSVZFMs0GP"
      },
      "source": [
        "## Actually Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlgKog0-s0GP"
      },
      "source": [
        "**Weird Decision:** Training the model with full batch training rather than stochastic gradient descent. We do this so to make training smoother and reduce the number of slingshots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ5bPzCis0GP"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "model_checkpoints = []\n",
        "checkpoint_epochs = []\n",
        "if TRAIN_MODEL:\n",
        "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "        train_logits = model(train_data)\n",
        "        train_loss = loss_fn(train_logits, train_labels)\n",
        "        train_loss.backward()\n",
        "        train_losses.append(train_loss.item())\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            test_logits = model(test_data)\n",
        "            test_loss = loss_fn(test_logits, test_labels)\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "        if ((epoch+1)%checkpoint_every)==0:\n",
        "            checkpoint_epochs.append(epoch)\n",
        "            model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
        "            print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcttAzjes0GP"
      },
      "outputs": [],
      "source": [
        "torch.save(\n",
        "    {\n",
        "        \"model\":model.state_dict(),\n",
        "        \"config\": model.cfg,\n",
        "        \"checkpoints\": model_checkpoints,\n",
        "        \"checkpoint_epochs\": checkpoint_epochs,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"train_indices\": train_indices,\n",
        "        \"test_indices\": test_indices,\n",
        "    },\n",
        "    PTH_LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYn9svOss0GP"
      },
      "outputs": [],
      "source": [
        "if not TRAIN_MODEL:\n",
        "    cached_data = torch.load(PTH_LOCATION)\n",
        "    model.load_state_dict(cached_data['model'])\n",
        "    model_checkpoints = cached_data[\"checkpoints\"]\n",
        "    checkpoint_epochs = cached_data[\"checkpoint_epochs\"]\n",
        "    test_losses = cached_data['test_losses']\n",
        "    train_losses = cached_data['train_losses']\n",
        "    train_indices = cached_data[\"train_indices\"]\n",
        "    test_indices = cached_data[\"test_indices\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V77TDhwos0GP"
      },
      "source": [
        "## Show Model Training Statistics, Check that it groks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhaZN_rns0GP"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
        "from neel_plotly.plot import line"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "line([train_losses[::100], test_losses[::100]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Training Curve for Fibonacci\", line_labels=['train', 'test'], toggle_x=True, toggle_y=True)"
      ],
      "metadata": {
        "id": "y_jB5NKa1FIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l7ceOyOs0GP"
      },
      "source": [
        "# Analysing the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYEowab5s0GP"
      },
      "source": [
        "## Standard Things to Try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBB-TNUJs0GQ"
      },
      "outputs": [],
      "source": [
        "original_logits, cache = model.run_with_cache(dataset)\n",
        "print(original_logits.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAO0GYFrs0GQ"
      },
      "source": [
        "Get key weight matrices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsN6BuXVs0GQ"
      },
      "outputs": [],
      "source": [
        "W_E = model.embed.W_E[:-1]\n",
        "print(\"W_E\", W_E.shape)\n",
        "W_neur = W_E @ model.blocks[0].attn.W_V @ model.blocks[0].attn.W_O @ model.blocks[0].mlp.W_in\n",
        "print(\"W_neur\", W_neur.shape)\n",
        "W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
        "print(\"W_logit\", W_logit.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-GmiCJjs0GQ"
      },
      "outputs": [],
      "source": [
        "original_loss = loss_fn(original_logits, labels).item()\n",
        "print(\"Original Loss:\", original_loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_number = 7\n",
        "embedding_vector_np = W_E[input_number].detach().cpu().numpy() # this gets us the embedded vector for input_number\n",
        "\n",
        "# first we try to plot directly the value of the entries of the embedded vector\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 6)) # Adjust figure size as needed\n",
        "plt.bar(np.arange(128), embedding_vector_np)\n",
        "plt.xlabel(\"Component Index (within Embedding Vector)\")\n",
        "plt.ylabel(\"Component Value\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rPRIYknoZFrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we will do a heatmap of the embedded vector for all possible inputs\n",
        "\n",
        "# We want dimensions along y-axis and inputs along x-axis,\n",
        "# so we need to transpose the matrix for imshow\n",
        "W_E_transposed = W_E.detach().cpu().numpy().T # Shape (d, N)\n",
        "\n",
        "# Get N and d from the original tensor shape\n",
        "N_vocab = W_E.shape[0]\n",
        "d_embed = W_E.shape[1]\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(8, 10)) # Adjust figsize as needed (width, height)\n",
        "# Use imshow to display the matrix as an image.\n",
        "# aspect='auto' allows the cells to be non-square to fit the plot area.\n",
        "# interpolation='nearest' avoids blurring pixels.\n",
        "# cmap='viridis' is a common colormap, change if you prefer another.\n",
        "im = plt.imshow(W_E_transposed, aspect='auto', interpolation='nearest', cmap='viridis')\n",
        "\n",
        "# Add labels and title|\n",
        "plt.xlabel(\"Input Token Index (k)\")\n",
        "plt.ylabel(\"Embedding Dimension Index\")\n",
        "plt.title(\"Heatmap of Embedding Vectors (W_E)\")\n",
        "\n",
        "# Set ticks to match indices\n",
        "# Show ticks for every input token if N is small\n",
        "if N_vocab <= 20: # Adjust threshold as needed\n",
        "     plt.xticks(ticks=np.arange(N_vocab), labels=np.arange(N_vocab))\n",
        "else:\n",
        "    # For larger N, show fewer ticks to avoid clutter\n",
        "     plt.xticks(ticks=np.linspace(0, N_vocab-1, num=min(N_vocab, 10), dtype=int))\n",
        "\n",
        "\n",
        "# Add a colorbar to show the mapping from color to value\n",
        "plt.colorbar(im, label='Embedding Component Value')\n",
        "\n",
        "# Ensure layout is tight\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1QWkH9b3bJOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks... kind of periodic? Let's now try to do PCA (or SVD) on this and see if we can learn anything."
      ],
      "metadata": {
        "id": "CbDgpPUPbyvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Center the data (subtract the mean of each dimension)\n",
        "W_E_mean = W_E.mean(dim=0, keepdim=True) # Calculate mean across samples (N) for each dimension (d)\n",
        "W_E_centered = W_E - W_E_mean # Broadcasting subtracts the mean from each row\n",
        "\n",
        "# 2. Perform SVD on the centered data\n",
        "# U shape: (N, K), S shape: (K,), Vt shape: (K, d) where K = min(N, d)\n",
        "# full_matrices=False is generally more efficient\n",
        "U, S, Vt = torch.linalg.svd(W_E_centered, full_matrices=False)\n",
        "\n",
        "# --- Results ---\n",
        "\n",
        "# PCA Scores (Data projected onto principal components):\n",
        "# This is often the primary result needed for visualization/dimensionality reduction.\n",
        "# It represents each original sample (row in W_E) in the new PCA coordinate system.\n",
        "pca_scores = U * S  # Shape: (N, K) - Scales the left singular vectors by singular values\n",
        "\n",
        "# Principal Components (Loadings / Directions of maximum variance):\n",
        "# These are the rows of Vt. Each row is a d-dimensional vector representing a principal direction.\n",
        "principal_components = Vt # Shape: (K, d)\n",
        "\n",
        "# Explained Variance Ratio (requires a bit more calculation):\n",
        "explained_variance = S.square() / (W_E.shape[0] - 1) # Variance explained by each component\n",
        "total_variance = explained_variance.sum()\n",
        "explained_variance_ratio = explained_variance / total_variance\n",
        "\n",
        "# Plot score on PC1 vs token index k (similar to previous PCA example)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(np.arange(W_E.shape[0]), pca_scores[:, 0].detach().cpu().numpy(), alpha=0.7, s=10)\n",
        "plt.xlabel(\"Input Token Index (k)\")\n",
        "plt.ylabel(\"Score on PC1\")\n",
        "plt.title(\"PyTorch PCA: Score on First Principal Component vs. Input Token Index\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "E0QhpicycT8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the principal component is linear, i.e., the model learns an embedding that is given by embedded_value = w * value + b, with value being the input token index. w is actually negative, and b is ~2.\n",
        "\n",
        "We now wish to investigate how important this first component, which is a linear function of the inputs, is in comparison with the other components. We do this by looking at how much variance is explained by each of the components."
      ],
      "metadata": {
        "id": "XXtXYhAueFLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Calculate Explained Variance (using PyTorch SVD results S) ---\n",
        "\n",
        "# Variance explained by each component (proportional to singular value squared)\n",
        "# Using N (W_E.shape[0]) instead of N-1 is fine for the ratio calculation\n",
        "explained_variance = S.square() / W_E.shape[0] # Variance = (singular value / sqrt(N))^2\n",
        "total_variance = explained_variance.sum()\n",
        "explained_variance_ratio = explained_variance / total_variance\n",
        "\n",
        "# Convert to numpy for printing/plotting if needed\n",
        "explained_variance_ratio_np = explained_variance_ratio.detach().cpu().numpy()\n",
        "\n",
        "# --- Print the Ratios ---\n",
        "print(f\"Explained Variance Ratio by Principal Component:\")\n",
        "# Limit printing to the number of components computed or a reasonable max (e.g., 10)\n",
        "num_components_to_print = min(len(explained_variance_ratio_np), 10)\n",
        "cumulative_variance = 0.0\n",
        "for i in range(num_components_to_print):\n",
        "    ratio = explained_variance_ratio_np[i]\n",
        "    cumulative_variance += ratio\n",
        "    print(f\"  PC{i+1}: {ratio:.4f} ({ratio*100:.2f}%) \\t| Cumulative: {cumulative_variance:.4f} ({cumulative_variance*100:.2f}%)\")\n",
        "\n",
        "# If you computed more components than printed:\n",
        "if len(explained_variance_ratio_np) > num_components_to_print:\n",
        "    print(f\"  ...\")\n",
        "    print(f\"Total Cumulative Variance (all {len(explained_variance_ratio_np)} components): {explained_variance_ratio_np.sum():.4f} ({explained_variance_ratio_np.sum()*100:.2f}%)\")\n",
        "\n",
        "\n",
        "# --- Visualize with a Scree Plot ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "component_indices = np.arange(1, len(explained_variance_ratio_np) + 1)\n",
        "\n",
        "# Plot individual explained variance ratios\n",
        "plt.bar(component_indices, explained_variance_ratio_np, alpha=0.7, align='center',\n",
        "        label='Individual Explained Variance Ratio')\n",
        "\n",
        "# Plot cumulative explained variance ratio\n",
        "cumulative_variance_ratio_np = np.cumsum(explained_variance_ratio_np)\n",
        "plt.plot(component_indices, cumulative_variance_ratio_np, marker='o', linestyle='--',\n",
        "         label='Cumulative Explained Variance Ratio')\n",
        "\n",
        "# Add threshold lines (optional, but common)\n",
        "plt.axhline(y=0.9, color='r', linestyle=':', linewidth=1, label='90% Threshold')\n",
        "plt.axhline(y=0.95, color='g', linestyle=':', linewidth=1, label='95% Threshold')\n",
        "\n",
        "\n",
        "plt.xlabel('Principal Component Index')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Scree Plot - Explained Variance by Principal Component')\n",
        "# Ensure x-axis ticks match component indices if not too many\n",
        "if len(component_indices) <= 15:\n",
        "    plt.xticks(ticks=component_indices)\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.ylim(0, 1.1) # Set y-axis limit slightly above 1.0\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "13URfrLpepYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the first component explains ~0.75 of the variance, and the second component ~0.25. The others are basically irrelevant. This does however mean that we also need to look at the second component to understand what is happening with the embedding. Let us do so now."
      ],
      "metadata": {
        "id": "HvM2bHtSeusY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot score on PC2 vs token index k\n",
        "# This is basically the same code as before, but we now plot the second component rather than the first one\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(np.arange(W_E.shape[0]), pca_scores[:, 1].detach().cpu().numpy(), alpha=0.7, s=10)\n",
        "plt.xlabel(\"Input Token Index (k)\")\n",
        "plt.ylabel(\"Score on PC2\")\n",
        "plt.title(\"PyTorch PCA: Score on Second Principal Component vs. Input Token Index\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rgfsr8XffEXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks like a parabola. I have no clue why! Let's plot below the scores on the first component against the scores on the second component.\n",
        "\n"
      ],
      "metadata": {
        "id": "tcWuRzYngWy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "token_indices = np.arange(p)\n",
        "scatter = plt.scatter(\n",
        "    pca_scores[:, 0].detach().cpu().numpy(),  # X-coordinates are scores on PC1\n",
        "    pca_scores[:, 1].detach().cpu().numpy(),  # Y-coordinates are scores on PC2\n",
        "    c=token_indices,                          # Color points based on the input token index (0-9)\n",
        "    cmap='viridis',                           # Colormap (e.g., 'viridis', 'plasma')\n",
        "    alpha=0.8,\n",
        "    s=50                                      # Increase point size for visibility\n",
        ")\n",
        "plt.xlabel(\"Score on PC1 (Linear Component)\")\n",
        "plt.ylabel(\"Score on PC2 (Parabolic Component)\")\n",
        "plt.title(\"Embeddings Projected onto First Two Principal Components\")\n",
        "plt.colorbar(scatter, label='Input Token Index (k)')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.gca().set_aspect('equal', adjustable='box') # Try to keep scales comparable"
      ],
      "metadata": {
        "id": "Hn4QQgl6guGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An attempt at a conclusion regarding the model's learned embedding:\n",
        "\n",
        "The model projects inputs (numbers 0-9) onto a low-dimensional (2D) manifold within the embedding space. This representation encodes the number's value primarily along one linear axis (PC1) and secondarily along an orthogonal parabolic axis (PC2). This specific geometric arrangement is likely learned to facilitate the downstream integer addition computation. (Leveraging linear order + perhaps magnitude/centrality).\n",
        "\n",
        "Let's consider some potential next steps to further understand the algorithm the model is implementing:\n",
        "\n",
        "Step 2: Attention Pattern Check\n",
        "\n",
        "Goal: Check how the model combines a and b info at the = position.\n",
        "\n",
        "Select Sample Inputs: Get various pairs (a, b).\n",
        "\n",
        "Hook Attention Weights: For each head, get the attention weights from position 2 (query: =) to positions 0 (a), 1 (b), and 2 (=). Store these weights [w_to_a, w_to_b, w_to_=] for each head and input pair.\n",
        "\n",
        "Visualize Average Patterns: For each head, average the weights w_to_a, w_to_b, w_to_= across all sample inputs. Plot these three average weights per head (e.g., using bar charts).\n",
        "\n",
        "Analyze: Does position 2 attend significantly to both position 0 (a) and position 1 (b)? Are there clear differences between heads? Is attention to position 2 (=) itself low?\n",
        "\n",
        "\n",
        "Expected Outcome: Confirm attention gathers info from a and b. Note basic patterns.\n",
        "\n",
        "Step 3: MLP Analysis\n",
        "\n",
        "Goal: Understand how the MLP computes a+b from the combined a and b representations.\n",
        "\n",
        "Hook MLP Activations: Get the n-dimensional activation vector after the ReLU inside the MLP, specifically for the state calculated at position 2 (=). Do this for various input pairs (a, b).\n",
        "\n",
        "Correlate Neurons to Task Variables: For each MLP neuron, calculate its activation across the sample pairs. Find the correlation between each neuron's activation and key variables: a, b, the target sum a+b, PC1 score of a, PC2 score of a, PC1 score of b, PC2 score of b.\n",
        "\n",
        "Identify Key Neurons: Note which neurons correlate strongly with the target sum a+b. Note any correlating strongly with input features (like PC1/PC2 scores).\n",
        "\n",
        "(Optional) Visualize Key Neuron Activations: Make 2D heatmaps (x-axis a, y-axis b) for a few key neurons (e.g., sum-correlated neurons). Does the activation map look like a+b?\n",
        "\n",
        "Probe MLP State: Train a linear regression model to predict the scalar a+b using the n-dimensional MLP activation vector as input. Check probe accuracy. High accuracy implies MLP state linearly encodes the sum.\n",
        "\n",
        "Formulate Hypothesis: Based on correlations and probes, hypothesize how the MLP uses the input features (represented by PC1/PC2) via its neurons to arrive at an internal state that encodes the sum a+b."
      ],
      "metadata": {
        "id": "yhkOl0cSkR6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "captured_mlp_activations = []\n",
        "\n",
        "# Define the hook function\n",
        "def store_mlp_post_activation_hook(\n",
        "    activation: torch.Tensor,\n",
        "    hook: 'HookPoint'\n",
        "):\n",
        "    \"\"\"\n",
        "    Hook function to capture the MLP activation output (after ReLU).\n",
        "    Selects only the activation for the target position (pos 2).\n",
        "    \"\"\"\n",
        "    # activation shape is likely [batch_size, sequence_length, d_mlp]\n",
        "    # We want the activation at the '=' token, which is at index 2\n",
        "    target_activation = activation[:, 2, :].detach().cpu() # Shape: [batch_size, d_mlp]\n",
        "    captured_mlp_activations.append(target_activation)\n",
        "\n",
        "    # MUST return the activation to allow the forward pass to continue\n",
        "    return activation\n",
        "\n",
        "# Identify the exact hook point name\n",
        "mlp_hook_point_name = \"blocks.0.mlp.hook_post\" # Output AFTER ReLU\n",
        "\n",
        "# Clear previous captures before running\n",
        "captured_mlp_activations = []\n",
        "\n",
        "activation_list = []\n",
        "target_sums = []\n",
        "\n",
        "print(f\"Processing {len(dataset)} input pairs...\")\n",
        "for a, b, equals in dataset:\n",
        "    input_tokens = torch.tensor([[a, b, equals]], dtype=torch.long)\n",
        "\n",
        "    # Clear captures for this specific run\n",
        "    captured_mlp_activations = []\n",
        "\n",
        "    # Run with hooks\n",
        "    _ = model.run_with_hooks(\n",
        "        input_tokens,\n",
        "        fwd_hooks=[(mlp_hook_point_name, store_mlp_post_activation_hook)]\n",
        "    )\n",
        "\n",
        "    # Store the result if captured\n",
        "    if captured_mlp_activations:\n",
        "        # Get the vector for the first (and only) item in the batch\n",
        "        mlp_activation_vector = captured_mlp_activations[0][0].numpy() # Convert to numpy array\n",
        "        activation_list.append(mlp_activation_vector)\n",
        "        target_sums.append(a + b)\n",
        "    else:\n",
        "        print(f\"Warning: Hook did not capture activation for pair ({a}, {b})\")\n",
        "\n",
        "print(f\"Collected {len(activation_list)} activation vectors.\")\n",
        "\n",
        "# Convert to numpy arrays for training the probe\n",
        "X_probe = np.array(activation_list) # Shape: [num_samples, d_mlp]\n",
        "y_probe = y_probe = np.array([item.cpu().numpy() for item in target_sums])     # Shape: [num_samples]\n",
        "\n",
        "print(\"Probe input data shapes:\")\n",
        "print(\"X_probe:\", X_probe.shape)\n",
        "print(\"y_probe:\", y_probe.shape)"
      ],
      "metadata": {
        "id": "mGdQDpBLZ-45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have the outputs of MLP for each input pair. We will try to fit this to a linear function to see if this directly correlates to the desired representation of the sum."
      ],
      "metadata": {
        "id": "YfEas_TZcK1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_probe, y_probe, test_size=0.3, random_state=42 # Adjust test_size if needed\n",
        ")\n",
        "\n",
        "# 2. Initialize and train the Linear Regression model\n",
        "probe_model = LinearRegression()\n",
        "probe_model.fit(X_train, y_train)\n",
        "\n",
        "# 3. Make predictions on the test set\n",
        "y_pred = probe_model.predict(X_test)\n",
        "\n",
        "# 4. Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Linear Probe Performance:\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"  R-squared (R2): {r2:.4f}\")\n",
        "\n",
        "# Optional: Basic baseline comparison (predicting the mean of training labels)\n",
        "baseline_pred = np.full_like(y_test, y_train.mean())\n",
        "baseline_mse = mean_squared_error(y_test, baseline_pred)\n",
        "baseline_r2 = r2_score(y_test, baseline_pred) # Will be close to 0 by definition\n",
        "\n",
        "print(f\"\\nBaseline Performance (Predicting Mean):\")\n",
        "print(f\"  Baseline MSE: {baseline_mse:.4f}\")\n",
        "print(f\"  Baseline R2: {baseline_r2:.4f}\")\n",
        "\n",
        "\n",
        "# Optional: Visualize predictions vs actual\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.7, label='Predictions')\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='red', label='Perfect Prediction')\n",
        "plt.xlabel(\"True Values (a+b)\")\n",
        "plt.ylabel(\"Predicted Values (a+b)\")\n",
        "plt.title(\"Linear Probe: True vs. Predicted Sums\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZwCrg85bctoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The linear regression we've done is almost perfectly accurate, which means that the post-ReLU activations contain a linear representation of the target sum a+b. This means that the bulk of the computation is being done by the MLP. Hence, we can likely find 'sum neurons', i.e., neurons that are mostly responsible for implementing the computation. We will do this by correlation analysis: what neurons' activations correlate more strongly with a+b?"
      ],
      "metadata": {
        "id": "WlxnMMODduVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Ensure inputs are numpy arrays\n",
        "X_probe = np.asarray(X_probe)\n",
        "y_probe = np.asarray(y_probe)\n",
        "\n",
        "num_samples, d_mlp = X_probe.shape\n",
        "neuron_sum_correlations = np.zeros(d_mlp)\n",
        "\n",
        "# Calculate Pearson correlation for each neuron\n",
        "for i in range(d_mlp):\n",
        "    # pearsonr returns (correlation_coefficient, p_value)\n",
        "    correlation, _ = pearsonr(X_probe[:, i], y_probe)\n",
        "    neuron_sum_correlations[i] = correlation\n",
        "\n",
        "# Handle potential NaN values if a neuron's activation was constant (zero variance)\n",
        "neuron_sum_correlations = np.nan_to_num(neuron_sum_correlations)\n",
        "\n",
        "print(f\"Calculated correlations for {d_mlp} neurons.\")\n",
        "# The 'neuron_sum_correlations' array now holds the correlation of each neuron with a+b.\n",
        "\n",
        "# Find the indices of neurons with highest absolute correlation\n",
        "top_n = 20\n",
        "# Get indices sorted by absolute correlation, descending\n",
        "indices_sorted_by_abs_corr = np.argsort(np.abs(neuron_sum_correlations))[::-1]\n",
        "\n",
        "print(f\"\\nTop {top_n} neurons by absolute correlation with sum (a+b):\")\n",
        "for i in range(top_n):\n",
        "    idx = indices_sorted_by_abs_corr[i]\n",
        "    print(f\"  Neuron {idx}: Correlation = {neuron_sum_correlations[idx]:.4f}\")"
      ],
      "metadata": {
        "id": "qSIkqzcDeWQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "# --- Visualization 1: Histogram of Correlations ---\n",
        "print(\"\\nGenerating Histogram of Correlations...\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(neuron_sum_correlations, bins=50, kde=True)\n",
        "plt.title('Histogram of Neuron Activations\\' Correlation with Sum (a+b)')\n",
        "plt.xlabel('Pearson Correlation Coefficient')\n",
        "plt.ylabel('Number of Neurons')\n",
        "plt.grid(axis='y', alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "# --- Visualization 2: Sorted Correlation Plot (Stem Plot) ---\n",
        "print(\"\\nGenerating Sorted Correlation Plot...\")\n",
        "sorted_indices = np.argsort(neuron_sum_correlations)\n",
        "sorted_correlations = neuron_sum_correlations[sorted_indices]\n",
        "ranks = np.arange(d_mlp)\n",
        "\n",
        "pos_mask = sorted_correlations >= 0\n",
        "neg_mask = sorted_correlations < 0\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "# Plot positive correlations\n",
        "markerline_pos, stemlines_pos, baseline_pos = plt.stem(\n",
        "    ranks[pos_mask], sorted_correlations[pos_mask],\n",
        "    linefmt='b-', markerfmt='bo', basefmt=' ', label='Positive Corr'\n",
        ")\n",
        "plt.setp(stemlines_pos, linewidth=1, alpha=0.7)\n",
        "plt.setp(markerline_pos, markersize=3, alpha=0.7)\n",
        "\n",
        "# Plot negative correlations\n",
        "markerline_neg, stemlines_neg, baseline_neg = plt.stem(\n",
        "    ranks[neg_mask], sorted_correlations[neg_mask],\n",
        "    linefmt='r-', markerfmt='ro', basefmt=' ', label='Negative Corr'\n",
        ")\n",
        "plt.setp(stemlines_neg, linewidth=1, alpha=0.7)\n",
        "plt.setp(markerline_neg, markersize=3, alpha=0.7)\n",
        "\n",
        "plt.title('Neuron Correlations with Sum (a+b), Sorted by Value')\n",
        "plt.xlabel('Neuron Rank (Sorted by Correlation)')\n",
        "plt.ylabel('Pearson Correlation Coefficient')\n",
        "plt.ylim(-1.05, 1.05)\n",
        "plt.grid(axis='y', alpha=0.5)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "num_outputs=20\n",
        "# --- Visualization 3: Heatmap of Sorted W_L ---\n",
        "# Ensure W_L is defined correctly (num_outputs x d_mlp)\n",
        "# Example: W_L = model.W_U @ model.blocks[0].mlp.W_out (or however you get it)\n",
        "if 'W_L' in locals() and W_L.shape == (num_outputs, d_mlp):\n",
        "    print(\"\\nGenerating Heatmap of Sorted W_L...\")\n",
        "    # Sort columns (neurons) of W_L based on correlation\n",
        "    neuron_corr_sorted_indices = np.argsort(neuron_sum_correlations)\n",
        "    W_L_sorted_by_neuron_corr = W_L[:, neuron_corr_sorted_indices]\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    sns.heatmap(W_L_sorted_by_neuron_corr, cmap='coolwarm', center=0) # 'coolwarm' is good for weights\n",
        "    plt.title('Neuron-Logit Map (W_L) Columns Sorted by Neuron Correlation')\n",
        "    plt.xlabel('Neurons (Sorted by Correlation with Sum a+b)')\n",
        "    plt.ylabel(f'Output Logit (0 to {num_outputs-1})')\n",
        "    plt.yticks(np.arange(num_outputs) + 0.5, labels=np.arange(num_outputs), rotation=0)\n",
        "    plt.xticks([]) # Hide neuron indices as they are too dense\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nSkipping Heatmap of W_L: W_L matrix not found or incorrect shape.\")\n",
        "    print(f\"Expected shape: ({num_outputs}, {d_mlp})\")\n",
        "    if 'W_L' in locals():\n",
        "        print(f\"Actual shape: {W_L.shape}\")\n",
        "\n",
        "\n",
        "# --- Visualization 4: Heatmap of Sorted Neuron Activations ---\n",
        "# Ensure X_probe and y_probe are defined correctly\n",
        "if 'X_probe' in locals() and 'y_probe' in locals() and X_probe.shape[0] == y_probe.shape[0]:\n",
        "    print(\"\\nGenerating Heatmap of Sorted Neuron Activations...\")\n",
        "    # 1. Sort columns (neurons) by correlation\n",
        "    neuron_corr_sorted_indices = np.argsort(neuron_sum_correlations)\n",
        "    X_probe_neurons_sorted = X_probe[:, neuron_corr_sorted_indices]\n",
        "\n",
        "    # 2. Sort rows (inputs) by the target sum y_probe\n",
        "    input_sum_sorted_indices = np.argsort(y_probe)\n",
        "    X_probe_fully_sorted = X_probe_neurons_sorted[input_sum_sorted_indices, :]\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    # Use robust=True to handle potential outliers in activations\n",
        "    # Use cbar=False to avoid clutter, as absolute activation values might vary\n",
        "    sns.heatmap(X_probe_fully_sorted, cmap='viridis', robust=True, cbar=False)\n",
        "    plt.title('MLP Activations (Sorted)')\n",
        "    plt.xlabel('Neurons (Sorted by Correlation with Sum a+b)')\n",
        "    plt.ylabel('Input Samples (Sorted by Sum a+b)')\n",
        "    plt.xticks([]) # Hide neuron indices\n",
        "    plt.yticks([]) # Hide sample indices\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nSkipping Heatmap of Activations: X_probe or y_probe not found or shapes mismatch.\")\n",
        "    if 'X_probe' in locals():\n",
        "        print(f\"X_probe shape: {X_probe.shape}\")\n",
        "    if 'y_probe' in locals():\n",
        "        print(f\"y_probe shape: {y_probe.shape}\")"
      ],
      "metadata": {
        "id": "G-Ep8Ey9cc6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe widespread strong positive correlation of neuron activations with the target sum. In fact, more than 130 neurons have a correlation exceeding 0.9 with the target sum. This shows that the MLP represents the sum in a highly distributed manner. There aren't localized, specialized 'sum' neurons, per se.\n",
        "\n",
        "We will now visualize the activation patterns of a few top-correlated neurons as heatmaps across the grid of all possible (a, b) input pairs. This reveals the specific function each of these top neurons computes according to input pairs. Hopefully from this we can hypothesize how they combine the principal components we identified earlier to achieve their strong correlation."
      ],
      "metadata": {
        "id": "WwBL5h1Cg7IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "equals_token_id = 10\n",
        "N = 10\n",
        "top_n_to_visualize = 3\n",
        "top_neuron_indices = indices_sorted_by_abs_corr[:top_n_to_visualize]\n",
        "mlp_hook_point_name = \"blocks.0.mlp.hook_post\" # Output AFTER ReLU\n",
        "\n",
        "# --- Data Storage ---\n",
        "# Create heatmaps initialized with NaN (or zero)\n",
        "neuron_activation_heatmaps = {\n",
        "    idx: np.full((N, N), np.nan) for idx in top_neuron_indices\n",
        "}\n",
        "captured_mlp_activation_storage = {} # Temporary storage during hook\n",
        "\n",
        "# --- Hook Function ---\n",
        "def capture_mlp_post_hook(activation: torch.Tensor, hook: 'HookPoint'):\n",
        "    \"\"\"Captures MLP activation at position 2.\"\"\"\n",
        "    captured_mlp_activation_storage['activation'] = activation[:, 2, :].detach().cpu()\n",
        "    return activation\n",
        "\n",
        "# --- Generate Activations ---\n",
        "print(f\"Generating activations for {top_n_to_visualize} neurons across {N}x{N} grid...\")\n",
        "model.eval() # Ensure model is in evaluation mode\n",
        "with torch.no_grad(): # No need to track gradients\n",
        "    for a in range(N):\n",
        "        for b in range(N):\n",
        "              input_tokens = torch.tensor([[a, b, equals_token_id]], dtype=torch.long)\n",
        "\n",
        "              # Clear previous capture\n",
        "              captured_mlp_activation_storage.clear()\n",
        "\n",
        "              # Run with hook\n",
        "              _ = model.run_with_hooks(\n",
        "                  input_tokens,\n",
        "                  fwd_hooks=[(mlp_hook_point_name, capture_mlp_post_hook)]\n",
        "              )\n",
        "\n",
        "              # Store the activations for the target neurons\n",
        "              if 'activation' in captured_mlp_activation_storage:\n",
        "                  full_activation_vector = captured_mlp_activation_storage['activation'][0] # Batch size 1\n",
        "                  for neuron_idx in top_neuron_indices:\n",
        "                      neuron_activation_heatmaps[neuron_idx][a, b] = full_activation_vector[neuron_idx].item()\n",
        "print(\"Activation generation complete.\")\n",
        "\n",
        "# --- Plotting ---\n",
        "fig, axes = plt.subplots(1, top_n_to_visualize, figsize=(6 * top_n_to_visualize, 5))\n",
        "if top_n_to_visualize == 1: # Handle case of single subplot\n",
        "    axes = [axes]\n",
        "\n",
        "fig.suptitle(\"MLP Neuron Activations (Post-ReLU) at Position '=' vs. Inputs (a, b)\")\n",
        "\n",
        "for i, neuron_idx in enumerate(top_neuron_indices):\n",
        "    ax = axes[i]\n",
        "    heatmap_data = neuron_activation_heatmaps[neuron_idx]\n",
        "    sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"viridis\", ax=ax,\n",
        "                linewidths=.5, linecolor='gray', cbar=True, square=True,\n",
        "                # Mask cells with NaN so they don't show color\n",
        "                mask=np.isnan(heatmap_data))\n",
        "    ax.set_title(f\"Neuron {neuron_idx}\\nCorr w/ Sum: {neuron_sum_correlations[neuron_idx]:.3f}\")\n",
        "    ax.set_xlabel(\"Input b = F(n-1)\")\n",
        "    ax.set_ylabel(\"Input a = F(n-2)\")\n",
        "    # Set ticks to match input values\n",
        "    ax.set_xticks(np.arange(N) + 0.5)\n",
        "    ax.set_yticks(np.arange(N) + 0.5)\n",
        "    ax.set_xticklabels(np.arange(N))\n",
        "    ax.set_yticklabels(np.arange(N))\n",
        "    ax.invert_yaxis() # Convention often puts (0,0) at top-left for matrices\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZTxa377eiafB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmaps show that the top-correlated neurons implement a function approximating ReLU(linear_function_of_sum). We observe a region of 0s (e.g., all values adding up to 6 or less for the middle neuron above), which is likely a result of the ReLU cutting off negatrive pre-activations, which basically creates a threshold below which the neurons do not fire. Further, we get roughly linear behavior with the sum of inputs above the threshold, as expected from the ReLU. The fact that the patterns are so similar across neurons again emphasizes that the computation implemented by the model is distributed.\n",
        "\n",
        "We will now examine the pre-ReLU activations for these same neurons to test our hypothesis. If the core computation is indeed a linear function of the sum that is simply thresholded by ReLU, then the pre-ReLU heatmaps should reveal this underlying linear relationship more clearly across the entire input grid, without the large zeroed-out regions, thus confirming the thresholding mechanism."
      ],
      "metadata": {
        "id": "Tml6Dlfukni5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_hook_point_name_pre = \"blocks.0.mlp.hook_pre\" # Input BEFORE ReLU\n",
        "\n",
        "# --- Data Storage ---\n",
        "neuron_pre_activation_heatmaps = {\n",
        "    idx: np.full((N, N), np.nan) for idx in top_neuron_indices\n",
        "}\n",
        "captured_mlp_pre_activation_storage = {} # Temporary storage during hook\n",
        "\n",
        "# --- Hook Function ---\n",
        "def capture_mlp_pre_hook(activation: torch.Tensor, hook: 'HookPoint'):\n",
        "    \"\"\"Captures MLP activation input (before ReLU) at position 2.\"\"\"\n",
        "    captured_mlp_pre_activation_storage['activation'] = activation[:, 2, :].detach().cpu()\n",
        "    return activation\n",
        "\n",
        "# --- Generate Activations ---\n",
        "print(f\"Generating PRE-ReLU activations for {len(top_neuron_indices)} neurons across {N}x{N} grid...\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for a in range(N):\n",
        "        for b in range(N):\n",
        "              input_tokens = torch.tensor([[a, b, equals_token_id]], dtype=torch.long)\n",
        "\n",
        "              # Clear previous capture\n",
        "              captured_mlp_pre_activation_storage.clear()\n",
        "\n",
        "              # Run with PRE-ReLU hook\n",
        "              _ = model.run_with_hooks(\n",
        "                  input_tokens,\n",
        "                  fwd_hooks=[(mlp_hook_point_name_pre, capture_mlp_pre_hook)]\n",
        "              )\n",
        "\n",
        "              # Store the activations for the target neurons\n",
        "              if 'activation' in captured_mlp_pre_activation_storage:\n",
        "                  full_activation_vector = captured_mlp_pre_activation_storage['activation'][0]\n",
        "                  for neuron_idx in top_neuron_indices:\n",
        "                      neuron_pre_activation_heatmaps[neuron_idx][a, b] = full_activation_vector[neuron_idx].item()\n",
        "print(\"PRE-ReLU activation generation complete.\")\n",
        "\n",
        "# --- Plotting ---\n",
        "fig, axes = plt.subplots(1, len(top_neuron_indices), figsize=(6 * len(top_neuron_indices), 5))\n",
        "if len(top_neuron_indices) == 1: axes = [axes]\n",
        "\n",
        "fig.suptitle(\"MLP Neuron Activations (PRE-ReLU) at Position '=' vs. Inputs (a, b)\")\n",
        "\n",
        "for i, neuron_idx in enumerate(top_neuron_indices):\n",
        "    ax = axes[i]\n",
        "    # Use a diverging colormap like 'coolwarm' or 'RdBu_r' for pre-ReLU\n",
        "    # as values can be positive or negative. Center the color map at 0.\n",
        "    heatmap_data = neuron_pre_activation_heatmaps[neuron_idx]\n",
        "    max_abs_val = np.nanmax(np.abs(heatmap_data)) # Find max absolute value for symmetric color scale\n",
        "    sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax,\n",
        "                linewidths=.5, linecolor='gray', cbar=True, square=True,\n",
        "                mask=np.isnan(heatmap_data),\n",
        "                vmin=-max_abs_val, vmax=max_abs_val) # Center colormap around 0\n",
        "    ax.set_title(f\"Neuron {neuron_idx} (Pre-ReLU)\")\n",
        "    ax.set_xlabel(\"Input b = F(n-1)\")\n",
        "    ax.set_ylabel(\"Input a = F(n-2)\")\n",
        "    ax.set_xticks(np.arange(N) + 0.5)\n",
        "    ax.set_yticks(np.arange(N) + 0.5)\n",
        "    ax.set_xticklabels(np.arange(N))\n",
        "    ax.set_yticklabels(np.arange(N))\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lncq4AklkrLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We in fact observe a linear relation between value of the sum and pre-ReLU neruon value. However, note that the top neuron has a lot of zero values, whereas the other two go from negative to positive with similar magnitude. This indicates that they learn different linear functions (slopes, biases). However, part of this difference is thresholded away by the ReLU activation.\n",
        "\n",
        "We now want to fit the pre-ReLU values to the PCA components to understand exactly what function thereof they are computing."
      ],
      "metadata": {
        "id": "JfAJwAf5mr_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "target_neuron_idx = 408\n",
        "feature_list = []\n",
        "target_activation_list = []\n",
        "\n",
        "# Ensure pca_scores is NumPy for easier indexing if needed, keep precision\n",
        "pca_scores_np = pca_scores.detach().cpu().numpy().astype(np.float32)\n",
        "pre_activation_target_neuron = neuron_pre_activation_heatmaps[target_neuron_idx] # Get the heatmap\n",
        "\n",
        "for a in range(N):\n",
        "    for b in range(N):\n",
        "        if a + b < N: # Only use valid sums\n",
        "            # Get pre-calculated pre-ReLU activation\n",
        "            target_activation = pre_activation_target_neuron[a, b]\n",
        "\n",
        "            # Ensure we don't include NaN values if any exist in heatmap\n",
        "            if not np.isnan(target_activation):\n",
        "                # Extract PCA features for a and b\n",
        "                pc1_a = pca_scores_np[a, 0]\n",
        "                pc2_a = pca_scores_np[a, 1]\n",
        "                pc1_b = pca_scores_np[b, 0]\n",
        "                pc2_b = pca_scores_np[b, 1]\n",
        "\n",
        "                # Add more features if desired (e.g., interactions)\n",
        "                # For now, just the 4 core PCA features\n",
        "                features = [pc1_a, pc2_a, pc1_b, pc2_b]\n",
        "                feature_list.append(features)\n",
        "                target_activation_list.append(target_activation)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "X_features_for_neuron_model = np.array(feature_list, dtype=np.float32)\n",
        "y_target_neuron_pre_activation = np.array(target_activation_list, dtype=np.float32)\n",
        "\n",
        "# Define feature names for interpreting coefficients\n",
        "feature_names = ['PC1(a)', 'PC2(a)', 'PC1(b)', 'PC2(b)']\n",
        "\n",
        "print(f\"Prepared data for modeling neuron {target_neuron_idx}:\")\n",
        "print(f\"  X_features shape: {X_features_for_neuron_model.shape}\")\n",
        "print(f\"  y_target shape: {y_target_neuron_pre_activation.shape}\")\n",
        "\n",
        "# --- Fit the Linear Regression Model ---\n",
        "neuron_model = LinearRegression()\n",
        "neuron_model.fit(X_features_for_neuron_model, y_target_neuron_pre_activation)\n",
        "\n",
        "# --- Evaluate the fit ---\n",
        "y_neuron_pred = neuron_model.predict(X_features_for_neuron_model)\n",
        "r2_neuron_fit = r2_score(y_target_neuron_pre_activation, y_neuron_pred)\n",
        "\n",
        "print(f\"\\nLinear Model Fit for Neuron {target_neuron_idx} (Pre-ReLU):\")\n",
        "print(f\"  R-squared of fit: {r2_neuron_fit:.4f}\")\n",
        "\n",
        "# --- Interpret the model ---\n",
        "print(f\"  Learned coefficients:\")\n",
        "coeffs = pd.Series(neuron_model.coef_, index=feature_names)\n",
        "print(coeffs)\n",
        "print(f\"  Intercept: {neuron_model.intercept_:.4f}\")"
      ],
      "metadata": {
        "id": "2jfGCoeAn6DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The excellent R² value confirms that Neuron 240's pre-ReLU activation is accurately modeled as a linear combination of the primary (linear value) and secondary (parabolic centrality) features derived from the input embeddings. The near-identical coefficients for the a and b components demonstrate symmetric treatment of the inputs, while the dominance of the PC1 coefficients confirms the neuron primarily computes a function proportional to the sum a+b. The smaller but non-zero PC2 coefficients suggest this core computation is subtly modulated based on the inputs' centrality, likely for fine-tuning or boundary adjustments within the N=10 range.\n",
        "\n",
        "Something confuses me: it seems that the model just directly learns to add, as the linear component of the PCA is the most significant. However, the fact that there's a non-negligible contribution from the second component of the PCA makes me doubt this. In fact, the first one only explains 73% of the variance! That's a lot, but far from everything.\n",
        "\n",
        "This motivates performing an ablation study, in which we get rid of the second principal component and investigate how (if at all) this affects model performance."
      ],
      "metadata": {
        "id": "uT4iKK2dpYkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_E_mean_gpu = W_E_mean.to(model.cfg.device) # Ensure mean is on correct device\n",
        "pc1_vec = principal_components[0].to(model.cfg.device)\n",
        "pc2_vec = principal_components[1].to(model.cfg.device)\n",
        "\n",
        "# --- Squeeze the mean vector to be explicitly 1D ---\n",
        "W_E_mean_1d_gpu = W_E_mean_gpu.squeeze(0) # Shape becomes [d_model]\n",
        "\n",
        "def ablate_pc2_hook(\n",
        "    activation: torch.Tensor, # Shape [batch, seq_len, d_model]\n",
        "    hook: 'HookPoint'\n",
        "):\n",
        "    # Make a copy to modify\n",
        "    ablated_activation = activation.clone()\n",
        "\n",
        "    # Iterate through batch and relevant sequence positions (0 and 1 for a, b)\n",
        "    for batch_idx in range(activation.shape[0]):\n",
        "        for seq_idx in [0, 1]: # Ablate for token a and token b\n",
        "            emb = activation[batch_idx, seq_idx, :] # Should be [d_model]\n",
        "\n",
        "            # --- Subtract the squeezed 1D mean ---\n",
        "            emb_centered = emb - W_E_mean_1d_gpu # Both are [d_model], result is [d_model]\n",
        "\n",
        "            # Project - Now both inputs to torch.dot are 1D\n",
        "            score_pc1 = torch.dot(emb_centered, pc1_vec)\n",
        "            # score_pc2 = torch.dot(emb_centered, pc2_vec) # Still not needed\n",
        "\n",
        "            # Reconstruct using only PC1 contribution\n",
        "            # score_pc1 is a scalar, pc1_vec is [d_model] -> broadcasting works\n",
        "            emb_centered_ablated = score_pc1 * pc1_vec\n",
        "\n",
        "            # Add the 1D mean back\n",
        "            emb_ablated = emb_centered_ablated + W_E_mean_1d_gpu\n",
        "\n",
        "            # Put modified embedding back\n",
        "            ablated_activation[batch_idx, seq_idx, :] = emb_ablated\n",
        "\n",
        "    # Return the modified activations\n",
        "    return ablated_activation\n",
        "\n",
        "# --- How to run ---\n",
        "embedding_hook_point = \"hook_embed\" # Or \"hook_pos_embed\", check model.hook_dict\n",
        "logits = model.run_with_hooks(\n",
        "     input_tokens,\n",
        "     fwd_hooks=[(embedding_hook_point, ablate_pc2_hook)]\n",
        " )\n",
        "\n",
        "test_pairs = []\n",
        "test_labels_list = []\n",
        "for a in range(N):\n",
        "    for b in range(N):\n",
        "          test_pairs.append([a, b, equals_token_id])\n",
        "          test_labels_list.append(a + b)\n",
        "\n",
        "# Convert to tensors\n",
        "test_input_tokens = torch.tensor(test_pairs, dtype=torch.long).to(model.cfg.device)\n",
        "test_labels = torch.tensor(test_labels_list, dtype=torch.long).to(model.cfg.device)\n",
        "\n",
        "print(f\"Created test data with {test_input_tokens.shape[0]} samples.\")\n",
        "\n",
        "# --- Helper function to calculate accuracy ---\n",
        "def calculate_accuracy(logits, labels):\n",
        "    \"\"\"Calculates accuracy given logits and labels.\"\"\"\n",
        "    # Logits shape: [batch, seq_len, d_vocab]\n",
        "    # We only care about the prediction at the last position (index 2)\n",
        "    prediction_logits = logits[:, -1, :] # Shape: [batch, d_vocab]\n",
        "    predicted_tokens = torch.argmax(prediction_logits, dim=-1) # Shape: [batch]\n",
        "    correct_predictions = (predicted_tokens == labels).sum().item()\n",
        "    total_predictions = labels.shape[0]\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return accuracy, correct_predictions, total_predictions\n",
        "\n",
        "# --- 2. Calculate Original Accuracy (No Hooks) ---\n",
        "model.eval() # Set model to evaluation mode\n",
        "with torch.no_grad(): # Use no_grad for inference efficiency\n",
        "    original_logits = model(test_input_tokens)\n",
        "    original_accuracy, orig_correct, orig_total = calculate_accuracy(original_logits, test_labels)\n",
        "\n",
        "print(f\"\\nOriginal Model Performance:\")\n",
        "print(f\"  Accuracy: {original_accuracy:.4f} ({orig_correct}/{orig_total})\")\n",
        "\n",
        "# --- 3. Calculate Ablated Accuracy (With Hook) ---\n",
        "with torch.no_grad():\n",
        "    ablated_logits = model.run_with_hooks(\n",
        "        test_input_tokens,\n",
        "        fwd_hooks=[(embedding_hook_point, ablate_pc2_hook)]\n",
        "    )\n",
        "    ablated_accuracy, ablated_correct, ablated_total = calculate_accuracy(ablated_logits, test_labels)\n",
        "\n",
        "print(f\"\\nPC2 Ablated Model Performance:\")\n",
        "print(f\"  Accuracy: {ablated_accuracy:.4f} ({ablated_correct}/{ablated_total})\")\n",
        "\n",
        "# --- 4. Compare ---\n",
        "accuracy_drop = original_accuracy - ablated_accuracy\n",
        "print(f\"\\nAccuracy Drop due to PC2 Ablation: {accuracy_drop:.4f}\")"
      ],
      "metadata": {
        "id": "pzZ208_Kq3_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy drops by a lot! We go from getting it right 100% of the time, to only 42%. So the second principal component is still very important, and whatever non-linearity it's doing actually matters. Let's now look specifically at which input pairs the model fails at when we ablate the PC2."
      ],
      "metadata": {
        "id": "P2IFjLdFsSE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Get predictions from the ablated run ---\n",
        "ablated_logits_last_pos = ablated_logits[:, -1, :] # Shape: [num_samples, d_vocab]\n",
        "ablated_predicted_tokens = torch.argmax(ablated_logits_last_pos, dim=-1) # Shape: [num_samples]\n",
        "\n",
        "# --- Identify incorrect predictions ---\n",
        "incorrect_mask = (ablated_predicted_tokens != test_labels)\n",
        "incorrect_indices = torch.where(incorrect_mask)[0].cpu().numpy()\n",
        "\n",
        "# --- Map indices back to (a, b) pairs ---\n",
        "failing_pairs = []\n",
        "original_a_values = test_input_tokens[:, 0].cpu().numpy()\n",
        "original_b_values = test_input_tokens[:, 1].cpu().numpy()\n",
        "true_sums = test_labels.cpu().numpy()\n",
        "predicted_sums_ablated = ablated_predicted_tokens.cpu().numpy()\n",
        "\n",
        "for idx in incorrect_indices:\n",
        "    a = original_a_values[idx]\n",
        "    b = original_b_values[idx]\n",
        "    true_sum = true_sums[idx]\n",
        "    predicted_sum = predicted_sums_ablated[idx]\n",
        "    failing_pairs.append({\n",
        "        'a': a,\n",
        "        'b': b,\n",
        "        'True Sum (a+b)': true_sum,\n",
        "        'Predicted Sum (Ablated)': predicted_sum\n",
        "    })\n",
        "\n",
        "# --- Display the failing pairs ---\n",
        "if not failing_pairs:\n",
        "    print(\"No failures found after PC2 ablation (unexpected based on previous accuracy).\")\n",
        "else:\n",
        "    print(f\"Found {len(failing_pairs)} failures after PC2 ablation:\")\n",
        "    fail_df = pd.DataFrame(failing_pairs)\n",
        "    # Sort for potentially easier pattern spotting\n",
        "    fail_df = fail_df.sort_values(by=['True Sum (a+b)', 'a', 'b']).reset_index(drop=True)\n",
        "    print(fail_df.to_string()) # Use to_string to print the full DataFrame\n",
        "\n",
        "# Optional: Visualize failures on a heatmap\n",
        "failure_heatmap = np.zeros((N, N))\n",
        "for failure in failing_pairs:\n",
        "    failure_heatmap[failure['a'], failure['b']] = 1 # Mark failures with 1\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(failure_heatmap, cmap=\"Reds\", linewidths=.5, linecolor='gray', cbar=False, annot=False)\n",
        "plt.title(\"Failure Cases (Red) after PC2 Ablation\")\n",
        "plt.xlabel(\"Input b = F(n-1)\")\n",
        "plt.ylabel(\"Input a = F(n-2)\")\n",
        "plt.xticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.yticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eT9WHfMAstJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is interesting... There's definitely patterns, but they are not easy to interpret. Some thoughts:\n",
        "\n",
        "1.   Symmetry: Confirmed. Reinforces that a and b are treated symmetrically.\n",
        "2.   Failures involving 0: This is a huge clue. The model fails for (0, 1) through (0, 8) and (1, 0) through (8, 0). However, it succeeds for (0, 0), (0, 9), and (9, 0).\n",
        "3.   Same observation for failures involving 0. This suggests some symmetry in how the model treats small and big numbers.\n",
        "3.   Central Failures: There's definitely a cluster of failures for pairs where both a and b are \"mid-range\" (roughly 2-7).\n",
        "\n",
        "Given that the PC2 ablation dramatically impacts accuracy with a complex failure pattern—particularly affecting pairs involving edge tokens (0 and 9) differently and the central diagonal—it strongly suggests the issue lies not just in the MLP's internal calculation, but crucially in how the final output is derived from the MLP state. This points directly to the Unembedding layer (W_U), which performs this final mapping to output logits. Therefore, the next logical step is to analyze W_U to understand how it interprets the MLP's combined PC1/PC2 representation and why removing the PC2 component leads to these specific, non-uniform failures in decoding the sum.\n"
      ],
      "metadata": {
        "id": "S8qv09Qqtm95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hook_point_mlp_out = \"blocks.0.hook_mlp_out\"       # Output of the MLP layer\n",
        "hook_point_resid_post = \"blocks.0.hook_resid_post\"  # Output of the whole block (after skip add)\n",
        "\n",
        "# --- Shared dictionary for activation passing ---\n",
        "# This dictionary will be accessible by both hooks during the run\n",
        "captured_activations_for_run = {}\n",
        "\n",
        "# --- Hook Function to CAPTURE mlp_out ---\n",
        "def capture_mlp_out_hook(activation, hook):\n",
        "    \"\"\"Captures the mlp_out activation into the shared dictionary.\"\"\"\n",
        "    captured_activations_for_run['mlp_out'] = activation.detach()\n",
        "    # Must return the original activation to not affect the forward pass here\n",
        "    return activation\n",
        "\n",
        "# --- Hook Function to MODIFY resid_post ---\n",
        "def replace_resid_post_hook(activation, hook):\n",
        "    \"\"\"\n",
        "    Replaces the resid_post activation with the captured mlp_out\n",
        "    from the shared dictionary.\n",
        "    \"\"\"\n",
        "    if 'mlp_out' not in captured_activations_for_run:\n",
        "         print(f\"Warning: Capture hook '{hook_point_mlp_out}' did not run before modify hook '{hook.name}'\")\n",
        "         return activation # Safety fallback\n",
        "\n",
        "    mlp_out_value = captured_activations_for_run['mlp_out']\n",
        "\n",
        "    # Ensure shapes are compatible\n",
        "    if activation.shape == mlp_out_value.shape:\n",
        "        # --- IMPORTANT: Return the captured mlp_out value ---\n",
        "        # This replaces the original resid_post value (attn_out + mlp_out)\n",
        "        # with just mlp_out for the rest of the forward pass.\n",
        "        return mlp_out_value\n",
        "    else:\n",
        "        print(f\"Shape mismatch warning: {hook.name} ({activation.shape}) vs captured mlp_out ({mlp_out_value.shape})\")\n",
        "        return activation # Fallback\n",
        "\n",
        "# --- Run the Ablation ---\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Clear the shared dictionary before the run\n",
        "    captured_activations_for_run.clear()\n",
        "\n",
        "    # Run with BOTH hooks active simultaneously\n",
        "    # The library executes hooks in the order they appear in the forward pass\n",
        "    # hook_mlp_out runs before hook_resid_post, so this works.\n",
        "    print(f\"Running model with capture hook at '{hook_point_mlp_out}' and modify hook at '{hook_point_resid_post}'\")\n",
        "    skip_ablated_logits = model.run_with_hooks(\n",
        "        test_input_tokens,\n",
        "        fwd_hooks=[\n",
        "            (hook_point_mlp_out, capture_mlp_out_hook),\n",
        "            (hook_point_resid_post, replace_resid_post_hook)\n",
        "        ]\n",
        "    )\n",
        "    print(\"Model run with hooks completed.\")\n",
        "\n",
        "\n",
        "# --- Calculate and Compare Accuracy ---\n",
        "# Recalculate original accuracy\n",
        "original_logits = model(test_input_tokens) # Run without hooks\n",
        "original_accuracy, orig_correct, orig_total = calculate_accuracy(original_logits, test_labels)\n",
        "\n",
        "# Calculate ablated accuracy\n",
        "skip_ablated_accuracy, skip_ablated_correct, skip_ablated_total = calculate_accuracy(skip_ablated_logits, test_labels)\n",
        "\n",
        "print(f\"\\nOriginal Model Accuracy: {original_accuracy:.4f} ({orig_correct}/{orig_total})\")\n",
        "print(f\"Skip Conn Ablated Accuracy (MLP Only): {skip_ablated_accuracy:.4f} ({skip_ablated_correct}/{skip_ablated_total})\")\n",
        "print(f\"Accuracy Drop due to Skip Ablation: {original_accuracy - skip_ablated_accuracy:.4f}\")\n",
        "\n",
        "# Clear the dictionary just in case\n",
        "captured_activations_for_run.clear()"
      ],
      "metadata": {
        "id": "xu7b4_Fzx4aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No accuracy is lost by dropping the skip connection (the bit where you re-add the MLP input to its output before following to the output layer of the transformer). This indicates that we can effectively ignore it. Nanda had observed the same. Just to be sure, we are now going to do the same analysis but with the test loss rather than accuracy."
      ],
      "metadata": {
        "id": "RKHwwWKD9q7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "captured_activations_for_run = {}\n",
        "\n",
        "# --- Run the Ablation and Calculate Logits (same as before) ---\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Clear the shared dictionary before the run\n",
        "    captured_activations_for_run.clear()\n",
        "\n",
        "    print(f\"Running model with capture hook at '{hook_point_mlp_out}' and modify hook at '{hook_point_resid_post}'\")\n",
        "    skip_ablated_logits = model.run_with_hooks(\n",
        "        test_input_tokens,\n",
        "        fwd_hooks=[\n",
        "            (hook_point_mlp_out, capture_mlp_out_hook),\n",
        "            (hook_point_resid_post, replace_resid_post_hook)\n",
        "        ]\n",
        "    )\n",
        "    print(\"Model run with hooks completed.\")\n",
        "\n",
        "    # Calculate original logits\n",
        "    original_logits = model(test_input_tokens)\n",
        "\n",
        "# --- Calculate Accuracy AND Loss ---\n",
        "\n",
        "# Original Performance\n",
        "original_accuracy, orig_correct, orig_total = calculate_accuracy(original_logits, test_labels)\n",
        "# Select logits for the prediction position (last token)\n",
        "original_pred_logits = original_logits[:, -1, :]\n",
        "original_loss = loss_fn(original_pred_logits, test_labels)\n",
        "\n",
        "# Ablated Performance\n",
        "skip_ablated_accuracy, skip_ablated_correct, skip_ablated_total = calculate_accuracy(skip_ablated_logits, test_labels)\n",
        "# Select logits for the prediction position (last token)\n",
        "skip_ablated_pred_logits = skip_ablated_logits[:, -1, :]\n",
        "skip_ablated_loss = loss_fn(skip_ablated_pred_logits, test_labels)\n",
        "\n",
        "\n",
        "# --- Print Results ---\n",
        "print(f\"\\nOriginal Model Performance:\")\n",
        "print(f\"  Accuracy: {original_accuracy:.4f} ({orig_correct}/{orig_total})\")\n",
        "print(f\"  Loss:     {original_loss.item():.6f}\") # Use .item() to get scalar value\n",
        "\n",
        "print(f\"\\nSkip Conn Ablated Accuracy (MLP Only):\")\n",
        "print(f\"  Accuracy: {skip_ablated_accuracy:.4f} ({skip_ablated_correct}/{skip_ablated_total})\")\n",
        "print(f\"  Loss:     {skip_ablated_loss.item():.6f}\")\n",
        "\n",
        "print(f\"\\nAccuracy Drop due to Skip Ablation: {original_accuracy - skip_ablated_accuracy:.4f}\")\n",
        "print(f\"Loss Change due to Skip Ablation:   {skip_ablated_loss.item() - original_loss.item():.6f}\")\n",
        "\n",
        "# Clear the dictionary\n",
        "captured_activations_for_run.clear()"
      ],
      "metadata": {
        "id": "AHhDW0QT-HRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss increases by a few orders of magnitude, but in absolute terms is still small. I'd say this verifies our findings.\n",
        "\n",
        "We are now going to look at the difference in logits when comparing the full trained model and the model in which we have ablated away the second PCA component (the parabolic one) in the embedded representation. We hope this sheds light on what it is that this 2nd component is doing."
      ],
      "metadata": {
        "id": "vLGuQFEq_Fu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Get Original and Ablated Logits ---\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Original logits\n",
        "    original_logits = model(test_input_tokens) # Shape: [num_samples, seq_len, d_vocab]\n",
        "\n",
        "    # Ablated logits (PC2 removed from embeddings)\n",
        "    ablated_logits = model.run_with_hooks(\n",
        "        test_input_tokens,\n",
        "        fwd_hooks=[(embedding_hook_point, ablate_pc2_hook)]\n",
        "    )\n",
        "\n",
        "# --- Calculate Logit Difference ---\n",
        "logit_diff = original_logits - ablated_logits # Shape: [num_samples, seq_len, d_vocab]\n",
        "\n",
        "# Focus on the logits at the prediction position (last token)\n",
        "logit_diff_pred_pos = logit_diff[:, -1, :].cpu().numpy() # Shape: [num_samples, d_vocab]\n",
        "original_logits_pred_pos = original_logits[:, -1, :].cpu().numpy()\n",
        "ablated_logits_pred_pos = ablated_logits[:, -1, :].cpu().numpy()\n",
        "\n",
        "test_labels_np = test_labels.cpu().numpy()\n",
        "\n",
        "# --- Analyze the Difference ---\n",
        "results = []\n",
        "original_a_values = test_input_tokens[:, 0].cpu().numpy()\n",
        "original_b_values = test_input_tokens[:, 1].cpu().numpy()\n",
        "ablated_predicted_tokens = np.argmax(ablated_logits_pred_pos, axis=-1)\n",
        "\n",
        "for i in range(len(test_labels_np)):\n",
        "    a = original_a_values[i]\n",
        "    b = original_b_values[i]\n",
        "    true_label = test_labels_np[i]\n",
        "    ablated_pred = ablated_predicted_tokens[i]\n",
        "\n",
        "    # Difference for the TRUE label's logit\n",
        "    diff_for_true_label = logit_diff_pred_pos[i, true_label]\n",
        "\n",
        "    # Difference for the ABLATED MODEL'S PREDICTED label's logit\n",
        "    diff_for_ablated_pred = logit_diff_pred_pos[i, ablated_pred]\n",
        "\n",
        "    original_correct_logit = original_logits_pred_pos[i, true_label]\n",
        "    ablated_correct_logit = ablated_logits_pred_pos[i, true_label]\n",
        "\n",
        "    results.append({\n",
        "        'a': a,\n",
        "        'b': b,\n",
        "        'True Sum': true_label,\n",
        "        'Ablated Pred': ablated_pred,\n",
        "        'LogitDiff (True)': diff_for_true_label,\n",
        "        'LogitDiff (AblatedPred)': diff_for_ablated_pred,\n",
        "        'OrigLogit (True)': original_correct_logit,\n",
        "        'AblatedLogit (True)': ablated_correct_logit,\n",
        "        'Failed': true_label != ablated_pred\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# --- Display Key Information ---\n",
        "print(\"Logit Difference Analysis (Original - Ablated):\")\n",
        "print(\"Positive 'LogitDiff (True)' means PC2 boosted the correct logit.\")\n",
        "print(\"Negative 'LogitDiff (AblatedPred)' for failures means PC2 suppressed the wrongly predicted logit.\")\n",
        "\n",
        "# Show stats for the change in the correct logit\n",
        "print(\"\\nStatistics for Logit Difference of the CORRECT sum:\")\n",
        "print(results_df['LogitDiff (True)'].describe())\n",
        "\n",
        "# Show failure cases focusing on logit changes\n",
        "failures_df = results_df[results_df['Failed']].sort_values(by=['True Sum', 'a', 'b'])\n",
        "print(f\"\\nAnalysis of {len(failures_df)} Failure Cases:\")\n",
        "if not failures_df.empty:\n",
        "    print(failures_df[['a', 'b', 'True Sum', 'Ablated Pred', 'LogitDiff (True)', 'LogitDiff (AblatedPred)', 'OrigLogit (True)', 'AblatedLogit (True)']].to_string())\n",
        "\n",
        "# --- Visualize LogitDiff for the Correct Label ---\n",
        "logit_diff_heatmap = np.full((N, N), np.nan)\n",
        "for i in range(len(results_df)):\n",
        "    row = results_df.iloc[i]\n",
        "    logit_diff_heatmap[row['a'], row['b']] = row['LogitDiff (True)']\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "max_abs_val = np.nanmax(np.abs(logit_diff_heatmap))\n",
        "sns.heatmap(logit_diff_heatmap, cmap=\"coolwarm\", center=0,\n",
        "            annot=True, fmt=\".2f\", linewidths=.5, linecolor='gray',\n",
        "            vmin=-max_abs_val, vmax=max_abs_val, # Center colorbar at 0\n",
        "            mask=np.isnan(logit_diff_heatmap), square=True)\n",
        "plt.title(\"Logit Difference (Orig - Ablated) for CORRECT Sum\")\n",
        "plt.xlabel(\"Input b = F(n-1)\")\n",
        "plt.ylabel(\"Input a = F(n-2)\")\n",
        "plt.xticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.yticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OseV7BsYFW4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PC2 isn't just \"fine-tuning\". It's part of a complex encoding strategy interpreted by the Unembedding layer (W_U).\n",
        "\n",
        "It acts as a strong identifier for \"double-extreme\" inputs ((0,0), (9,9)), massively boosting their correct logits.\n",
        "\n",
        "It acts differently for \"single-extreme\" inputs ((0,k), (k,9)), where its net effect might even reduce the correct logit slightly, but likely plays a vital role in suppressing competitors.\n",
        "\n",
        "It provides a moderate general boost for central inputs.\n",
        "\n",
        "Let us now visualize by how much the ablated model is wrong, rather than just the fact that it is wrong."
      ],
      "metadata": {
        "id": "OifaSNWnHKDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_error_heatmap = np.full((N, N), np.nan)\n",
        "\n",
        "num_samples = len(test_labels_np)\n",
        "prediction_errors = np.zeros(num_samples) # Store errors for potential stats\n",
        "\n",
        "for i in range(num_samples):\n",
        "    a_val = original_a_values[i]\n",
        "    b_val = original_b_values[i]\n",
        "    ablated_pred = ablated_predicted_tokens[i]\n",
        "    true_label = test_labels_np[i]\n",
        "\n",
        "    # Calculate the arithmetic difference\n",
        "    error = float(ablated_pred - true_label) # Ensure float for calculations\n",
        "    prediction_errors[i] = error\n",
        "\n",
        "    # Place the error in the heatmap\n",
        "    prediction_error_heatmap[a_val, b_val] = error\n",
        "\n",
        "# --- Visualize Prediction Error Heatmap ---\n",
        "plt.figure(figsize=(7, 6))\n",
        "\n",
        "# Find the max absolute error for centering the color bar correctly\n",
        "max_abs_error = np.nanmax(np.abs(prediction_error_heatmap))\n",
        "# Handle case where there are no errors or all NaNs\n",
        "if np.isnan(max_abs_error):\n",
        "    max_abs_error = 1.0 # Default if no errors\n",
        "\n",
        "# Use a diverging colormap since error can be positive or negative\n",
        "cmap = \"coolwarm\" # Red for positive error (prediction > true), Blue for negative\n",
        "\n",
        "sns.heatmap(prediction_error_heatmap, cmap=cmap, center=0,\n",
        "            annot=True, fmt=\".0f\", # Show integer errors\n",
        "            linewidths=.5, linecolor='lightgray',\n",
        "            square=True,\n",
        "            vmin=-max_abs_error, vmax=max_abs_error, # Center color scale around 0\n",
        "            cbar_kws={'label': 'Ablated Prediction - True Sum'},\n",
        "            mask=np.isnan(prediction_error_heatmap)) # Mask cells not in test data\n",
        "\n",
        "plt.title(\"Prediction Error of Ablated Model (PC2 Removed)\")\n",
        "plt.xlabel(\"Input b = F(n-1)\")\n",
        "plt.ylabel(\"Input a = F(n-2)\")\n",
        "plt.xticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.yticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# --- Optional: Display Error Statistics ---\n",
        "print(\"\\nStatistics for Prediction Error (Ablated Pred - True Sum):\")\n",
        "# Create a temporary series excluding NaNs if your test set wasn't exhaustive\n",
        "valid_errors = prediction_errors[~np.isnan(prediction_errors)]\n",
        "if len(valid_errors) > 0:\n",
        "    error_series = pd.Series(valid_errors)\n",
        "    print(error_series.describe())\n",
        "    print(f\"\\nNumber of incorrect predictions (error != 0): {np.sum(valid_errors != 0)}\")\n",
        "    print(f\"Number of correct predictions (error == 0): {np.sum(valid_errors == 0)}\")\n",
        "else:\n",
        "    print(\"No valid error data found (check test set population).\")"
      ],
      "metadata": {
        "id": "QJ7T2laLgXo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some observations: It's never wrong by more than 3. And is most often wrong by 1. It's off by 3 only at 0,0 and 9,9 -- it predicts 3 for 0,0 and 15 for 9,9. It's off by positive values when at least one of the inputs is 0, off by negative values when at least one of the inputs is 9. Exception made to 0,9 and 9,0, where it is correct -- suggesting that two wrongs make a right. Further, for low-ish values of a,b it underpredicts (off by negative), for high-ish values of a,b it overpredicts (off by positive).\n",
        "\n",
        "Can we conclude anything about whatever algorithm it's implementing based on this? Let's fit the predictions of the ablated model to a linear function of the sum."
      ],
      "metadata": {
        "id": "tRvUNLDsg3sK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fitting ablated predictions to a linear function of the true sum...\")\n",
        "\n",
        "# --- Prepare Data for Linear Regression ---\n",
        "# X: Independent variable (True Sum)\n",
        "# Needs to be a 2D array for scikit-learn, even with one feature\n",
        "X_true_sum = test_labels_np.reshape(-1, 1)\n",
        "\n",
        "# y: Dependent variable (Ablated Prediction)\n",
        "y_ablated_pred = ablated_predicted_tokens\n",
        "\n",
        "# --- Fit Linear Regression Model ---\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_true_sum, y_ablated_pred)\n",
        "\n",
        "# --- Get Results ---\n",
        "# Slope (m)\n",
        "learned_m = lin_reg.coef_[0]\n",
        "# Intercept (c)\n",
        "learned_c = lin_reg.intercept_\n",
        "# Predictions from the fitted linear model\n",
        "y_linear_fit_pred = lin_reg.predict(X_true_sum)\n",
        "\n",
        "# --- Evaluate the Fit ---\n",
        "# R-squared score\n",
        "r2 = r2_score(y_ablated_pred, y_linear_fit_pred)\n",
        "# Alternatively: r2 = lin_reg.score(X_true_sum, y_ablated_pred)\n",
        "# Mean Squared Error\n",
        "mse = mean_squared_error(y_ablated_pred, y_linear_fit_pred)\n",
        "\n",
        "# --- Print Results and Comparison ---\n",
        "print(\"\\n--- Linear Fit Results: AblatedPred = m * TrueSum + c ---\")\n",
        "print(f\"Learned Slope (m): {learned_m:.4f}\")\n",
        "print(f\"Learned Intercept (c): {learned_c:.4f}\")\n",
        "print(f\"R-squared score: {r2:.4f}\")\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "# Compare to hypothesized values\n",
        "hypothesized_m = 2/3\n",
        "hypothesized_c = 3\n",
        "print(\"\\nComparison to Hypothesis (m = 2/3, c = 3):\")\n",
        "print(f\"Hypothesized m: {hypothesized_m:.4f}\")\n",
        "print(f\"Hypothesized c: {hypothesized_c:.4f}\")\n",
        "print(f\"Difference in m: {learned_m - hypothesized_m:.4f}\")\n",
        "print(f\"Difference in c: {learned_c - hypothesized_c:.4f}\")\n",
        "\n",
        "# --- Visualize the Fit ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(test_labels_np, y_ablated_pred, alpha=0.5, label='Actual Ablated Predictions')\n",
        "plt.plot(test_labels_np, y_linear_fit_pred, color='red', linewidth=2, label=f'Linear Fit (R²={r2:.3f})')\n",
        "# Plot the hypothesized line\n",
        "hypothesized_line = hypothesized_m * test_labels_np + hypothesized_c\n",
        "plt.plot(test_labels_np, hypothesized_line, color='green', linestyle='--', linewidth=2, label='Hypothesized Line (m=2/3, c=3)')\n",
        "# Plot the ideal line (y=x) for reference\n",
        "plt.plot(test_labels_np, test_labels_np, color='gray', linestyle=':', linewidth=1, label='Ideal (y=x)')\n",
        "\n",
        "\n",
        "plt.title('Linear Fit: Ablated Prediction vs. True Sum')\n",
        "plt.xlabel('True Sum (a+b)')\n",
        "plt.ylabel('Ablated Model Prediction')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "# Ensure axes cover the full possible range (0 to max possible sum, e.g., 18)\n",
        "max_sum = np.max(test_labels_np) if len(test_labels_np) > 0 else 18\n",
        "max_pred = np.max(y_ablated_pred) if len(y_ablated_pred) > 0 else max_sum\n",
        "plot_max = max(max_sum, max_pred)\n",
        "plt.xlim(-0.5, plot_max + 0.5)\n",
        "plt.ylim(-0.5, plot_max + 0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yxWJPZTsi5FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: the ablated model makes different predictions for some combinations of inputs which are different but have the same sum, which is why there are multiple blue dots on the same vertical line in the plot above. This necessarily implies that we cannot perfectly fit the ablated model's predictions to a function of the sum, we need to consider a and b separately."
      ],
      "metadata": {
        "id": "ju8Rz1zVjd07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFitting ablated predictions to a linear function of a and b separately...\")\n",
        "\n",
        "# --- Prepare Data for Multiple Linear Regression ---\n",
        "# X: Independent variables (a, b)\n",
        "# Create a 2D array where columns are 'a' and 'b'\n",
        "X_inputs_ab = np.stack((original_a_values, original_b_values), axis=-1) # Shape: [num_samples, 2]\n",
        "\n",
        "# y: Dependent variable (Ablated Prediction)\n",
        "y_ablated_pred = ablated_predicted_tokens\n",
        "\n",
        "# --- Fit Linear Regression Model ---\n",
        "multi_lin_reg = LinearRegression()\n",
        "multi_lin_reg.fit(X_inputs_ab, y_ablated_pred)\n",
        "\n",
        "# --- Get Results ---\n",
        "# Coefficients (w_a, w_b)\n",
        "learned_wa, learned_wb = multi_lin_reg.coef_\n",
        "# Intercept (c)\n",
        "learned_c_multi = multi_lin_reg.intercept_\n",
        "# Predictions from the fitted linear model\n",
        "y_multilinear_fit_pred = multi_lin_reg.predict(X_inputs_ab)\n",
        "\n",
        "# --- Evaluate the Fit ---\n",
        "r2_multi = r2_score(y_ablated_pred, y_multilinear_fit_pred)\n",
        "mse_multi = mean_squared_error(y_ablated_pred, y_multilinear_fit_pred)\n",
        "\n",
        "# --- Print Results ---\n",
        "print(\"\\n--- Multiple Linear Fit Results: AblatedPred = wa*a + wb*b + c ---\")\n",
        "print(f\"Learned Weight for a (wa): {learned_wa:.4f}\")\n",
        "print(f\"Learned Weight for b (wb): {learned_wb:.4f}\")\n",
        "print(f\"Learned Intercept (c): {learned_c_multi:.4f}\")\n",
        "print(f\"R-squared score: {r2_multi:.4f}\")\n",
        "print(f\"Mean Squared Error: {mse_multi:.4f}\")\n",
        "\n",
        "# --- Compare with previous single-variable fit ---\n",
        "# Assuming 'r2', 'learned_m', 'learned_c' from the previous fit exist\n",
        "if 'r2' in locals():\n",
        "    print(f\"\\nComparison to previous fit (AblatedPred = m*(a+b) + c):\")\n",
        "    print(f\"  Previous R-squared: {r2:.4f}\")\n",
        "    print(f\"  Improvement in R-squared: {r2_multi - r2:.4f}\")\n",
        "    print(f\"  Previous m: {learned_m:.4f} (Avg of wa, wb = {(learned_wa + learned_wb)/2:.4f})\")\n",
        "    print(f\"  Previous c: {learned_c:.4f} (Current c = {learned_c_multi:.4f})\")\n",
        "\n",
        "# --- Optional: Visualize Residuals (If R-squared isn't close to 1) ---\n",
        "residuals = y_ablated_pred - y_multilinear_fit_pred\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_multilinear_fit_pred, residuals, alpha=0.5)\n",
        "plt.hlines(0, xmin=min(y_multilinear_fit_pred), xmax=max(y_multilinear_fit_pred), colors='red', linestyles='--')\n",
        "plt.title('Residual Plot for Multi-Linear Fit')\n",
        "plt.xlabel('Fitted Values (wa*a + wb*b + c)')\n",
        "plt.ylabel('Residuals (Actual - Fitted)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# --- Optional: Heatmap of Residuals ---\n",
        "residual_heatmap = np.full((N, N), np.nan)\n",
        "fitted_values_heatmap = np.full((N, N), np.nan)\n",
        "num_samples = len(y_ablated_pred)\n",
        "\n",
        "for i in range(num_samples):\n",
        "    a_val = original_a_values[i]\n",
        "    b_val = original_b_values[i]\n",
        "    residual_heatmap[a_val, b_val] = residuals[i]\n",
        "    fitted_values_heatmap[a_val, b_val] = y_multilinear_fit_pred[i] # Store fitted value\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "max_abs_resid = np.nanmax(np.abs(residual_heatmap))\n",
        "if np.isnan(max_abs_resid): max_abs_resid = 1.0\n",
        "\n",
        "cmap_resid = \"coolwarm\"\n",
        "sns.heatmap(residual_heatmap, cmap=cmap_resid, center=0,\n",
        "            annot=True, fmt=\".2f\",\n",
        "            linewidths=.5, linecolor='lightgray',\n",
        "            square=True,\n",
        "            vmin=-max_abs_resid, vmax=max_abs_resid,\n",
        "            cbar_kws={'label': 'Residual (Actual Ablated - Fitted)'},\n",
        "            mask=np.isnan(residual_heatmap))\n",
        "plt.title(\"Residuals of Multi-Linear Fit (wa*a + wb*b + c)\")\n",
        "plt.xlabel(\"Input b\")\n",
        "plt.ylabel(\"Input a\")\n",
        "plt.xticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.yticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pPwQvcX7kRyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fit to a and b individually is the same as the fit to a+b. Hence, the dependence of the ablated model's prediction depends linearly on the inputs only via their sum. Given that the fit is not perfect (R^2 of ~0.93, not 1), this indicates that there is some non-linear component contributing to the model's output. We think this is introduced by the ReLU, as we see no other sources of non-linearity.\n",
        "\n",
        "Hypothesis: note that the desired behavior is linear, i.e., just sum the inputs. The complete model implements this, albeit with a PC2 component which is non-linear. The ablated model that does not have the PC2 component fails to implement the linear behavior, and errs. Hence, it might be that the PC2 component's task is to cancel out the wrong non-linearity that the rest of the model appears to learn. We can test this by fitting the error in the ablated model's prediction to the value of the PC2 component for the same inputs."
      ],
      "metadata": {
        "id": "rPalwBgclldh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures # For potential non-linear fit\n",
        "\n",
        "# --- 1. Extract PC2 Features using EXISTING pca_scores ---\n",
        "print(\"Extracting PC2 features from existing pca_scores...\")\n",
        "try:\n",
        "    # Ensure pca_scores is available and is a tensor\n",
        "    if 'pca_scores' not in locals() or not isinstance(pca_scores, torch.Tensor):\n",
        "         raise NameError(\"Variable 'pca_scores' not found or is not a PyTorch tensor.\")\n",
        "\n",
        "    # Convert scores to numpy ONCE for use with sklearn and indexing\n",
        "    pca_scores_np = pca_scores.detach().cpu().numpy()\n",
        "    # PC2 values for all tokens in the vocabulary (index 1 is the second PC)\n",
        "    if pca_scores_np.shape[1] < 2:\n",
        "         raise ValueError(f\"pca_scores_np has shape {pca_scores_np.shape}, but need at least 2 components to extract PC2.\")\n",
        "    pc2_values_all_tokens = pca_scores_np[:, 1]\n",
        "\n",
        "    # Get PC2 value for input 'a' and 'b' for each sample in the test set\n",
        "    pc2_a_features = pc2_values_all_tokens[original_a_values]\n",
        "    pc2_b_features = pc2_values_all_tokens[original_b_values]\n",
        "\n",
        "    # Create the feature matrix for regression\n",
        "    X_pc2_features = np.stack((pc2_a_features, pc2_b_features), axis=-1) # Shape: [num_samples, 2]\n",
        "    pc2_extracted_successfully = True\n",
        "    print(\"PC2 features extracted successfully.\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Please ensure 'pca_scores' (Tensor) is computed and available.\")\n",
        "    pc2_extracted_successfully = False\n",
        "except IndexError as e:\n",
        "     print(f\"Error indexing PC2 values. Check token indices range (0-{N-1}) vs vocab size used in PCA: {e}\")\n",
        "     pc2_extracted_successfully = False\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during PC2 feature extraction: {e}\")\n",
        "    pc2_extracted_successfully = False\n",
        "print(pc2_b_features)\n",
        "# --- Proceed only if PC2 features were extracted successfully ---\n",
        "if pc2_extracted_successfully:\n",
        "    print(\"\\nFitting Ablated Model Error to PC2 Features...\")\n",
        "\n",
        "    # --- 2. Calculate the Error ---\n",
        "    y_error = test_labels_np - ablated_predicted_tokens # Correction needed\n",
        "\n",
        "    # --- 3. Perform Linear Regression ---\n",
        "    error_reg = LinearRegression()\n",
        "    error_reg.fit(X_pc2_features, y_error)\n",
        "\n",
        "    # --- Get Results ---\n",
        "    learned_w_pc2a, learned_w_pc2b = error_reg.coef_\n",
        "    learned_c_error = error_reg.intercept_\n",
        "    y_error_pred = error_reg.predict(X_pc2_features)\n",
        "\n",
        "    # --- Evaluate Fit ---\n",
        "    r2_error_fit = r2_score(y_error, y_error_pred)\n",
        "    mse_error_fit = mean_squared_error(y_error, y_error_pred)\n",
        "\n",
        "    print(\"\\n--- Linear Fit Results: Error ≈ w_pc2a*PC2(a) + w_pc2b*PC2(b) + c ---\")\n",
        "    print(f\"Learned Weight for PC2(a): {learned_w_pc2a:.4f}\")\n",
        "    print(f\"Learned Weight for PC2(b): {learned_w_pc2b:.4f}\")\n",
        "    print(f\"Learned Intercept: {learned_c_error:.4f}\")\n",
        "    print(f\"R-squared score: {r2_error_fit:.4f}\")\n",
        "    print(f\"Mean Squared Error: {mse_error_fit:.4f}\")\n",
        "\n",
        "    # --- Optional: Try Polynomial Regression (Degree 2) ---\n",
        "    print(\"\\n--- Trying Polynomial Fit (Degree 2) ---\")\n",
        "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "    # Create interaction & quadratic features: [pc2a, pc2b, pc2a^2, pc2a*pc2b, pc2b^2]\n",
        "    X_pc2_poly_features = poly.fit_transform(X_pc2_features)\n",
        "\n",
        "    error_poly_reg = LinearRegression()\n",
        "    error_poly_reg.fit(X_pc2_poly_features, y_error)\n",
        "    y_error_poly_pred = error_poly_reg.predict(X_pc2_poly_features)\n",
        "    r2_error_poly_fit = r2_score(y_error, y_error_poly_pred)\n",
        "    mse_error_poly_fit = mean_squared_error(y_error, y_error_poly_pred)\n",
        "\n",
        "    print(f\"Polynomial Fit R-squared score: {r2_error_poly_fit:.4f}\")\n",
        "    print(f\"Polynomial Fit Mean Squared Error: {mse_error_poly_fit:.4f}\")\n",
        "    print(f\"Improvement in R-squared: {r2_error_poly_fit - r2_error_fit:.4f}\")\n",
        "\n",
        "\n",
        "    # --- Visualize Error vs Predicted Error ---\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_error, y_error_pred, alpha=0.5, label=f'Linear Fit (R²={r2_error_fit:.3f})')\n",
        "    # Only plot polynomial fit if it's meaningfully better\n",
        "    if r2_error_poly_fit > r2_error_fit + 0.01:\n",
        "         plt.scatter(y_error, y_error_poly_pred, alpha=0.5, marker='x', label=f'Poly Fit (R²={r2_error_poly_fit:.3f})')\n",
        "    # Ideal line y=x\n",
        "    min_val = min(np.min(y_error), np.min(y_error_pred)) - 0.5 # Add buffer\n",
        "    max_val = max(np.max(y_error), np.max(y_error_pred)) + 0.5 # Add buffer\n",
        "    #plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Ideal Fit (y=x)')\n",
        "    plt.title('Predicting Ablated Model Error using PC2 Features')\n",
        "    plt.xlabel('Actual Error (True Sum - Ablated Pred)')\n",
        "    plt.ylabel('Predicted Error (from PC2 features)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping error prediction analysis due to issues extracting PC2 features.\")"
      ],
      "metadata": {
        "id": "mN_I8gaImzql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "d_vocab = N # Or N+1 if your output includes a special token mapped within 0..N\n",
        "d_mlp = model.cfg.d_mlp # Should be 512\n",
        "num_key_neurons = 100 # Analyze the top 100 correlated neurons\n",
        "key_neuron_indices = indices_sorted_by_abs_corr[:num_key_neurons]\n",
        "\n",
        "# --- Step 1: Calculate W_L = W_U.T @ W_out.T ---\n",
        "\n",
        "# Get the weight matrices\n",
        "try:\n",
        "    # Assume stored as [d_model, d_vocab]\n",
        "    W_U_stored = model.unembed.W_U.detach().cpu()\n",
        "    W_U = W_U_stored.T # Transpose to get [d_vocab, d_model]\n",
        "except AttributeError:\n",
        "    # Assume stored as [d_model, d_vocab]\n",
        "    W_U_stored = model.W_U.detach().cpu()\n",
        "    W_U = W_U_stored.T # Transpose to get [d_vocab, d_model]\n",
        "\n",
        "# Assume stored as [d_mlp, d_model]\n",
        "W_out_stored = model.blocks[0].mlp.W_out.detach().cpu()\n",
        "W_out = W_out_stored.T # Transpose to get [d_model, d_mlp]\n",
        "\n",
        "# Calculate the effective neuron->logit map\n",
        "print(f\"Multiplying W_U shape {W_U.shape} with W_out shape {W_out.shape}\")\n",
        "# This should now be [d_vocab, d_model] @ [d_model, d_mlp]\n",
        "W_L = W_U @ W_out # Shape: [d_vocab, d_mlp]\n",
        "W_L_np = W_L.numpy()\n",
        "\n",
        "print(f\"Calculated W_L with shape: {W_L_np.shape}\") # Should be (d_vocab, d_mlp)\n",
        "\n",
        "# --- Step 2: Analyze Rows of W_L (Code remains the same) ---\n",
        "# ... (rest of the analysis code) ...\n",
        "\n",
        "print(f\"Calculated W_L with shape: {W_L_np.shape}\") # Should be (d_vocab, d_mlp), e.g., (10, 512)\n",
        "\n",
        "# --- Step 2: Analyze Rows of W_L and Neuron Contributions ---\n",
        "\n",
        "print(f\"\\nAnalyzing weights in W_L for top {num_key_neurons} sum-correlated neurons:\")\n",
        "\n",
        "analysis_results = []\n",
        "\n",
        "for k in range(d_vocab): # Iterate through each output logit (0 to N-1)\n",
        "    # Get the weights from W_L for this specific logit 'k'\n",
        "    # applied to the key neurons\n",
        "    weights_for_key_neurons = W_L_np[k, key_neuron_indices] # Shape: [num_key_neurons]\n",
        "\n",
        "    # Hypothesis Check: Contribution to correct logit\n",
        "    positivity_ratio = np.mean(weights_for_key_neurons > 0)\n",
        "\n",
        "    # Redundancy Check: Distribution statistics\n",
        "    mean_weight = np.mean(weights_for_key_neurons)\n",
        "    std_weight = np.std(weights_for_key_neurons)\n",
        "    max_abs_weight = np.max(np.abs(weights_for_key_neurons))\n",
        "    # Check sparsity: % of weights close to zero (e.g., < 1% of max abs weight)\n",
        "    sparsity_threshold = 0.01 * max_abs_weight\n",
        "    sparsity_ratio = np.mean(np.abs(weights_for_key_neurons) < sparsity_threshold)\n",
        "\n",
        "\n",
        "    print(f\"\\n--- Logit for output token k={k} ---\")\n",
        "    print(f\"  Positivity Ratio (Key Neurons): {positivity_ratio:.2f}\")\n",
        "    print(f\"  Sparsity Ratio (<{sparsity_threshold:.4f}): {sparsity_ratio:.2f}\")\n",
        "    print(f\"  Weight Stats (Key Neurons): Mean={mean_weight:.4f}, Std={std_weight:.4f}, MaxAbs={max_abs_weight:.4f}\")\n",
        "\n",
        "    analysis_results.append({\n",
        "        'k': k,\n",
        "        'positivity_ratio': positivity_ratio,\n",
        "        'sparsity_ratio': sparsity_ratio,\n",
        "        'mean_weight': mean_weight,\n",
        "        'std_weight': std_weight,\n",
        "        'max_abs_weight': max_abs_weight\n",
        "    })\n",
        "\n",
        "# Optional: Create a DataFrame for easier viewing\n",
        "analysis_df = pd.DataFrame(analysis_results)\n",
        "print(\"\\n--- Summary Table ---\")\n",
        "print(analysis_df.round(3))\n",
        "\n",
        "# Optional: Plot histogram for a specific k (e.g., k=5)\n",
        "plt.figure(figsize=(8, 4))\n",
        "k_to_plot = 5\n",
        "plt.hist(W_L_np[k_to_plot, key_neuron_indices], bins=30, alpha=0.7)\n",
        "plt.title(f\"Weight Distribution in W_L[{k_to_plot}, :] for Top {num_key_neurons} Neurons\")\n",
        "plt.xlabel(\"Weight Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(axis='y', alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "upD5rJX9KE3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That didn't work! Let's try and fit the output of the ablated model to a quadratic function of the inputs."
      ],
      "metadata": {
        "id": "OQr_D2PUu6JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFitting ablated predictions to quadratic functions of a and b...\")\n",
        "\n",
        "# --- Prepare Data ---\n",
        "# y: Dependent variable (Ablated Prediction)\n",
        "y_ablated_pred = ablated_predicted_tokens\n",
        "\n",
        "# --- Fit 1: Requested Quadratic Model: x*(a+b) + y*a^2 + z*b^2 + c ---\n",
        "print(\"\\n--- Fitting Requested Model: x*(a+b) + y*a^2 + z*b^2 + c ---\")\n",
        "\n",
        "# Construct Features: [a+b, a^2, b^2]\n",
        "sum_ab = original_a_values + original_b_values\n",
        "a_squared = original_a_values**2\n",
        "b_squared = original_b_values**2\n",
        "X_quadratic_requested = np.stack((sum_ab, a_squared, b_squared), axis=-1) # Shape: [num_samples, 3]\n",
        "\n",
        "# Fit Linear Regression Model\n",
        "quad_reg_req = LinearRegression()\n",
        "quad_reg_req.fit(X_quadratic_requested, y_ablated_pred)\n",
        "\n",
        "# Get Results\n",
        "learned_x, learned_y, learned_z = quad_reg_req.coef_\n",
        "learned_c_req = quad_reg_req.intercept_\n",
        "y_quad_req_pred = quad_reg_req.predict(X_quadratic_requested)\n",
        "\n",
        "# Evaluate Fit\n",
        "r2_quad_req = r2_score(y_ablated_pred, y_quad_req_pred)\n",
        "mse_quad_req = mean_squared_error(y_ablated_pred, y_quad_req_pred)\n",
        "\n",
        "print(f\"Learned Coefficient for (a+b) (x): {learned_x:.4f}\")\n",
        "print(f\"Learned Coefficient for a^2 (y):   {learned_y:.4f}\")\n",
        "print(f\"Learned Coefficient for b^2 (z):   {learned_z:.4f}\")\n",
        "print(f\"Learned Intercept (c):             {learned_c_req:.4f}\")\n",
        "print(f\"R-squared score:                   {r2_quad_req:.4f}\")\n",
        "print(f\"Mean Squared Error:                {mse_quad_req:.4f}\")\n",
        "\n",
        "\n",
        "# --- Fit 2: General Quadratic Model using PolynomialFeatures ---\n",
        "print(\"\\n--- Fitting General Quadratic Model (includes a*b term) ---\")\n",
        "\n",
        "# Create polynomial features (degree 2) from [a, b]\n",
        "# Includes: a, b, a^2, a*b, b^2 (bias term is handled by LinearRegression)\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_inputs_ab = np.stack((original_a_values, original_b_values), axis=-1) # Shape: [num_samples, 2]\n",
        "X_quadratic_general = poly.fit_transform(X_inputs_ab)\n",
        "# Get feature names for clarity (order might vary slightly by sklearn version)\n",
        "feature_names = poly.get_feature_names_out(['a', 'b'])\n",
        "\n",
        "\n",
        "# Fit Linear Regression Model\n",
        "quad_reg_gen = LinearRegression()\n",
        "quad_reg_gen.fit(X_quadratic_general, y_ablated_pred)\n",
        "\n",
        "# Get Results\n",
        "learned_coeffs_gen = quad_reg_gen.coef_\n",
        "learned_c_gen = quad_reg_gen.intercept_\n",
        "y_quad_gen_pred = quad_reg_gen.predict(X_quadratic_general)\n",
        "\n",
        "# Evaluate Fit\n",
        "r2_quad_gen = r2_score(y_ablated_pred, y_quad_gen_pred)\n",
        "mse_quad_gen = mean_squared_error(y_ablated_pred, y_quad_gen_pred)\n",
        "\n",
        "print(\"General Quadratic Model: Pred ≈ w_a*a + w_b*b + w_aa*a^2 + w_ab*a*b + w_bb*b^2 + c_gen\")\n",
        "print(\"Learned Coefficients:\")\n",
        "for name, coeff in zip(feature_names, learned_coeffs_gen):\n",
        "    print(f\"  Weight for {name}: {coeff:.4f}\")\n",
        "print(f\"Learned Intercept (c_gen): {learned_c_gen:.4f}\")\n",
        "print(f\"R-squared score:           {r2_quad_gen:.4f}\")\n",
        "print(f\"Mean Squared Error:        {mse_quad_gen:.4f}\")\n",
        "\n",
        "\n",
        "# --- Compare Fits ---\n",
        "print(\"\\n--- R-squared Comparison ---\")\n",
        "if 'r2_multi' in locals(): # Check if previous linear fit results exist\n",
        "    print(f\"Linear Fit (wa*a + wb*b + c):       R² = {r2_multi:.4f}\")\n",
        "else:\n",
        "    print(\"Multi-linear fit results not available for comparison.\")\n",
        "print(f\"Requested Quadratic Fit:          R² = {r2_quad_req:.4f}\")\n",
        "print(f\"General Quadratic Fit (w/ a*b):   R² = {r2_quad_gen:.4f}\")\n",
        "\n",
        "\n",
        "# --- Visualize Residuals for the Best Quadratic Fit ---\n",
        "best_r2_quad = max(r2_quad_req, r2_quad_gen)\n",
        "print(f\"\\nVisualizing residuals for the best quadratic fit (R²={best_r2_quad:.4f})...\")\n",
        "if r2_quad_gen >= r2_quad_req:\n",
        "    residuals = y_ablated_pred - y_quad_gen_pred\n",
        "    fitted_values = y_quad_gen_pred\n",
        "    title_suffix = \"General Quadratic Fit\"\n",
        "else:\n",
        "    residuals = y_ablated_pred - y_quad_req_pred\n",
        "    fitted_values = y_quad_req_pred\n",
        "    title_suffix = \"Requested Quadratic Fit\"\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(fitted_values, residuals, alpha=0.5)\n",
        "plt.hlines(0, xmin=min(fitted_values), xmax=max(fitted_values), colors='red', linestyles='--')\n",
        "plt.title(f'Residual Plot for {title_suffix}')\n",
        "plt.xlabel('Fitted Values')\n",
        "plt.ylabel('Residuals (Actual Ablated - Fitted)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Optional: Heatmap of Residuals\n",
        "residual_heatmap = np.full((N, N), np.nan)\n",
        "num_samples = len(y_ablated_pred)\n",
        "for i in range(num_samples):\n",
        "    a_val = original_a_values[i]\n",
        "    b_val = original_b_values[i]\n",
        "    residual_heatmap[a_val, b_val] = residuals[i] # Use the correct residuals\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "max_abs_resid = np.nanmax(np.abs(residual_heatmap))\n",
        "if np.isnan(max_abs_resid) or max_abs_resid == 0: max_abs_resid = 1.0\n",
        "\n",
        "cmap_resid = \"coolwarm\"\n",
        "sns.heatmap(residual_heatmap, cmap=cmap_resid, center=0,\n",
        "            annot=True, fmt=\".2f\",\n",
        "            linewidths=.5, linecolor='lightgray',\n",
        "            square=True,\n",
        "            vmin=-max_abs_resid, vmax=max_abs_resid,\n",
        "            cbar_kws={'label': 'Residual (Actual Ablated - Fitted)'},\n",
        "            mask=np.isnan(residual_heatmap))\n",
        "plt.title(f\"Residuals of {title_suffix}\")\n",
        "plt.xlabel(\"Input b\")\n",
        "plt.ylabel(\"Input a\")\n",
        "plt.xticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.yticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NoUb4GyYvGYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately our hypothesis about quadratic PC2 term simply being a correction on top of the linear output was incorrect. Hence, its contribution must be more subtle. To try to understand it, we will now compare the activations of the neurons in the ablated and the full model, to see how removing the PC2 affects their firing."
      ],
      "metadata": {
        "id": "o9fJToyeyCYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Storage for Activations ---\n",
        "activation_store = {}\n",
        "\n",
        "def store_activation_hook(activation, hook, storage_key):\n",
        "    # Store activation at the final sequence position\n",
        "    # Clone and detach to prevent memory leaks and graph issues\n",
        "    activation_store[storage_key] = activation[:, -1, :].detach().clone()\n",
        "    # Return original activation to not interfere\n",
        "    return activation\n",
        "\n",
        "# --- Execute and Capture Activations ---\n",
        "a_mlp_full = None\n",
        "a_mlp_ablated = None\n",
        "mlp_activation_hook_point = utils.get_act_name(\"mlp_post\", 0)\n",
        "print(mlp_activation_hook_point)\n",
        "if mlp_activation_hook_point:\n",
        "    model.eval() # Ensure model is in evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # --- Run 1: Get Full Model Activations ---\n",
        "        print(\"Running model normally to capture full MLP activations...\")\n",
        "        activation_store.clear() # Clear previous storage\n",
        "        _ = model.run_with_hooks(\n",
        "            test_input_tokens,\n",
        "            fwd_hooks=[(mlp_activation_hook_point,\n",
        "                        lambda act, hook: store_activation_hook(act, hook, 'full'))]\n",
        "        )\n",
        "        if 'full' in activation_store:\n",
        "            a_mlp_full = activation_store['full'].cpu().numpy()\n",
        "            print(f\"Captured full activations, shape: {a_mlp_full.shape}\")\n",
        "        else:\n",
        "            print(\"Failed to capture full activations.\")\n",
        "\n",
        "        # --- Run 2: Get Ablated Model Activations ---\n",
        "        print(\"Running model with PC2 ablation to capture ablated MLP activations...\")\n",
        "        activation_store.clear() # Clear previous storage\n",
        "        _ = model.run_with_hooks(\n",
        "            test_input_tokens,\n",
        "            fwd_hooks=[(embedding_hook_point, ablate_pc2_hook), # Ablate PC2 first\n",
        "                       (mlp_activation_hook_point,\n",
        "                        lambda act, hook: store_activation_hook(act, hook, 'ablated'))]\n",
        "        )\n",
        "        if 'ablated' in activation_store:\n",
        "            a_mlp_ablated = activation_store['ablated'].cpu().numpy()\n",
        "            print(f\"Captured ablated activations, shape: {a_mlp_ablated.shape}\")\n",
        "        else:\n",
        "            print(\"Failed to capture ablated activations.\")\n",
        "\n",
        "\n",
        "# --- Calculate Difference and Analyze ---\n",
        "if a_mlp_full is not None and a_mlp_ablated is not None:\n",
        "    if a_mlp_full.shape == a_mlp_ablated.shape:\n",
        "        print(\"\\nCalculating activation differences...\")\n",
        "        delta_a_mlp = a_mlp_full - a_mlp_ablated # Shape: [num_samples, d_mlp]\n",
        "\n",
        "        # --- Analysis Focused on Failures ---\n",
        "        failure_mask = results_df['Failed'].values\n",
        "        num_failures = failure_mask.sum()\n",
        "\n",
        "        if num_failures > 0:\n",
        "            print(f\"Analyzing {num_failures} failure cases...\")\n",
        "            delta_a_mlp_failures = delta_a_mlp[failure_mask] # Shape: [num_failures, d_mlp]\n",
        "\n",
        "            # --- 1. Which neurons change the most on average during failure? ---\n",
        "            mean_abs_delta_failures = np.mean(np.abs(delta_a_mlp_failures), axis=0)\n",
        "            sorted_neuron_indices_by_delta = np.argsort(mean_abs_delta_failures)[::-1]\n",
        "\n",
        "            print(\"\\nTop 10 Neurons by Mean Absolute Activation Change During Failures:\")\n",
        "            for i in range(min(10, d_mlp)):\n",
        "                idx = sorted_neuron_indices_by_delta[i]\n",
        "                print(f\"  Neuron {idx}: Mean Abs Delta = {mean_abs_delta_failures[idx]:.4f} (Corr={neuron_sum_correlations[idx]:.3f})\")\n",
        "\n",
        "            # --- 2. Visualize Average Delta for ALL Failures ---\n",
        "            mean_delta_failures = np.mean(delta_a_mlp_failures, axis=0) # Shape: [d_mlp,]\n",
        "\n",
        "            # Sort by correlation group for visualization\n",
        "            corr_sorted_indices = np.argsort(neuron_sum_correlations)\n",
        "            mean_delta_failures_sorted = mean_delta_failures[corr_sorted_indices]\n",
        "            correlations_sorted = neuron_sum_correlations[corr_sorted_indices]\n",
        "\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            colors = ['red' if c < 0 else 'blue' for c in correlations_sorted]\n",
        "            plt.bar(range(d_mlp), mean_delta_failures_sorted, color=colors, width=1.0)\n",
        "            plt.xlabel(\"Neurons (Sorted by Correlation with Sum a+b)\")\n",
        "            plt.ylabel(\"Mean Activation Diff (Full - Ablated)\")\n",
        "            plt.title(f\"Mean MLP Activation Difference on Failure Cases ({num_failures} samples)\")\n",
        "            plt.xticks([]) # Too many neurons to label\n",
        "            plt.grid(axis='y', alpha=0.5)\n",
        "            # Add legend for color (optional, can be tricky for bar charts)\n",
        "            from matplotlib.lines import Line2D\n",
        "            legend_elements = [Line2D([0], [0], color='blue', lw=4, label='Positively Correlated Neurons'),\n",
        "                               Line2D([0], [0], color='red', lw=4, label='Negatively Correlated Neurons')]\n",
        "            plt.legend(handles=legend_elements)\n",
        "            plt.show()\n",
        "\n",
        "            # --- 3. Visualize Average Delta for Specific Failure Groups (Example: a=0 failures) ---\n",
        "            a0_failure_mask = failure_mask & (results_df['a'] == 0)\n",
        "            num_a0_failures = a0_failure_mask.sum()\n",
        "\n",
        "            if num_a0_failures > 0:\n",
        "                print(f\"\\nAnalyzing {num_a0_failures} failure cases where a=0...\")\n",
        "                delta_a_mlp_a0_failures = delta_a_mlp[a0_failure_mask]\n",
        "                mean_delta_a0_failures = np.mean(delta_a_mlp_a0_failures, axis=0)\n",
        "                mean_delta_a0_failures_sorted = mean_delta_a0_failures[corr_sorted_indices]\n",
        "\n",
        "                plt.figure(figsize=(15, 6))\n",
        "                plt.bar(range(d_mlp), mean_delta_a0_failures_sorted, color=colors, width=1.0)\n",
        "                plt.xlabel(\"Neurons (Sorted by Correlation with Sum a+b)\")\n",
        "                plt.ylabel(\"Mean Activation Diff (Full - Ablated)\")\n",
        "                plt.title(f\"Mean MLP Activation Difference on Failure Cases with a=0 ({num_a0_failures} samples)\")\n",
        "                plt.xticks([])\n",
        "                plt.grid(axis='y', alpha=0.5)\n",
        "                plt.legend(handles=legend_elements)\n",
        "                plt.show()\n",
        "            else:\n",
        "                print(\"No failure cases found where a=0.\")\n",
        "\n",
        "            # --- 4. Visualize Average Delta for Specific Failure Groups (Example: a=9 failures) ---\n",
        "            a9_failure_mask = failure_mask & (results_df['a'] == 9)\n",
        "            num_a9_failures = a9_failure_mask.sum()\n",
        "\n",
        "            if num_a9_failures > 0:\n",
        "                print(f\"\\nAnalyzing {num_a9_failures} failure cases where a=0...\")\n",
        "                delta_a_mlp_a0_failures = delta_a_mlp[a9_failure_mask]\n",
        "                mean_delta_a0_failures = np.mean(delta_a_mlp_a0_failures, axis=0)\n",
        "                mean_delta_a0_failures_sorted = mean_delta_a0_failures[corr_sorted_indices]\n",
        "\n",
        "                plt.figure(figsize=(15, 6))\n",
        "                plt.bar(range(d_mlp), mean_delta_a0_failures_sorted, color=colors, width=1.0)\n",
        "                plt.xlabel(\"Neurons (Sorted by Correlation with Sum a+b)\")\n",
        "                plt.ylabel(\"Mean Activation Diff (Full - Ablated)\")\n",
        "                plt.title(f\"Mean MLP Activation Difference on Failure Cases with a=9 ({num_a9_failures} samples)\")\n",
        "                plt.xticks([])\n",
        "                plt.grid(axis='y', alpha=0.5)\n",
        "                plt.legend(handles=legend_elements)\n",
        "                plt.show()\n",
        "            else:\n",
        "                print(\"No failure cases found where a=0.\")\n",
        "\n",
        "        else:\n",
        "            print(\"No failure cases detected to analyze activation differences.\")\n",
        "    else:\n",
        "        print(\"Error: Shape mismatch between full and ablated activations.\")\n",
        "        print(f\"Full shape: {a_mlp_full.shape}, Ablated shape: {a_mlp_ablated.shape}\")\n",
        "else:\n",
        "    print(\"Skipping activation difference analysis because activations were not captured.\")\n"
      ],
      "metadata": {
        "id": "HEtGz6boyRmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's some patterns here, in that the changes to the activations seem to be grouped by correlation values, again pointing towards the 'groups of neurons' hypothesis. Let's perform a similar analysis for the pre-ReLU values."
      ],
      "metadata": {
        "id": "OjiapcyQ1UaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "mlp_pre_activation_hook_point = \"blocks.0.mlp.hook_pre\" # Input BEFORE ReLU\n",
        "print(f\"Using MLP pre-activation hook point: {mlp_pre_activation_hook_point}\")\n",
        "\n",
        "# --- Storage ---\n",
        "pre_activation_store = {}\n",
        "\n",
        "# --- Hook Function ---\n",
        "def store_pre_activation_hook(activation, hook, storage_key):\n",
        "    # Store pre-activation at the final sequence position (usually where prediction happens)\n",
        "    pre_activation_store[storage_key] = activation[:, -1, :].detach().clone()\n",
        "    return activation\n",
        "\n",
        "# --- Execute and Capture Activations ---\n",
        "z_mlp_full = None\n",
        "z_mlp_ablated = None\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "\n",
        "    # --- Run 1: Get Full Model Pre-Activations ---\n",
        "    print(f\"\\nRunning model normally, hooking: {mlp_pre_activation_hook_point}\")\n",
        "    pre_activation_store.clear()\n",
        "    fwd_hooks_full = [(\n",
        "        mlp_pre_activation_hook_point,\n",
        "        partial(store_pre_activation_hook, storage_key='full') # Use partial\n",
        "    )]\n",
        "    _ = model.run_with_hooks(test_input_tokens, fwd_hooks=fwd_hooks_full)\n",
        "    if 'full' in pre_activation_store:\n",
        "        z_mlp_full = pre_activation_store['full'].cpu().numpy()\n",
        "        print(f\"Captured full pre-activations, shape: {z_mlp_full.shape}\")\n",
        "    else:\n",
        "        print(f\"FAILED to capture full pre-activations using hook: {mlp_pre_activation_hook_point}\")\n",
        "\n",
        "    # --- Run 2: Get Ablated Model Pre-Activations ---\n",
        "    if z_mlp_full is not None: # Proceed only if first run succeeded\n",
        "        print(f\"\\nRunning model with PC2 ablation, hooking: {mlp_pre_activation_hook_point}\")\n",
        "        pre_activation_store.clear()\n",
        "        fwd_hooks_ablated = [\n",
        "            (embedding_hook_point, ablate_pc2_hook), # Ablate PC2\n",
        "            (mlp_pre_activation_hook_point, partial(store_pre_activation_hook, storage_key='ablated')) # Store pre-activation\n",
        "        ]\n",
        "        _ = model.run_with_hooks(test_input_tokens, fwd_hooks=fwd_hooks_ablated)\n",
        "        if 'ablated' in pre_activation_store:\n",
        "            z_mlp_ablated = pre_activation_store['ablated'].cpu().numpy()\n",
        "            print(f\"Captured ablated pre-activations, shape: {z_mlp_ablated.shape}\")\n",
        "        else:\n",
        "            print(f\"FAILED to capture ablated pre-activations using hook: {mlp_pre_activation_hook_point}\")\n",
        "\n",
        "\n",
        "# --- Calculate Difference and Analyze ---\n",
        "if z_mlp_full is not None and z_mlp_ablated is not None and z_mlp_full.shape == z_mlp_ablated.shape:\n",
        "    print(\"\\nCalculating pre-activation differences (delta_z_mlp)...\")\n",
        "    delta_z_mlp = z_mlp_full - z_mlp_ablated\n",
        "\n",
        "    failure_mask = results_df['Failed'].values\n",
        "    num_failures = failure_mask.sum()\n",
        "\n",
        "    if num_failures > 0:\n",
        "        print(f\"Analyzing {num_failures} failure cases...\")\n",
        "        delta_z_mlp_failures = delta_z_mlp[failure_mask]\n",
        "\n",
        "        # --- 1. Top Changing Neurons ---\n",
        "        mean_abs_delta_z_failures = np.mean(np.abs(delta_z_mlp_failures), axis=0)\n",
        "        sorted_neuron_indices_by_delta_z = np.argsort(mean_abs_delta_z_failures)[::-1]\n",
        "        print(\"\\nTop 10 Neurons by Mean Absolute Pre-Activation Change During Failures:\")\n",
        "        for i in range(min(10, d_mlp)):\n",
        "            idx = sorted_neuron_indices_by_delta_z[i]\n",
        "            print(f\"  Neuron {idx}: Mean Abs Delta_Z = {mean_abs_delta_z_failures[idx]:.4f} (Corr={neuron_sum_correlations[idx]:.3f})\")\n",
        "\n",
        "        # --- 2. Average Delta_Z for ALL Failures ---\n",
        "        mean_delta_z_failures = np.mean(delta_z_mlp_failures, axis=0)\n",
        "        corr_sorted_indices = np.argsort(neuron_sum_correlations)\n",
        "        mean_delta_z_failures_sorted = mean_delta_z_failures[corr_sorted_indices]\n",
        "        correlations_sorted = neuron_sum_correlations[corr_sorted_indices]\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        colors = ['red' if c < 0 else 'blue' for c in correlations_sorted]\n",
        "        plt.bar(range(d_mlp), mean_delta_z_failures_sorted, color=colors, width=1.0)\n",
        "        plt.xlabel(\"Neurons (Sorted by Correlation with Sum a+b)\")\n",
        "        plt.ylabel(\"Mean Pre-Activation Diff (Full - Ablated)\")\n",
        "        plt.title(f\"Mean MLP Pre-Activation Difference (delta_z_mlp) on Failure Cases ({num_failures} samples)\")\n",
        "        plt.xticks([])\n",
        "        plt.grid(axis='y', alpha=0.5)\n",
        "        from matplotlib.lines import Line2D\n",
        "        legend_elements = [Line2D([0], [0], color='blue', lw=4, label='Positively Correlated Neurons'),\n",
        "                           Line2D([0], [0], color='red', lw=4, label='Negatively Correlated Neurons')]\n",
        "        plt.legend(handles=legend_elements)\n",
        "        plt.show()\n",
        "\n",
        "        # --- 3. Average Delta_Z for Specific Failure Groups (a=0) ---\n",
        "        a0_failure_mask = failure_mask & (results_df['a'] == 0)\n",
        "        num_a0_failures = a0_failure_mask.sum()\n",
        "        if num_a0_failures > 0:\n",
        "            print(f\"\\nAnalyzing {num_a0_failures} failure cases where a=0...\")\n",
        "            delta_z_mlp_a0_failures = delta_z_mlp[a0_failure_mask]\n",
        "            mean_delta_z_a0_failures = np.mean(delta_z_mlp_a0_failures, axis=0)\n",
        "            mean_delta_z_a0_failures_sorted = mean_delta_z_a0_failures[corr_sorted_indices]\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.bar(range(d_mlp), mean_delta_z_a0_failures_sorted, color=colors, width=1.0)\n",
        "            plt.xlabel(\"Neurons (Sorted by Correlation with Sum a+b)\")\n",
        "            plt.ylabel(\"Mean Pre-Activation Diff (Full - Ablated)\")\n",
        "            plt.title(f\"Mean MLP Pre-Activation Difference (delta_z_mlp) on Failure Cases with a=0 ({num_a0_failures} samples)\")\n",
        "            plt.xticks([])\n",
        "            plt.grid(axis='y', alpha=0.5)\n",
        "            plt.legend(handles=legend_elements)\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"No failure cases found where a=0.\")\n",
        "\n",
        "    else:\n",
        "        print(\"No failure cases detected.\")\n",
        "else:\n",
        "    print(\"\\nAnalysis skipped: Could not capture both full and ablated pre-activations.\")\n"
      ],
      "metadata": {
        "id": "NMxPLTpJ1vTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H5rXGKys0GQ"
      },
      "source": [
        "### Looking at Activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbHIVRkas0GQ"
      },
      "source": [
        "Helper variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNdBs3C1s0GQ"
      },
      "outputs": [],
      "source": [
        "pattern_a = cache[\"pattern\", 0, \"attn\"][:, :, -1, 0]\n",
        "pattern_b = cache[\"pattern\", 0, \"attn\"][:, :, -1, 1]\n",
        "neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
        "neuron_pre_acts = cache[\"pre\", 0, \"mlp\"][:, -1, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91s0skkZs0GQ"
      },
      "source": [
        "Get all shapes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdqUdekLs0GQ"
      },
      "outputs": [],
      "source": [
        "for param_name, param in cache.items():\n",
        "    print(param_name, param.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKKPmmvRs0GR"
      },
      "outputs": [],
      "source": [
        "imshow(cache[\"pattern\", 0].mean(dim=0)[:, -1, :], title=\"Average Attention Pattern per Head\", xaxis=\"Source\", yaxis=\"Head\", x=['a', 'b', '='])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePEWnaCts0GR"
      },
      "outputs": [],
      "source": [
        "imshow(cache[\"pattern\", 0][5][:, -1, :], title=\"Average Attention Pattern per Head\", xaxis=\"Source\", yaxis=\"Head\", x=['a', 'b', '='])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XXJM97Fs0GS"
      },
      "outputs": [],
      "source": [
        "dataset[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-xn4mH8s0GS"
      },
      "outputs": [],
      "source": [
        "imshow(cache[\"pattern\", 0][:, 0, -1, 0].reshape(p, p), title=\"Attention for Head 0 from a -> =\", xaxis=\"b\", yaxis=\"a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBrWJ8vLs0GS"
      },
      "outputs": [],
      "source": [
        "imshow(\n",
        "    einops.rearrange(cache[\"pattern\", 0][:, :, -1, 0], \"(a b) head -> head a b\", a=p, b=p),\n",
        "    title=\"Attention for Head 0 from a -> =\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWLpHkZFs0GS"
      },
      "source": [
        "Plotting neuron activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0drBmrps0GS"
      },
      "outputs": [],
      "source": [
        "cache[\"post\", 0, \"mlp\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgQak5VHs0GS"
      },
      "outputs": [],
      "source": [
        "imshow(\n",
        "    einops.rearrange(neuron_acts[:, :5], \"(a b) neuron -> neuron a b\", a=p, b=p),\n",
        "    title=\"First 5 neuron acts\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HnVxne3s0GS"
      },
      "source": [
        "### Singular Value Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U16Z4TJSs0GS"
      },
      "outputs": [],
      "source": [
        "W_E.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiC6n2Fes0GS"
      },
      "outputs": [],
      "source": [
        "U, S, Vh = torch.svd(W_E)\n",
        "line(S, title=\"Singular Values\")\n",
        "imshow(U, title=\"Principal Components on the Input\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvVbeQwRs0GT"
      },
      "outputs": [],
      "source": [
        "# Control - random Gaussian matrix\n",
        "U, S, Vh = torch.svd(torch.randn_like(W_E))\n",
        "line(S, title=\"Singular Values Random\")\n",
        "imshow(U, title=\"Principal Components Random\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSyCHC1Fs0GT"
      },
      "source": [
        "## Explaining Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2hCWMaPs0GT"
      },
      "source": [
        "### Analyse the Embedding - It's a Lookup Table!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVqefI_Ns0GT"
      },
      "outputs": [],
      "source": [
        "U, S, Vh = torch.svd(W_E)\n",
        "line(U[:, :8].T, title=\"Principal Components of the embedding\", xaxis=\"Input Vocabulary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAczKOwOs0GT"
      },
      "outputs": [],
      "source": [
        "fourier_basis = []\n",
        "fourier_basis_names = []\n",
        "fourier_basis.append(torch.ones(p))\n",
        "fourier_basis_names.append(\"Constant\")\n",
        "for freq in range(1, p//2+1):\n",
        "    fourier_basis.append(torch.sin(torch.arange(p)*2 * torch.pi * freq / p))\n",
        "    fourier_basis_names.append(f\"Sin {freq}\")\n",
        "    fourier_basis.append(torch.cos(torch.arange(p)*2 * torch.pi * freq / p))\n",
        "    fourier_basis_names.append(f\"Cos {freq}\")\n",
        "fourier_basis = torch.stack(fourier_basis, dim=0).to(device)\n",
        "fourier_basis = fourier_basis/fourier_basis.norm(dim=-1, keepdim=True)\n",
        "imshow(fourier_basis, xaxis=\"Input\", yaxis=\"Component\", y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTjqVcRfs0GT"
      },
      "outputs": [],
      "source": [
        "line(fourier_basis[:8], xaxis=\"Input\", line_labels=fourier_basis_names[:8], title=\"First 8 Fourier Components\")\n",
        "line(fourier_basis[25:29], xaxis=\"Input\", line_labels=fourier_basis_names[25:29], title=\"Middle Fourier Components\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG-59vX6s0GT"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ fourier_basis.T, title=\"All Fourier Vectors are Orthogonal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22IuwAJIs0GT"
      },
      "source": [
        "### Analyse the Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFVp-W6rs0GT"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ W_E, yaxis=\"Fourier Component\", xaxis=\"Residual Stream\", y=fourier_basis_names, title=\"Embedding in Fourier Basis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmkPoFBis0GT"
      },
      "outputs": [],
      "source": [
        "line((fourier_basis @ W_E).norm(dim=-1), xaxis=\"Fourier Component\", x=fourier_basis_names, title=\"Norms of Embedding in Fourier Basis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAt0F9ers0GT"
      },
      "outputs": [],
      "source": [
        "key_freqs = [17, 25, 32, 47]\n",
        "key_freq_indices = [33, 34, 49, 50, 63, 64, 93, 94]\n",
        "fourier_embed = fourier_basis @ W_E\n",
        "key_fourier_embed = fourier_embed[key_freq_indices]\n",
        "print(\"key_fourier_embed\", key_fourier_embed.shape)\n",
        "imshow(key_fourier_embed @ key_fourier_embed.T, title=\"Dot Product of embedding of key Fourier Terms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlzF0ohGs0GT"
      },
      "source": [
        "### Key Frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj-YO2jas0GT"
      },
      "outputs": [],
      "source": [
        "line(fourier_basis[[34, 50, 64, 94]], title=\"Cos of key freqs\", line_labels=[34, 50, 64, 94])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-rRcCUBs0GT"
      },
      "outputs": [],
      "source": [
        "line(fourier_basis[[34, 50, 64, 94]].mean(0), title=\"Constructive Interference\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxZIrLDXs0GT"
      },
      "source": [
        "## Analyse Neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBPiUgyis0GT"
      },
      "outputs": [],
      "source": [
        "imshow(\n",
        "    einops.rearrange(neuron_acts[:, :5], \"(a b) neuron -> neuron a b\", a=p, b=p),\n",
        "    title=\"First 5 neuron acts\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUsIbU1os0GT"
      },
      "outputs": [],
      "source": [
        "imshow(\n",
        "    einops.rearrange(neuron_acts[:, 0], \"(a b) -> a b\", a=p, b=p),\n",
        "    title=\"First neuron act\", xaxis=\"b\", yaxis=\"a\",)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4vEkIPZs0GT"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis[94][None, :] * fourier_basis[94][:, None], title=\"Cos 47a * cos 47b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5ar09d0s0GU"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis[94][None, :] * fourier_basis[0][:, None], title=\"Cos 47a * const\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eMysImAs0GU"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ neuron_acts[:, 0].reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of neuron 0\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EeLu17xs0GU"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ neuron_acts[:, 5].reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of neuron 5\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cmp_ADus0GU"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ torch.randn_like(neuron_acts[:, 0]).reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of RANDOM\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkEM5FuKs0GU"
      },
      "source": [
        "### Neuron Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKXWKzTvs0GU"
      },
      "outputs": [],
      "source": [
        "fourier_neuron_acts = fourier_basis @ einops.rearrange(neuron_acts, \"(a b) neuron -> neuron a b\", a=p, b=p) @ fourier_basis.T\n",
        "# Center these by removing the mean - doesn't matter!\n",
        "fourier_neuron_acts[:, 0, 0] = 0.\n",
        "print(\"fourier_neuron_acts\", fourier_neuron_acts.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7bVd4uHs0GU"
      },
      "outputs": [],
      "source": [
        "neuron_freq_norm = torch.zeros(p//2, model.cfg.d_mlp).to(device)\n",
        "for freq in range(0, p//2):\n",
        "    for x in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
        "        for y in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
        "            neuron_freq_norm[freq] += fourier_neuron_acts[:, x, y]**2\n",
        "neuron_freq_norm = neuron_freq_norm / fourier_neuron_acts.pow(2).sum(dim=[-1, -2])[None, :]\n",
        "imshow(neuron_freq_norm, xaxis=\"Neuron\", yaxis=\"Freq\", y=torch.arange(1, p//2+1), title=\"Neuron Frac Explained by Freq\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgFc7jTes0GU"
      },
      "outputs": [],
      "source": [
        "line(neuron_freq_norm.max(dim=0).values.sort().values, xaxis=\"Neuron\", title=\"Max Neuron Frac Explained over Freqs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "604eOgv1s0GV"
      },
      "source": [
        "## Read Off the Neuron-Logit Weights to Interpret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-YB2lybs0GV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQUj_Oqts0GV"
      },
      "outputs": [],
      "source": [
        "W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
        "print(\"W_logit\", W_logit.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JeTG5uSs0GV"
      },
      "outputs": [],
      "source": [
        "line((W_logit @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title=\"W_logit in the Fourier Basis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YdSwqDws0GV"
      },
      "outputs": [],
      "source": [
        "neurons_17 = neuron_freq_norm[17-1]>0.85\n",
        "neurons_17.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK18zt9Ys0GV"
      },
      "outputs": [],
      "source": [
        "neurons_17.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmg_8lWMs0GV"
      },
      "outputs": [],
      "source": [
        "line((W_logit[neurons_17] @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title=\"W_logit for freq 17 neurons in the Fourier Basis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyjOvArYs0GV"
      },
      "source": [
        "Study sin 17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_uaGUnVs0GV"
      },
      "outputs": [],
      "source": [
        "freq = 17\n",
        "W_logit_fourier = W_logit @ fourier_basis\n",
        "neurons_sin_17 = W_logit_fourier[:, 2*freq-1]\n",
        "line(neurons_sin_17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FO9TUuNs0GV"
      },
      "outputs": [],
      "source": [
        "neuron_acts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyaLVHtrs0GV"
      },
      "outputs": [],
      "source": [
        "inputs_sin_17c = neuron_acts @ neurons_sin_17\n",
        "imshow(fourier_basis @ inputs_sin_17c.reshape(p, p) @ fourier_basis.T, title=\"Fourier Heatmap over inputs for sin17c\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv0OEDwms0GW"
      },
      "source": [
        "# Black Box Methods + Progress Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ydLen4As0GW"
      },
      "source": [
        "## Setup Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_Rjpunas0GW"
      },
      "source": [
        "Code to plot embedding freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naOtz04Ys0GW"
      },
      "outputs": [],
      "source": [
        "def embed_to_cos_sin(fourier_embed):\n",
        "    if len(fourier_embed.shape) == 1:\n",
        "        return torch.stack([fourier_embed[1::2], fourier_embed[2::2]])\n",
        "    else:\n",
        "        return torch.stack([fourier_embed[:, 1::2], fourier_embed[:, 2::2]], dim=1)\n",
        "\n",
        "from neel_plotly.plot import melt\n",
        "\n",
        "def plot_embed_bars(\n",
        "    fourier_embed,\n",
        "    title=\"Norm of embedding of each Fourier Component\",\n",
        "    return_fig=False,\n",
        "    **kwargs\n",
        "):\n",
        "    cos_sin_embed = embed_to_cos_sin(fourier_embed)\n",
        "    df = melt(cos_sin_embed)\n",
        "    # display(df)\n",
        "    group_labels = {0: \"sin\", 1: \"cos\"}\n",
        "    df[\"Trig\"] = df[\"0\"].map(lambda x: group_labels[x])\n",
        "    fig = px.bar(\n",
        "        df,\n",
        "        barmode=\"group\",\n",
        "        color=\"Trig\",\n",
        "        x=\"1\",\n",
        "        y=\"value\",\n",
        "        labels={\"1\": \"$w_k$\", \"value\": \"Norm\"},\n",
        "        title=title,\n",
        "        **kwargs\n",
        "    )\n",
        "    fig.update_layout(dict(legend_title=\"\"))\n",
        "\n",
        "    if return_fig:\n",
        "        return fig\n",
        "    else:\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evAuHNhvs0GW"
      },
      "source": [
        "Code to test a tensor of edited logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP4qn-ags0GW"
      },
      "outputs": [],
      "source": [
        "def test_logits(logits, bias_correction=False, original_logits=None, mode=\"all\"):\n",
        "    # Calculates cross entropy loss of logits representing a batch of all p^2\n",
        "    # possible inputs\n",
        "    # Batch dimension is assumed to be first\n",
        "    if logits.shape[1] == p * p:\n",
        "        logits = logits.T\n",
        "    if logits.shape == torch.Size([p * p, p + 1]):\n",
        "        logits = logits[:, :-1]\n",
        "    logits = logits.reshape(p * p, p)\n",
        "    if bias_correction:\n",
        "        # Applies bias correction - we correct for any missing bias terms,\n",
        "        # independent of the input, by centering the new logits along the batch\n",
        "        # dimension, and then adding the average original logits across all inputs\n",
        "        logits = (\n",
        "            einops.reduce(original_logits - logits, \"batch ... -> ...\", \"mean\") + logits\n",
        "        )\n",
        "    if mode == \"train\":\n",
        "        return loss_fn(logits[train_indices], labels[train_indices])\n",
        "    elif mode == \"test\":\n",
        "        return loss_fn(logits[test_indices], labels[test_indices])\n",
        "    elif mode == \"all\":\n",
        "        return loss_fn(logits, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2pGSGLZs0GW"
      },
      "source": [
        "Code to run a metric over every checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6B559Xvs0GW"
      },
      "outputs": [],
      "source": [
        "metric_cache = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkX1wnA7s0GW"
      },
      "outputs": [],
      "source": [
        "def get_metrics(model, metric_cache, metric_fn, name, reset=False):\n",
        "    if reset or (name not in metric_cache) or (len(metric_cache[name]) == 0):\n",
        "        metric_cache[name] = []\n",
        "        for c, sd in enumerate(tqdm.tqdm((model_checkpoints))):\n",
        "            model.reset_hooks()\n",
        "            model.load_state_dict(sd)\n",
        "            out = metric_fn(model)\n",
        "            if type(out) == torch.Tensor:\n",
        "                out = utils.to_numpy(out)\n",
        "            metric_cache[name].append(out)\n",
        "        model.load_state_dict(model_checkpoints[-1])\n",
        "        try:\n",
        "            metric_cache[name] = torch.tensor(metric_cache[name])\n",
        "        except:\n",
        "            metric_cache[name] = torch.tensor(np.array(metric_cache[name]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP7HYsw4s0GW"
      },
      "source": [
        "## Defining Progress Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-X6SmHLs0GX"
      },
      "source": [
        "### Loss Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dk8WFsss0GX"
      },
      "outputs": [],
      "source": [
        "memorization_end_epoch = 1500\n",
        "circuit_formation_end_epoch = 13300\n",
        "cleanup_end_epoch = 16600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkOXufDOs0GX"
      },
      "outputs": [],
      "source": [
        "def add_lines(figure):\n",
        "    figure.add_vline(memorization_end_epoch, line_dash=\"dash\", opacity=0.7)\n",
        "    figure.add_vline(circuit_formation_end_epoch, line_dash=\"dash\", opacity=0.7)\n",
        "    figure.add_vline(cleanup_end_epoch, line_dash=\"dash\", opacity=0.7)\n",
        "    return figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBp4jo-ms0GX"
      },
      "outputs": [],
      "source": [
        "fig = line([train_losses[::100], test_losses[::100]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Training Curve for Modular Addition\", line_labels=['train', 'test'], toggle_x=True, toggle_y=True, return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7oWhlUFs0GX"
      },
      "source": [
        "### Logit Periodicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRdbxqLxs0GX"
      },
      "outputs": [],
      "source": [
        "all_logits = original_logits[:, -1, :]\n",
        "print(all_logits.shape)\n",
        "all_logits = einops.rearrange(all_logits, \"(a b) c -> a b c\", a=p, b=p)\n",
        "print(all_logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI27BHros0GX"
      },
      "outputs": [],
      "source": [
        "coses = {}\n",
        "for freq in key_freqs:\n",
        "    print(\"Freq:\", freq)\n",
        "    a = torch.arange(p)[:, None, None]\n",
        "    b = torch.arange(p)[None, :, None]\n",
        "    c = torch.arange(p)[None, None, :]\n",
        "    cube_predicted_logits = torch.cos(freq * 2 * torch.pi / p * (a + b - c)).to(device)\n",
        "    cube_predicted_logits /= cube_predicted_logits.norm()\n",
        "    coses[freq] = cube_predicted_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XCK0pKWs0GX"
      },
      "outputs": [],
      "source": [
        "approximated_logits = torch.zeros_like(all_logits)\n",
        "for freq in key_freqs:\n",
        "    print(\"Freq:\", freq)\n",
        "    coeff = (all_logits * coses[freq]).sum()\n",
        "    print(\"Coeff:\", coeff)\n",
        "    cosine_sim = coeff / all_logits.norm()\n",
        "    print(\"Cosine Sim:\", cosine_sim)\n",
        "    approximated_logits += coeff * coses[freq]\n",
        "residual = all_logits - approximated_logits\n",
        "print(\"Residual size:\", residual.norm())\n",
        "print(\"Residual fraction of norm:\", residual.norm()/all_logits.norm())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1LJbXAJs0GX"
      },
      "outputs": [],
      "source": [
        "random_logit_cube = torch.randn_like(all_logits)\n",
        "print((all_logits * random_logit_cube).sum()/random_logit_cube.norm()/all_logits.norm())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2mNngkks0GX"
      },
      "outputs": [],
      "source": [
        "test_logits(all_logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgJrRka_s0GX"
      },
      "outputs": [],
      "source": [
        "test_logits(approximated_logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWPzR90Us0GX"
      },
      "source": [
        "#### Look During Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrldGN_0s0GX"
      },
      "outputs": [],
      "source": [
        "cos_cube = []\n",
        "for freq in range(1, p//2 + 1):\n",
        "    a = torch.arange(p)[:, None, None]\n",
        "    b = torch.arange(p)[None, :, None]\n",
        "    c = torch.arange(p)[None, None, :]\n",
        "    cube_predicted_logits = torch.cos(freq * 2 * torch.pi / p * (a + b - c)).to(device)\n",
        "    cube_predicted_logits /= cube_predicted_logits.norm()\n",
        "    cos_cube.append(cube_predicted_logits)\n",
        "cos_cube = torch.stack(cos_cube, dim=0)\n",
        "print(cos_cube.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAh-wilSs0GX"
      },
      "outputs": [],
      "source": [
        "def get_cos_coeffs(model):\n",
        "    logits = model(dataset)[:, -1]\n",
        "    logits = einops.rearrange(logits, \"(a b) c -> a b c\", a=p, b=p)\n",
        "    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])\n",
        "    return vals\n",
        "\n",
        "\n",
        "get_metrics(model, metric_cache, get_cos_coeffs, \"cos_coeffs\")\n",
        "print(metric_cache[\"cos_coeffs\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY_4kiNks0GY"
      },
      "outputs": [],
      "source": [
        "fig = line(metric_cache[\"cos_coeffs\"].T, line_labels=[f\"Freq {i}\" for i in range(1, p//2+1)], title=\"Coefficients with Predicted Logits\", xaxis=\"Epoch\", x=checkpoint_epochs, yaxis=\"Coefficient\", return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeI-gBNbs0GY"
      },
      "outputs": [],
      "source": [
        "def get_cos_sim(model):\n",
        "    logits = model(dataset)[:, -1]\n",
        "    logits = einops.rearrange(logits, \"(a b) c -> a b c\", a=p, b=p)\n",
        "    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])\n",
        "    return vals / logits.norm()\n",
        "\n",
        "get_metrics(model, metric_cache, get_cos_sim, \"cos_sim\") # You may need a big GPU. If you don't have one and can't work around this, raise an issue for help!\n",
        "print(metric_cache[\"cos_sim\"].shape)\n",
        "\n",
        "fig = line(metric_cache[\"cos_sim\"].T, line_labels=[f\"Freq {i}\" for i in range(1, p//2+1)], title=\"Cosine Sim with Predicted Logits\", xaxis=\"Epoch\", x=checkpoint_epochs, yaxis=\"Cosine Sim\", return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG0nbFRjs0GY"
      },
      "outputs": [],
      "source": [
        "def get_residual_cos_sim(model):\n",
        "    logits = model(dataset)[:, -1]\n",
        "    logits = einops.rearrange(logits, \"(a b) c -> a b c\", a=p, b=p)\n",
        "    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])\n",
        "    residual = logits - (vals[:, None, None, None] * cos_cube).sum(dim=0)\n",
        "    return residual.norm() / logits.norm()\n",
        "\n",
        "get_metrics(model, metric_cache, get_residual_cos_sim, \"residual_cos_sim\")\n",
        "print(metric_cache[\"residual_cos_sim\"].shape)\n",
        "\n",
        "fig = line([metric_cache[\"cos_sim\"][:, i] for i in range(p//2)]+[metric_cache[\"residual_cos_sim\"]], line_labels=[f\"Freq {i}\" for i in range(1, p//2+1)]+[\"residual\"], title=\"Cosine Sim with Predicted Logits + Residual\", xaxis=\"Epoch\", x=checkpoint_epochs, yaxis=\"Cosine Sim\", return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2K5cIU5s0GY"
      },
      "source": [
        "## Restricted Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvdkOwvis0GY"
      },
      "outputs": [],
      "source": [
        "neuron_acts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUW11uiis0GY"
      },
      "outputs": [],
      "source": [
        "neuron_acts_square = einops.rearrange(neuron_acts, \"(a b) neur -> a b neur\", a=p, b=p).clone()\n",
        "# Center it\n",
        "neuron_acts_square -= einops.reduce(neuron_acts_square, \"a b neur -> 1 1 neur\", \"mean\")\n",
        "neuron_acts_square_fourier = einsum(\"a b neur, fa a, fb b -> fa fb neur\", neuron_acts_square, fourier_basis, fourier_basis)\n",
        "imshow(neuron_acts_square_fourier.norm(dim=-1), xaxis=\"Fourier Component b\", yaxis=\"Fourier Component a\", title=\"Norms of neuron activations by Fourier Component\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHGB6i6Is0GY"
      },
      "outputs": [],
      "source": [
        "original_logits, cache = model.run_with_cache(dataset)\n",
        "print(original_logits.numel())\n",
        "neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rti1zUJ-s0GY"
      },
      "outputs": [],
      "source": [
        "approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
        "approx_neuron_acts += neuron_acts.mean(dim=0)\n",
        "a = torch.arange(p)[:, None]\n",
        "b = torch.arange(p)[None, :]\n",
        "for freq in key_freqs:\n",
        "    cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "    cos_apb_vec /= cos_apb_vec.norm()\n",
        "    cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
        "    approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
        "    sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "    sin_apb_vec /= sin_apb_vec.norm()\n",
        "    sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
        "    approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
        "restricted_logits = approx_neuron_acts @ W_logit\n",
        "print(loss_fn(restricted_logits[test_indices], test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYIXtp_Ys0GY"
      },
      "outputs": [],
      "source": [
        "print(loss_fn(all_logits, labels)) # This bugged on models not fully trained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv05m50Is0GY"
      },
      "source": [
        "### Look During Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IinjgaGWs0GY"
      },
      "outputs": [],
      "source": [
        "def get_restricted_loss(model):\n",
        "    logits, cache = model.run_with_cache(dataset)\n",
        "    logits = logits[:, -1, :]\n",
        "    neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
        "    approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
        "    approx_neuron_acts += neuron_acts.mean(dim=0)\n",
        "    a = torch.arange(p)[:, None]\n",
        "    b = torch.arange(p)[None, :]\n",
        "    for freq in key_freqs:\n",
        "        cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "        cos_apb_vec /= cos_apb_vec.norm()\n",
        "        cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
        "        approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
        "        sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "        sin_apb_vec /= sin_apb_vec.norm()\n",
        "        sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
        "        approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
        "    restricted_logits = approx_neuron_acts @ model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
        "    # Add bias term\n",
        "    restricted_logits += logits.mean(dim=0, keepdim=True) - restricted_logits.mean(dim=0, keepdim=True)\n",
        "    return loss_fn(restricted_logits[test_indices], test_labels)\n",
        "get_restricted_loss(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_qhNrM7s0GY"
      },
      "outputs": [],
      "source": [
        "get_metrics(model, metric_cache, get_restricted_loss, \"restricted_loss\", reset=True)\n",
        "print(metric_cache[\"restricted_loss\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T2erlNUs0GZ"
      },
      "outputs": [],
      "source": [
        "fig = line([train_losses[::100], test_losses[::100], metric_cache[\"restricted_loss\"]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Restricted Loss Curve\", line_labels=['train', 'test', \"restricted_loss\"], toggle_x=True, toggle_y=True, return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bewRWQ8s0GZ"
      },
      "outputs": [],
      "source": [
        "fig = line([torch.tensor(test_losses[::100])/metric_cache[\"restricted_loss\"]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Restricted Loss to Test Loss Ratio\", toggle_x=True, toggle_y=True, return_fig=True)\n",
        "# WARNING: bugged when cancelling training half way thr ough\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybIDyH4gs0GZ"
      },
      "source": [
        "## Excluded Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKd6GtA4s0GZ"
      },
      "outputs": [],
      "source": [
        "approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
        "# approx_neuron_acts += neuron_acts.mean(dim=0)\n",
        "a = torch.arange(p)[:, None]\n",
        "b = torch.arange(p)[None, :]\n",
        "for freq in key_freqs:\n",
        "    cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "    cos_apb_vec /= cos_apb_vec.norm()\n",
        "    cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
        "    approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
        "    sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "    sin_apb_vec /= sin_apb_vec.norm()\n",
        "    sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
        "    approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
        "excluded_neuron_acts = neuron_acts - approx_neuron_acts\n",
        "excluded_logits = excluded_neuron_acts @ W_logit\n",
        "print(loss_fn(excluded_logits[train_indices], train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uoji2678s0GZ"
      },
      "outputs": [],
      "source": [
        "def get_excluded_loss(model):\n",
        "    logits, cache = model.run_with_cache(dataset)\n",
        "    logits = logits[:, -1, :]\n",
        "    neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
        "    approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
        "    # approx_neuron_acts += neuron_acts.mean(dim=0)\n",
        "    a = torch.arange(p)[:, None]\n",
        "    b = torch.arange(p)[None, :]\n",
        "    for freq in key_freqs:\n",
        "        cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "        cos_apb_vec /= cos_apb_vec.norm()\n",
        "        cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
        "        approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
        "        sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "        sin_apb_vec /= sin_apb_vec.norm()\n",
        "        sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
        "        approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
        "    excluded_neuron_acts = neuron_acts - approx_neuron_acts\n",
        "    residual_stream_final = excluded_neuron_acts @ model.blocks[0].mlp.W_out + cache[\"resid_mid\", 0][:, -1, :]\n",
        "    excluded_logits = residual_stream_final @ model.unembed.W_U\n",
        "    return loss_fn(excluded_logits[train_indices], train_labels)\n",
        "get_excluded_loss(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHuD_-Fcs0GZ"
      },
      "outputs": [],
      "source": [
        "get_metrics(model, metric_cache, get_excluded_loss, \"excluded_loss\", reset=True)\n",
        "print(metric_cache[\"excluded_loss\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrzhqw-Xs0GZ"
      },
      "outputs": [],
      "source": [
        "fig = line([train_losses[::100], test_losses[::100], metric_cache[\"excluded_loss\"], metric_cache[\"restricted_loss\"]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Excluded and Restricted Loss Curve\", line_labels=['train', 'test', \"excluded_loss\", \"restricted_loss\"], toggle_x=True, toggle_y=True, return_fig=True)\n",
        "\n",
        "add_lines(fig)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8859a5491331dba93123a91c2831400aced845b502848170e05fcb48b2c144be"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}