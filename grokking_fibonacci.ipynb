{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBTfiQFVs0GI"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Grokking_Demo.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B58KqSBps0GK"
      },
      "source": [
        "# Grokking Demo Notebook\n",
        "\n",
        "<b style=\"color: red\">To use this notebook, go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wU3VZxxs0GK"
      },
      "source": [
        "# Setup\n",
        "(No need to read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYEoHvphs0GK"
      },
      "outputs": [],
      "source": [
        "TRAIN_MODEL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVqsonBas0GL"
      },
      "outputs": [],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "import os\n",
        "\n",
        "DEVELOPMENT_MODE = True\n",
        "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
        "    # # Install another version of node that makes PySvelte work way faster\n",
        "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")\n",
        "\n",
        "if IN_COLAB or IN_GITHUB:\n",
        "    %pip install transformer_lens\n",
        "    %pip install circuitsvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5Zu_bOxs0GL"
      },
      "outputs": [],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCQeYT1Ys0GM"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95rrdZOxs0GM"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import os\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaWkHpV8s0GM"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms-9EWghs0GM"
      },
      "source": [
        "Plotting helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHddh5X0s0GM"
      },
      "outputs": [],
      "source": [
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBdM6Xa6s0GN"
      },
      "outputs": [],
      "source": [
        "# Define the location to save the model, using a relative path\n",
        "PTH_LOCATION = \"workspace/_scratch/grokking_demo.pth\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FbCNqRNs0GN"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr1ajHuWs0GN"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRhNvT8vs0GN"
      },
      "outputs": [],
      "source": [
        "p = 10\n",
        "frac_train = 0.72\n",
        "\n",
        "# Optimizer config\n",
        "lr = 1e-3\n",
        "wd = 1.\n",
        "betas = (0.9, 0.98)\n",
        "\n",
        "num_epochs = 10000\n",
        "checkpoint_every = 100\n",
        "\n",
        "DATA_SEED = 598"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVV5pB7Es0GN"
      },
      "source": [
        "## Define Task\n",
        "* Define generalized Fibonacci\n",
        "* Define the dataset & labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0CzC5Rfs0GN"
      },
      "source": [
        "Input format:\n",
        "|a|b|=|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7RcAWSfs0GN"
      },
      "outputs": [],
      "source": [
        "a_vector = einops.repeat(torch.arange(p), \"i -> (i j)\", j=p)\n",
        "b_vector = einops.repeat(torch.arange(p), \"j -> (i j)\", i=p)\n",
        "equals_vector = einops.repeat(torch.tensor(p), \" -> (i j)\", i=p, j=p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8usqEh7Ss0GN"
      },
      "outputs": [],
      "source": [
        "dataset = torch.stack([a_vector, b_vector, equals_vector], dim=1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spCsQ8_ks0GO"
      },
      "outputs": [],
      "source": [
        "labels = (dataset[:, 0] + dataset[:, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KawgPijs0GO"
      },
      "source": [
        "Convert this to a train + test set - 30% in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9oFB_m3s0GO"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(DATA_SEED)\n",
        "indices = torch.randperm(p*p)\n",
        "cutoff = int(p*p*frac_train)\n",
        "train_indices = indices[:cutoff]\n",
        "test_indices = indices[cutoff:]\n",
        "\n",
        "train_data = dataset[train_indices]\n",
        "train_labels = labels[train_indices]\n",
        "test_data = dataset[test_indices]\n",
        "test_labels = labels[test_indices]\n",
        "print(train_data[:5])\n",
        "print(train_labels[:5])\n",
        "print(train_data.shape)\n",
        "print(test_data[:5])\n",
        "print(test_labels[:5])\n",
        "print(test_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIztxc8Is0GO"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWa9jdIps0GO"
      },
      "outputs": [],
      "source": [
        "\n",
        "cfg = HookedTransformerConfig(\n",
        "    n_layers = 1,\n",
        "    n_heads = 4,\n",
        "    d_model = 128,\n",
        "    d_head = 32,\n",
        "    d_mlp = 512,\n",
        "    act_fn = \"relu\",\n",
        "    normalization_type=None,\n",
        "    d_vocab=p+1,\n",
        "    d_vocab_out=2*p,\n",
        "    n_ctx=3,\n",
        "    init_weights=True,\n",
        "    device=device,\n",
        "    seed = 999,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxdHppAIs0GO"
      },
      "outputs": [],
      "source": [
        "model = HookedTransformer(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpV0nvhAs0GO"
      },
      "source": [
        "Disable the biases, as we don't need them for this task and it makes things easier to interpret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2PLgqTzs0GO"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if \"b_\" in name:\n",
        "        param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9xFcKkJs0GO"
      },
      "source": [
        "## Define Optimizer + Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i4SmHnLs0GO"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1iogkzys0GO"
      },
      "outputs": [],
      "source": [
        "def loss_fn(logits, labels):\n",
        "    if len(logits.shape)==3:\n",
        "        logits = logits[:, -1]\n",
        "    logits = logits.to(torch.float64)\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
        "    return -correct_log_probs.mean()\n",
        "train_logits = model(train_data)\n",
        "train_loss = loss_fn(train_logits, train_labels)\n",
        "print(train_loss)\n",
        "test_logits = model(test_data)\n",
        "test_loss = loss_fn(test_logits, test_labels)\n",
        "print(test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhyxMx3os0GP"
      },
      "outputs": [],
      "source": [
        "print(\"Uniform loss:\")\n",
        "print(np.log(2*p))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbGSVZFMs0GP"
      },
      "source": [
        "## Actually Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlgKog0-s0GP"
      },
      "source": [
        "**Weird Decision:** Training the model with full batch training rather than stochastic gradient descent. We do this so to make training smoother and reduce the number of slingshots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ5bPzCis0GP"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "model_checkpoints = []\n",
        "checkpoint_epochs = []\n",
        "if TRAIN_MODEL:\n",
        "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "        train_logits = model(train_data)\n",
        "        train_loss = loss_fn(train_logits, train_labels)\n",
        "        train_loss.backward()\n",
        "        train_losses.append(train_loss.item())\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            test_logits = model(test_data)\n",
        "            test_loss = loss_fn(test_logits, test_labels)\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "        if ((epoch+1)%checkpoint_every)==0:\n",
        "            checkpoint_epochs.append(epoch)\n",
        "            model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
        "            print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcttAzjes0GP"
      },
      "outputs": [],
      "source": [
        "torch.save(\n",
        "    {\n",
        "        \"model\":model.state_dict(),\n",
        "        \"config\": model.cfg,\n",
        "        \"checkpoints\": model_checkpoints,\n",
        "        \"checkpoint_epochs\": checkpoint_epochs,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"train_indices\": train_indices,\n",
        "        \"test_indices\": test_indices,\n",
        "    },\n",
        "    PTH_LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYn9svOss0GP"
      },
      "outputs": [],
      "source": [
        "if not TRAIN_MODEL:\n",
        "    cached_data = torch.load(PTH_LOCATION)\n",
        "    model.load_state_dict(cached_data['model'])\n",
        "    model_checkpoints = cached_data[\"checkpoints\"]\n",
        "    checkpoint_epochs = cached_data[\"checkpoint_epochs\"]\n",
        "    test_losses = cached_data['test_losses']\n",
        "    train_losses = cached_data['train_losses']\n",
        "    train_indices = cached_data[\"train_indices\"]\n",
        "    test_indices = cached_data[\"test_indices\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V77TDhwos0GP"
      },
      "source": [
        "## Show Model Training Statistics, Check that it groks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhaZN_rns0GP"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
        "from neel_plotly.plot import line"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "line([train_losses[::100], test_losses[::100]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Training Curve for Fibonacci\", line_labels=['train', 'test'], toggle_x=True, toggle_y=True)"
      ],
      "metadata": {
        "id": "y_jB5NKa1FIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l7ceOyOs0GP"
      },
      "source": [
        "# Analysing the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYEowab5s0GP"
      },
      "source": [
        "## Standard Things to Try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBB-TNUJs0GQ"
      },
      "outputs": [],
      "source": [
        "original_logits, cache = model.run_with_cache(dataset)\n",
        "print(original_logits.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAO0GYFrs0GQ"
      },
      "source": [
        "Get key weight matrices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsN6BuXVs0GQ"
      },
      "outputs": [],
      "source": [
        "W_E = model.embed.W_E[:-1]\n",
        "print(\"W_E\", W_E.shape)\n",
        "W_neur = W_E @ model.blocks[0].attn.W_V @ model.blocks[0].attn.W_O @ model.blocks[0].mlp.W_in\n",
        "print(\"W_neur\", W_neur.shape)\n",
        "W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
        "print(\"W_logit\", W_logit.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-GmiCJjs0GQ"
      },
      "outputs": [],
      "source": [
        "original_loss = loss_fn(original_logits, labels).item()\n",
        "print(\"Original Loss:\", original_loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_number = 7\n",
        "embedding_vector_np = W_E[input_number].detach().cpu().numpy() # this gets us the embedded vector for input_number\n",
        "\n",
        "# first we try to plot directly the value of the entries of the embedded vector\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 6)) # Adjust figure size as needed\n",
        "plt.bar(np.arange(128), embedding_vector_np)\n",
        "plt.xlabel(\"Component Index (within Embedding Vector)\")\n",
        "plt.ylabel(\"Component Value\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rPRIYknoZFrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we will do a heatmap of the embedded vector for all possible inputs\n",
        "\n",
        "# We want dimensions along y-axis and inputs along x-axis,\n",
        "# so we need to transpose the matrix for imshow\n",
        "W_E_transposed = W_E.detach().cpu().numpy().T # Shape (d, N)\n",
        "\n",
        "# Get N and d from the original tensor shape\n",
        "N_vocab = W_E.shape[0]\n",
        "d_embed = W_E.shape[1]\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(8, 10)) # Adjust figsize as needed (width, height)\n",
        "# Use imshow to display the matrix as an image.\n",
        "# aspect='auto' allows the cells to be non-square to fit the plot area.\n",
        "# interpolation='nearest' avoids blurring pixels.\n",
        "# cmap='viridis' is a common colormap, change if you prefer another.\n",
        "im = plt.imshow(W_E_transposed, aspect='auto', interpolation='nearest', cmap='viridis')\n",
        "\n",
        "# Add labels and title|\n",
        "plt.xlabel(\"Input Token Index (k)\")\n",
        "plt.ylabel(\"Embedding Dimension Index\")\n",
        "plt.title(\"Heatmap of Embedding Vectors (W_E)\")\n",
        "\n",
        "# Set ticks to match indices\n",
        "# Show ticks for every input token if N is small\n",
        "if N_vocab <= 20: # Adjust threshold as needed\n",
        "     plt.xticks(ticks=np.arange(N_vocab), labels=np.arange(N_vocab))\n",
        "else:\n",
        "    # For larger N, show fewer ticks to avoid clutter\n",
        "     plt.xticks(ticks=np.linspace(0, N_vocab-1, num=min(N_vocab, 10), dtype=int))\n",
        "\n",
        "\n",
        "# Add a colorbar to show the mapping from color to value\n",
        "plt.colorbar(im, label='Embedding Component Value')\n",
        "\n",
        "# Ensure layout is tight\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1QWkH9b3bJOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks... kind of periodic? Let's now try to do PCA (or SVD) on this and see if we can learn anything."
      ],
      "metadata": {
        "id": "CbDgpPUPbyvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Center the data (subtract the mean of each dimension)\n",
        "W_E_mean = W_E.mean(dim=0, keepdim=True) # Calculate mean across samples (N) for each dimension (d)\n",
        "W_E_centered = W_E - W_E_mean # Broadcasting subtracts the mean from each row\n",
        "\n",
        "# 2. Perform SVD on the centered data\n",
        "# U shape: (N, K), S shape: (K,), Vt shape: (K, d) where K = min(N, d)\n",
        "# full_matrices=False is generally more efficient\n",
        "U, S, Vt = torch.linalg.svd(W_E_centered, full_matrices=False)\n",
        "\n",
        "# --- Results ---\n",
        "\n",
        "# PCA Scores (Data projected onto principal components):\n",
        "# This is often the primary result needed for visualization/dimensionality reduction.\n",
        "# It represents each original sample (row in W_E) in the new PCA coordinate system.\n",
        "pca_scores = U * S  # Shape: (N, K) - Scales the left singular vectors by singular values\n",
        "\n",
        "# Principal Components (Loadings / Directions of maximum variance):\n",
        "# These are the rows of Vt. Each row is a d-dimensional vector representing a principal direction.\n",
        "principal_components = Vt # Shape: (K, d)\n",
        "\n",
        "# Explained Variance Ratio (requires a bit more calculation):\n",
        "explained_variance = S.square() / (W_E.shape[0] - 1) # Variance explained by each component\n",
        "total_variance = explained_variance.sum()\n",
        "explained_variance_ratio = explained_variance / total_variance\n",
        "\n",
        "# Plot score on PC1 vs token index k (similar to previous PCA example)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(np.arange(W_E.shape[0]), pca_scores[:, 0].detach().cpu().numpy(), alpha=0.7, s=10)\n",
        "plt.xlabel(\"Input Token Index (k)\")\n",
        "plt.ylabel(\"Score on PC1\")\n",
        "plt.title(\"PyTorch PCA: Score on First Principal Component vs. Input Token Index\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "E0QhpicycT8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the principal component is linear, i.e., the model learns an embedding that is given by embedded_value = w * value + b, with value being the input token index. w is actually negative, and b is ~2.\n",
        "\n",
        "We now wish to investigate how important this first component, which is a linear function of the inputs, is in comparison with the other components. We do this by looking at how much variance is explained by each of the components."
      ],
      "metadata": {
        "id": "XXtXYhAueFLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Calculate Explained Variance (using PyTorch SVD results S) ---\n",
        "\n",
        "# Variance explained by each component (proportional to singular value squared)\n",
        "# Using N (W_E.shape[0]) instead of N-1 is fine for the ratio calculation\n",
        "explained_variance = S.square() / W_E.shape[0] # Variance = (singular value / sqrt(N))^2\n",
        "total_variance = explained_variance.sum()\n",
        "explained_variance_ratio = explained_variance / total_variance\n",
        "\n",
        "# Convert to numpy for printing/plotting if needed\n",
        "explained_variance_ratio_np = explained_variance_ratio.detach().cpu().numpy()\n",
        "\n",
        "# --- Print the Ratios ---\n",
        "print(f\"Explained Variance Ratio by Principal Component:\")\n",
        "# Limit printing to the number of components computed or a reasonable max (e.g., 10)\n",
        "num_components_to_print = min(len(explained_variance_ratio_np), 10)\n",
        "cumulative_variance = 0.0\n",
        "for i in range(num_components_to_print):\n",
        "    ratio = explained_variance_ratio_np[i]\n",
        "    cumulative_variance += ratio\n",
        "    print(f\"  PC{i+1}: {ratio:.4f} ({ratio*100:.2f}%) \\t| Cumulative: {cumulative_variance:.4f} ({cumulative_variance*100:.2f}%)\")\n",
        "\n",
        "# If you computed more components than printed:\n",
        "if len(explained_variance_ratio_np) > num_components_to_print:\n",
        "    print(f\"  ...\")\n",
        "    print(f\"Total Cumulative Variance (all {len(explained_variance_ratio_np)} components): {explained_variance_ratio_np.sum():.4f} ({explained_variance_ratio_np.sum()*100:.2f}%)\")\n",
        "\n",
        "\n",
        "# --- Visualize with a Scree Plot ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "component_indices = np.arange(1, len(explained_variance_ratio_np) + 1)\n",
        "\n",
        "# Plot individual explained variance ratios\n",
        "plt.bar(component_indices, explained_variance_ratio_np, alpha=0.7, align='center',\n",
        "        label='Individual Explained Variance Ratio')\n",
        "\n",
        "# Plot cumulative explained variance ratio\n",
        "cumulative_variance_ratio_np = np.cumsum(explained_variance_ratio_np)\n",
        "plt.plot(component_indices, cumulative_variance_ratio_np, marker='o', linestyle='--',\n",
        "         label='Cumulative Explained Variance Ratio')\n",
        "\n",
        "# Add threshold lines (optional, but common)\n",
        "plt.axhline(y=0.9, color='r', linestyle=':', linewidth=1, label='90% Threshold')\n",
        "plt.axhline(y=0.95, color='g', linestyle=':', linewidth=1, label='95% Threshold')\n",
        "\n",
        "\n",
        "plt.xlabel('Principal Component Index')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Scree Plot - Explained Variance by Principal Component')\n",
        "# Ensure x-axis ticks match component indices if not too many\n",
        "if len(component_indices) <= 15:\n",
        "    plt.xticks(ticks=component_indices)\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.ylim(0, 1.1) # Set y-axis limit slightly above 1.0\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "13URfrLpepYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the first component explains ~0.75 of the variance, and the second component ~0.25. The others are basically irrelevant. This does however mean that we also need to look at the second component to understand what is happening with the embedding. Let us do so now."
      ],
      "metadata": {
        "id": "HvM2bHtSeusY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot score on PC2 vs token index k\n",
        "# This is basically the same code as before, but we now plot the second component rather than the first one\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(np.arange(W_E.shape[0]), pca_scores[:, 1].detach().cpu().numpy(), alpha=0.7, s=10)\n",
        "plt.xlabel(\"Input Token Index (k)\")\n",
        "plt.ylabel(\"Score on PC2\")\n",
        "plt.title(\"PyTorch PCA: Score on Second Principal Component vs. Input Token Index\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rgfsr8XffEXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot score on PC2 vs token index k\n",
        "# This is basically the same code as before, but we now plot the third component rather than the first one\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(np.arange(W_E.shape[0]), pca_scores[:, 2].detach().cpu().numpy(), alpha=0.7, s=10)\n",
        "plt.xlabel(\"Input Token Index (k)\")\n",
        "plt.ylabel(\"Score on PC2\")\n",
        "plt.title(\"PyTorch PCA: Score on Third Principal Component vs. Input Token Index\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fX9SL-_HahxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks like a parabola. I have no clue why! Let's plot below the scores on the first component against the scores on the second component.\n",
        "\n"
      ],
      "metadata": {
        "id": "tcWuRzYngWy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "token_indices = np.arange(p)\n",
        "scatter = plt.scatter(\n",
        "    pca_scores[:, 0].detach().cpu().numpy(),  # X-coordinates are scores on PC1\n",
        "    pca_scores[:, 1].detach().cpu().numpy(),  # Y-coordinates are scores on PC2\n",
        "    c=token_indices,                          # Color points based on the input token index (0-9)\n",
        "    cmap='viridis',                           # Colormap (e.g., 'viridis', 'plasma')\n",
        "    alpha=0.8,\n",
        "    s=50                                      # Increase point size for visibility\n",
        ")\n",
        "plt.xlabel(\"Score on PC1 (Linear Component)\")\n",
        "plt.ylabel(\"Score on PC2 (Parabolic Component)\")\n",
        "plt.title(\"Embeddings Projected onto First Two Principal Components\")\n",
        "plt.colorbar(scatter, label='Input Token Index (k)')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.gca().set_aspect('equal', adjustable='box') # Try to keep scales comparable"
      ],
      "metadata": {
        "id": "Hn4QQgl6guGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An attempt at a conclusion regarding the model's learned embedding:\n",
        "\n",
        "The model projects inputs (numbers 0-9) onto a low-dimensional (2D) manifold within the embedding space. This representation encodes the number's value primarily along one linear axis (PC1) and secondarily along an orthogonal parabolic axis (PC2). This specific geometric arrangement is likely learned to facilitate the downstream integer addition computation. (Leveraging linear order + perhaps magnitude/centrality).\n",
        "\n",
        "Let's consider some potential next steps to further understand the algorithm the model is implementing:\n",
        "\n",
        "Step 2: Attention Pattern Check\n",
        "\n",
        "Goal: Check how the model combines a and b info at the = position.\n",
        "\n",
        "Select Sample Inputs: Get various pairs (a, b).\n",
        "\n",
        "Hook Attention Weights: For each head, get the attention weights from position 2 (query: =) to positions 0 (a), 1 (b), and 2 (=). Store these weights [w_to_a, w_to_b, w_to_=] for each head and input pair.\n",
        "\n",
        "Visualize Average Patterns: For each head, average the weights w_to_a, w_to_b, w_to_= across all sample inputs. Plot these three average weights per head (e.g., using bar charts).\n",
        "\n",
        "Analyze: Does position 2 attend significantly to both position 0 (a) and position 1 (b)? Are there clear differences between heads? Is attention to position 2 (=) itself low?\n",
        "\n",
        "\n",
        "Expected Outcome: Confirm attention gathers info from a and b. Note basic patterns.\n",
        "\n",
        "Step 3: MLP Analysis\n",
        "\n",
        "Goal: Understand how the MLP computes a+b from the combined a and b representations.\n",
        "\n",
        "Hook MLP Activations: Get the n-dimensional activation vector after the ReLU inside the MLP, specifically for the state calculated at position 2 (=). Do this for various input pairs (a, b).\n",
        "\n",
        "Correlate Neurons to Task Variables: For each MLP neuron, calculate its activation across the sample pairs. Find the correlation between each neuron's activation and key variables: a, b, the target sum a+b, PC1 score of a, PC2 score of a, PC1 score of b, PC2 score of b.\n",
        "\n",
        "Identify Key Neurons: Note which neurons correlate strongly with the target sum a+b. Note any correlating strongly with input features (like PC1/PC2 scores).\n",
        "\n",
        "(Optional) Visualize Key Neuron Activations: Make 2D heatmaps (x-axis a, y-axis b) for a few key neurons (e.g., sum-correlated neurons). Does the activation map look like a+b?\n",
        "\n",
        "Probe MLP State: Train a linear regression model to predict the scalar a+b using the n-dimensional MLP activation vector as input. Check probe accuracy. High accuracy implies MLP state linearly encodes the sum.\n",
        "\n",
        "Formulate Hypothesis: Based on correlations and probes, hypothesize how the MLP uses the input features (represented by PC1/PC2) via its neurons to arrive at an internal state that encodes the sum a+b."
      ],
      "metadata": {
        "id": "yhkOl0cSkR6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "captured_mlp_activations = []\n",
        "\n",
        "# Define the hook function\n",
        "def store_mlp_post_activation_hook(\n",
        "    activation: torch.Tensor,\n",
        "    hook: 'HookPoint'\n",
        "):\n",
        "    \"\"\"\n",
        "    Hook function to capture the MLP activation output (after ReLU).\n",
        "    Selects only the activation for the target position (pos 2).\n",
        "    \"\"\"\n",
        "    # activation shape is likely [batch_size, sequence_length, d_mlp]\n",
        "    # We want the activation at the '=' token, which is at index 2\n",
        "    target_activation = activation[:, 2, :].detach().cpu() # Shape: [batch_size, d_mlp]\n",
        "    captured_mlp_activations.append(target_activation)\n",
        "\n",
        "    # MUST return the activation to allow the forward pass to continue\n",
        "    return activation\n",
        "\n",
        "# Identify the exact hook point name\n",
        "mlp_hook_point_name = \"blocks.0.mlp.hook_post\" # Output AFTER ReLU\n",
        "\n",
        "# Clear previous captures before running\n",
        "captured_mlp_activations = []\n",
        "\n",
        "activation_list = []\n",
        "target_sums = []\n",
        "\n",
        "print(f\"Processing {len(dataset)} input pairs...\")\n",
        "for a, b, equals in dataset:\n",
        "    input_tokens = torch.tensor([[a, b, equals]], dtype=torch.long)\n",
        "\n",
        "    # Clear captures for this specific run\n",
        "    captured_mlp_activations = []\n",
        "\n",
        "    # Run with hooks\n",
        "    _ = model.run_with_hooks(\n",
        "        input_tokens,\n",
        "        fwd_hooks=[(mlp_hook_point_name, store_mlp_post_activation_hook)]\n",
        "    )\n",
        "\n",
        "    # Store the result if captured\n",
        "    if captured_mlp_activations:\n",
        "        # Get the vector for the first (and only) item in the batch\n",
        "        mlp_activation_vector = captured_mlp_activations[0][0].numpy() # Convert to numpy array\n",
        "        activation_list.append(mlp_activation_vector)\n",
        "        target_sums.append(a + b)\n",
        "    else:\n",
        "        print(f\"Warning: Hook did not capture activation for pair ({a}, {b})\")\n",
        "\n",
        "print(f\"Collected {len(activation_list)} activation vectors.\")\n",
        "\n",
        "# Convert to numpy arrays for training the probe\n",
        "X_probe = np.array(activation_list) # Shape: [num_samples, d_mlp]\n",
        "y_probe = y_probe = np.array([item.cpu().numpy() for item in target_sums])     # Shape: [num_samples]\n",
        "\n",
        "print(\"Probe input data shapes:\")\n",
        "print(\"X_probe:\", X_probe.shape)\n",
        "print(\"y_probe:\", y_probe.shape)"
      ],
      "metadata": {
        "id": "mGdQDpBLZ-45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have the outputs of MLP for each input pair. We will try to fit this to a linear function to see if this directly correlates to the desired representation of the sum."
      ],
      "metadata": {
        "id": "YfEas_TZcK1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_probe, y_probe, test_size=0.3, random_state=42 # Adjust test_size if needed\n",
        ")\n",
        "\n",
        "# 2. Initialize and train the Linear Regression model\n",
        "probe_model = LinearRegression()\n",
        "probe_model.fit(X_train, y_train)\n",
        "\n",
        "# 3. Make predictions on the test set\n",
        "y_pred = probe_model.predict(X_test)\n",
        "\n",
        "# 4. Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Linear Probe Performance:\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"  R-squared (R2): {r2:.4f}\")\n",
        "\n",
        "# Optional: Basic baseline comparison (predicting the mean of training labels)\n",
        "baseline_pred = np.full_like(y_test, y_train.mean())\n",
        "baseline_mse = mean_squared_error(y_test, baseline_pred)\n",
        "baseline_r2 = r2_score(y_test, baseline_pred) # Will be close to 0 by definition\n",
        "\n",
        "print(f\"\\nBaseline Performance (Predicting Mean):\")\n",
        "print(f\"  Baseline MSE: {baseline_mse:.4f}\")\n",
        "print(f\"  Baseline R2: {baseline_r2:.4f}\")\n",
        "\n",
        "\n",
        "# Optional: Visualize predictions vs actual\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.7, label='Predictions')\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='red', label='Perfect Prediction')\n",
        "plt.xlabel(\"True Values (a+b)\")\n",
        "plt.ylabel(\"Predicted Values (a+b)\")\n",
        "plt.title(\"Linear Probe: True vs. Predicted Sums\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZwCrg85bctoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The linear regression we've done is almost perfectly accurate, which means that the post-ReLU activations contain a linear representation of the target sum a+b. This means that the bulk of the computation is being done by the MLP. Hence, we can likely find 'sum neurons', i.e., neurons that are mostly responsible for implementing the computation. We will do this by correlation analysis: what neurons' activations correlate more strongly with a+b?"
      ],
      "metadata": {
        "id": "WlxnMMODduVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Ensure inputs are numpy arrays\n",
        "X_probe = np.asarray(X_probe)\n",
        "y_probe = np.asarray(y_probe)\n",
        "\n",
        "num_samples, d_mlp = X_probe.shape\n",
        "neuron_sum_correlations = np.zeros(d_mlp)\n",
        "\n",
        "# Calculate Pearson correlation for each neuron\n",
        "for i in range(d_mlp):\n",
        "    # pearsonr returns (correlation_coefficient, p_value)\n",
        "    correlation, _ = pearsonr(X_probe[:, i], y_probe)\n",
        "    neuron_sum_correlations[i] = correlation\n",
        "\n",
        "# Handle potential NaN values if a neuron's activation was constant (zero variance)\n",
        "neuron_sum_correlations = np.nan_to_num(neuron_sum_correlations)\n",
        "\n",
        "print(f\"Calculated correlations for {d_mlp} neurons.\")\n",
        "# The 'neuron_sum_correlations' array now holds the correlation of each neuron with a+b.\n",
        "\n",
        "# Find the indices of neurons with highest absolute correlation\n",
        "top_n = 100\n",
        "# Get indices sorted by absolute correlation, descending\n",
        "indices_sorted_by_abs_corr = np.argsort(np.abs(neuron_sum_correlations))[::-1]\n",
        "\n",
        "print(f\"\\nTop {top_n} neurons by absolute correlation with sum (a+b):\")\n",
        "for i in range(top_n):\n",
        "    idx = indices_sorted_by_abs_corr[i]\n",
        "    print(f\"  Neuron {idx}: Correlation = {neuron_sum_correlations[idx]:.4f}\")"
      ],
      "metadata": {
        "id": "qSIkqzcDeWQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "# --- Visualization 1: Histogram of Correlations ---\n",
        "print(\"\\nGenerating Histogram of Correlations...\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(neuron_sum_correlations, bins=50, kde=True)\n",
        "plt.title('Histogram of Neuron Activations\\' Correlation with Sum (a+b)')\n",
        "plt.xlabel('Pearson Correlation Coefficient')\n",
        "plt.ylabel('Number of Neurons')\n",
        "plt.grid(axis='y', alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "# --- Visualization 2: Sorted Correlation Plot (Stem Plot) ---\n",
        "print(\"\\nGenerating Sorted Correlation Plot...\")\n",
        "sorted_indices = np.argsort(neuron_sum_correlations)\n",
        "sorted_correlations = neuron_sum_correlations[sorted_indices]\n",
        "ranks = np.arange(d_mlp)\n",
        "\n",
        "pos_mask = sorted_correlations >= 0\n",
        "neg_mask = sorted_correlations < 0\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "# Plot positive correlations\n",
        "markerline_pos, stemlines_pos, baseline_pos = plt.stem(\n",
        "    ranks[pos_mask], sorted_correlations[pos_mask],\n",
        "    linefmt='b-', markerfmt='bo', basefmt=' ', label='Positive Corr'\n",
        ")\n",
        "plt.setp(stemlines_pos, linewidth=1, alpha=0.7)\n",
        "plt.setp(markerline_pos, markersize=3, alpha=0.7)\n",
        "\n",
        "# Plot negative correlations\n",
        "markerline_neg, stemlines_neg, baseline_neg = plt.stem(\n",
        "    ranks[neg_mask], sorted_correlations[neg_mask],\n",
        "    linefmt='r-', markerfmt='ro', basefmt=' ', label='Negative Corr'\n",
        ")\n",
        "plt.setp(stemlines_neg, linewidth=1, alpha=0.7)\n",
        "plt.setp(markerline_neg, markersize=3, alpha=0.7)\n",
        "\n",
        "plt.title('Neuron Correlations with Sum (a+b), Sorted by Value')\n",
        "plt.xlabel('Neuron Rank (Sorted by Correlation)')\n",
        "plt.ylabel('Pearson Correlation Coefficient')\n",
        "plt.ylim(-1.05, 1.05)\n",
        "plt.grid(axis='y', alpha=0.5)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "num_outputs=20\n",
        "# --- Visualization 3: Heatmap of Sorted W_L ---\n",
        "# Ensure W_L is defined correctly (num_outputs x d_mlp)\n",
        "# Example: W_L = model.W_U @ model.blocks[0].mlp.W_out (or however you get it)\n",
        "if 'W_L' in locals() and W_L.shape == (num_outputs, d_mlp):\n",
        "    print(\"\\nGenerating Heatmap of Sorted W_L...\")\n",
        "    # Sort columns (neurons) of W_L based on correlation\n",
        "    neuron_corr_sorted_indices = np.argsort(neuron_sum_correlations)\n",
        "    W_L_sorted_by_neuron_corr = W_L[:, neuron_corr_sorted_indices]\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    sns.heatmap(W_L_sorted_by_neuron_corr, cmap='coolwarm', center=0) # 'coolwarm' is good for weights\n",
        "    plt.title('Neuron-Logit Map (W_L) Columns Sorted by Neuron Correlation')\n",
        "    plt.xlabel('Neurons (Sorted by Correlation with Sum a+b)')\n",
        "    plt.ylabel(f'Output Logit (0 to {num_outputs-1})')\n",
        "    plt.yticks(np.arange(num_outputs) + 0.5, labels=np.arange(num_outputs), rotation=0)\n",
        "    plt.xticks([]) # Hide neuron indices as they are too dense\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nSkipping Heatmap of W_L: W_L matrix not found or incorrect shape.\")\n",
        "    print(f\"Expected shape: ({num_outputs}, {d_mlp})\")\n",
        "    if 'W_L' in locals():\n",
        "        print(f\"Actual shape: {W_L.shape}\")\n",
        "\n",
        "\n",
        "# --- Visualization 4: Heatmap of Sorted Neuron Activations ---\n",
        "# Ensure X_probe and y_probe are defined correctly\n",
        "if 'X_probe' in locals() and 'y_probe' in locals() and X_probe.shape[0] == y_probe.shape[0]:\n",
        "    print(\"\\nGenerating Heatmap of Sorted Neuron Activations...\")\n",
        "    # 1. Sort columns (neurons) by correlation\n",
        "    neuron_corr_sorted_indices = np.argsort(neuron_sum_correlations)\n",
        "    X_probe_neurons_sorted = X_probe[:, neuron_corr_sorted_indices]\n",
        "\n",
        "    # 2. Sort rows (inputs) by the target sum y_probe\n",
        "    input_sum_sorted_indices = np.argsort(y_probe)\n",
        "    X_probe_fully_sorted = X_probe_neurons_sorted[input_sum_sorted_indices, :]\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    # Use robust=True to handle potential outliers in activations\n",
        "    # Use cbar=False to avoid clutter, as absolute activation values might vary\n",
        "    sns.heatmap(X_probe_fully_sorted, cmap='viridis', robust=True, cbar=False)\n",
        "    plt.title('MLP Activations (Sorted)')\n",
        "    plt.xlabel('Neurons (Sorted by Correlation with Sum a+b)')\n",
        "    plt.ylabel('Input Samples (Sorted by Sum a+b)')\n",
        "    plt.xticks([]) # Hide neuron indices\n",
        "    plt.yticks([]) # Hide sample indices\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nSkipping Heatmap of Activations: X_probe or y_probe not found or shapes mismatch.\")\n",
        "    if 'X_probe' in locals():\n",
        "        print(f\"X_probe shape: {X_probe.shape}\")\n",
        "    if 'y_probe' in locals():\n",
        "        print(f\"y_probe shape: {y_probe.shape}\")"
      ],
      "metadata": {
        "id": "G-Ep8Ey9cc6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe widespread strong positive correlation of neuron activations with the target sum. In fact, more than 130 neurons have a correlation exceeding 0.9 with the target sum. This shows that the MLP represents the sum in a highly distributed manner. There aren't localized, specialized 'sum' neurons, per se.\n",
        "\n",
        "We will now visualize the activation patterns of a few top-correlated neurons as heatmaps across the grid of all possible (a, b) input pairs. This reveals the specific function each of these top neurons computes according to input pairs. Hopefully from this we can hypothesize how they combine the principal components we identified earlier to achieve their strong correlation."
      ],
      "metadata": {
        "id": "WwBL5h1Cg7IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "equals_token_id = 10\n",
        "N = 10\n",
        "top_n_to_visualize = 3\n",
        "top_neuron_indices = indices_sorted_by_abs_corr[:top_n_to_visualize]\n",
        "mlp_hook_point_name = \"blocks.0.mlp.hook_post\" # Output AFTER ReLU\n",
        "\n",
        "# --- Data Storage ---\n",
        "# Create heatmaps initialized with NaN (or zero)\n",
        "neuron_activation_heatmaps = {\n",
        "    idx: np.full((N, N), np.nan) for idx in top_neuron_indices\n",
        "}\n",
        "captured_mlp_activation_storage = {} # Temporary storage during hook\n",
        "\n",
        "# --- Hook Function ---\n",
        "def capture_mlp_post_hook(activation: torch.Tensor, hook: 'HookPoint'):\n",
        "    \"\"\"Captures MLP activation at position 2.\"\"\"\n",
        "    captured_mlp_activation_storage['activation'] = activation[:, 2, :].detach().cpu()\n",
        "    return activation\n",
        "\n",
        "# --- Generate Activations ---\n",
        "print(f\"Generating activations for {top_n_to_visualize} neurons across {N}x{N} grid...\")\n",
        "model.eval() # Ensure model is in evaluation mode\n",
        "with torch.no_grad(): # No need to track gradients\n",
        "    for a in range(N):\n",
        "        for b in range(N):\n",
        "              input_tokens = torch.tensor([[a, b, equals_token_id]], dtype=torch.long)\n",
        "\n",
        "              # Clear previous capture\n",
        "              captured_mlp_activation_storage.clear()\n",
        "\n",
        "              # Run with hook\n",
        "              _ = model.run_with_hooks(\n",
        "                  input_tokens,\n",
        "                  fwd_hooks=[(mlp_hook_point_name, capture_mlp_post_hook)]\n",
        "              )\n",
        "\n",
        "              # Store the activations for the target neurons\n",
        "              if 'activation' in captured_mlp_activation_storage:\n",
        "                  full_activation_vector = captured_mlp_activation_storage['activation'][0] # Batch size 1\n",
        "                  for neuron_idx in top_neuron_indices:\n",
        "                      neuron_activation_heatmaps[neuron_idx][a, b] = full_activation_vector[neuron_idx].item()\n",
        "print(\"Activation generation complete.\")\n",
        "\n",
        "# --- Plotting ---\n",
        "fig, axes = plt.subplots(1, top_n_to_visualize, figsize=(6 * top_n_to_visualize, 5))\n",
        "if top_n_to_visualize == 1: # Handle case of single subplot\n",
        "    axes = [axes]\n",
        "\n",
        "fig.suptitle(\"MLP Neuron Activations (Post-ReLU) at Position '=' vs. Inputs (a, b)\")\n",
        "\n",
        "for i, neuron_idx in enumerate(top_neuron_indices):\n",
        "    ax = axes[i]\n",
        "    heatmap_data = neuron_activation_heatmaps[neuron_idx]\n",
        "    sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"viridis\", ax=ax,\n",
        "                linewidths=.5, linecolor='gray', cbar=True, square=True,\n",
        "                # Mask cells with NaN so they don't show color\n",
        "                mask=np.isnan(heatmap_data))\n",
        "    ax.set_title(f\"Neuron {neuron_idx}\\nCorr w/ Sum: {neuron_sum_correlations[neuron_idx]:.3f}\")\n",
        "    ax.set_xlabel(\"Input b = F(n-1)\")\n",
        "    ax.set_ylabel(\"Input a = F(n-2)\")\n",
        "    # Set ticks to match input values\n",
        "    ax.set_xticks(np.arange(N) + 0.5)\n",
        "    ax.set_yticks(np.arange(N) + 0.5)\n",
        "    ax.set_xticklabels(np.arange(N))\n",
        "    ax.set_yticklabels(np.arange(N))\n",
        "    ax.invert_yaxis() # Convention often puts (0,0) at top-left for matrices\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZTxa377eiafB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmaps show that the top-correlated neurons implement a function approximating ReLU(linear_function_of_sum). We observe a region of 0s (e.g., all values adding up to 6 or less for the middle neuron above), which is likely a result of the ReLU cutting off negatrive pre-activations, which basically creates a threshold below which the neurons do not fire. Further, we get roughly linear behavior with the sum of inputs above the threshold, as expected from the ReLU. The fact that the patterns are so similar across neurons again emphasizes that the computation implemented by the model is distributed.\n",
        "\n",
        "We will now examine the pre-ReLU activations for these same neurons to test our hypothesis. If the core computation is indeed a linear function of the sum that is simply thresholded by ReLU, then the pre-ReLU heatmaps should reveal this underlying linear relationship more clearly across the entire input grid, without the large zeroed-out regions, thus confirming the thresholding mechanism."
      ],
      "metadata": {
        "id": "Tml6Dlfukni5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_hook_point_name_pre = \"blocks.0.mlp.hook_pre\" # Input BEFORE ReLU\n",
        "\n",
        "# --- Data Storage ---\n",
        "neuron_pre_activation_heatmaps = {\n",
        "    idx: np.full((N, N), np.nan) for idx in top_neuron_indices\n",
        "}\n",
        "captured_mlp_pre_activation_storage = {} # Temporary storage during hook\n",
        "\n",
        "# --- Hook Function ---\n",
        "def capture_mlp_pre_hook(activation: torch.Tensor, hook: 'HookPoint'):\n",
        "    \"\"\"Captures MLP activation input (before ReLU) at position 2.\"\"\"\n",
        "    captured_mlp_pre_activation_storage['activation'] = activation[:, 2, :].detach().cpu()\n",
        "    return activation\n",
        "\n",
        "# --- Generate Activations ---\n",
        "print(f\"Generating PRE-ReLU activations for {len(top_neuron_indices)} neurons across {N}x{N} grid...\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for a in range(N):\n",
        "        for b in range(N):\n",
        "              input_tokens = torch.tensor([[a, b, equals_token_id]], dtype=torch.long)\n",
        "\n",
        "              # Clear previous capture\n",
        "              captured_mlp_pre_activation_storage.clear()\n",
        "\n",
        "              # Run with PRE-ReLU hook\n",
        "              _ = model.run_with_hooks(\n",
        "                  input_tokens,\n",
        "                  fwd_hooks=[(mlp_hook_point_name_pre, capture_mlp_pre_hook)]\n",
        "              )\n",
        "\n",
        "              # Store the activations for the target neurons\n",
        "              if 'activation' in captured_mlp_pre_activation_storage:\n",
        "                  full_activation_vector = captured_mlp_pre_activation_storage['activation'][0]\n",
        "                  for neuron_idx in top_neuron_indices:\n",
        "                      neuron_pre_activation_heatmaps[neuron_idx][a, b] = full_activation_vector[neuron_idx].item()\n",
        "print(\"PRE-ReLU activation generation complete.\")\n",
        "\n",
        "# --- Plotting ---\n",
        "fig, axes = plt.subplots(1, len(top_neuron_indices), figsize=(6 * len(top_neuron_indices), 5))\n",
        "if len(top_neuron_indices) == 1: axes = [axes]\n",
        "\n",
        "fig.suptitle(\"MLP Neuron Activations (PRE-ReLU) at Position '=' vs. Inputs (a, b)\")\n",
        "\n",
        "for i, neuron_idx in enumerate(top_neuron_indices):\n",
        "    ax = axes[i]\n",
        "    # Use a diverging colormap like 'coolwarm' or 'RdBu_r' for pre-ReLU\n",
        "    # as values can be positive or negative. Center the color map at 0.\n",
        "    heatmap_data = neuron_pre_activation_heatmaps[neuron_idx]\n",
        "    max_abs_val = np.nanmax(np.abs(heatmap_data)) # Find max absolute value for symmetric color scale\n",
        "    sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax,\n",
        "                linewidths=.5, linecolor='gray', cbar=True, square=True,\n",
        "                mask=np.isnan(heatmap_data),\n",
        "                vmin=-max_abs_val, vmax=max_abs_val) # Center colormap around 0\n",
        "    ax.set_title(f\"Neuron {neuron_idx} (Pre-ReLU)\")\n",
        "    ax.set_xlabel(\"Input b = F(n-1)\")\n",
        "    ax.set_ylabel(\"Input a = F(n-2)\")\n",
        "    ax.set_xticks(np.arange(N) + 0.5)\n",
        "    ax.set_yticks(np.arange(N) + 0.5)\n",
        "    ax.set_xticklabels(np.arange(N))\n",
        "    ax.set_yticklabels(np.arange(N))\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lncq4AklkrLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We in fact observe a linear relation between value of the sum and pre-ReLU neruon value. However, note that the top neuron has a lot of zero values, whereas the other two go from negative to positive with similar magnitude. This indicates that they learn different linear functions (slopes, biases). However, part of this difference is thresholded away by the ReLU activation.\n",
        "\n",
        "We now want to fit the pre-ReLU values to the PCA components to understand exactly what function thereof they are computing."
      ],
      "metadata": {
        "id": "JfAJwAf5mr_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "target_neuron_idx = 408\n",
        "feature_list = []\n",
        "target_activation_list = []\n",
        "\n",
        "# Ensure pca_scores is NumPy for easier indexing if needed, keep precision\n",
        "pca_scores_np = pca_scores.detach().cpu().numpy().astype(np.float32)\n",
        "pre_activation_target_neuron = neuron_pre_activation_heatmaps[target_neuron_idx] # Get the heatmap\n",
        "\n",
        "for a in range(N):\n",
        "    for b in range(N):\n",
        "        if a + b < N: # Only use valid sums\n",
        "            # Get pre-calculated pre-ReLU activation\n",
        "            target_activation = pre_activation_target_neuron[a, b]\n",
        "\n",
        "            # Ensure we don't include NaN values if any exist in heatmap\n",
        "            if not np.isnan(target_activation):\n",
        "                # Extract PCA features for a and b\n",
        "                pc1_a = pca_scores_np[a, 0]\n",
        "                pc2_a = pca_scores_np[a, 1]\n",
        "                pc1_b = pca_scores_np[b, 0]\n",
        "                pc2_b = pca_scores_np[b, 1]\n",
        "\n",
        "                # Add more features if desired (e.g., interactions)\n",
        "                # For now, just the 4 core PCA features\n",
        "                features = [pc1_a, pc2_a, pc1_b, pc2_b]\n",
        "                feature_list.append(features)\n",
        "                target_activation_list.append(target_activation)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "X_features_for_neuron_model = np.array(feature_list, dtype=np.float32)\n",
        "y_target_neuron_pre_activation = np.array(target_activation_list, dtype=np.float32)\n",
        "\n",
        "# Define feature names for interpreting coefficients\n",
        "feature_names = ['PC1(a)', 'PC2(a)', 'PC1(b)', 'PC2(b)']\n",
        "\n",
        "print(f\"Prepared data for modeling neuron {target_neuron_idx}:\")\n",
        "print(f\"  X_features shape: {X_features_for_neuron_model.shape}\")\n",
        "print(f\"  y_target shape: {y_target_neuron_pre_activation.shape}\")\n",
        "\n",
        "# --- Fit the Linear Regression Model ---\n",
        "neuron_model = LinearRegression()\n",
        "neuron_model.fit(X_features_for_neuron_model, y_target_neuron_pre_activation)\n",
        "\n",
        "# --- Evaluate the fit ---\n",
        "y_neuron_pred = neuron_model.predict(X_features_for_neuron_model)\n",
        "r2_neuron_fit = r2_score(y_target_neuron_pre_activation, y_neuron_pred)\n",
        "\n",
        "print(f\"\\nLinear Model Fit for Neuron {target_neuron_idx} (Pre-ReLU):\")\n",
        "print(f\"  R-squared of fit: {r2_neuron_fit:.4f}\")\n",
        "\n",
        "# --- Interpret the model ---\n",
        "print(f\"  Learned coefficients:\")\n",
        "coeffs = pd.Series(neuron_model.coef_, index=feature_names)\n",
        "print(coeffs)\n",
        "print(f\"  Intercept: {neuron_model.intercept_:.4f}\")"
      ],
      "metadata": {
        "id": "2jfGCoeAn6DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The excellent R value confirms that Neuron 240's pre-ReLU activation is accurately modeled as a linear combination of the primary (linear value) and secondary (parabolic centrality) features derived from the input embeddings. The near-identical coefficients for the a and b components demonstrate symmetric treatment of the inputs, while the dominance of the PC1 coefficients confirms the neuron primarily computes a function proportional to the sum a+b. The smaller but non-zero PC2 coefficients suggest this core computation is subtly modulated based on the inputs' centrality, likely for fine-tuning or boundary adjustments within the N=10 range.\n",
        "\n",
        "Something confuses me: it seems that the model just directly learns to add, as the linear component of the PCA is the most significant. However, the fact that there's a non-negligible contribution from the second component of the PCA makes me doubt this. In fact, the first one only explains 73% of the variance! That's a lot, but far from everything.\n",
        "\n",
        "This motivates performing an ablation study, in which we get rid of the second principal component and investigate how (if at all) this affects model performance."
      ],
      "metadata": {
        "id": "uT4iKK2dpYkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_E_mean_gpu = W_E_mean.to(model.cfg.device) # Ensure mean is on correct device\n",
        "pc1_vec = principal_components[0].to(model.cfg.device)\n",
        "pc2_vec = principal_components[1].to(model.cfg.device)\n",
        "\n",
        "# --- Squeeze the mean vector to be explicitly 1D ---\n",
        "W_E_mean_1d_gpu = W_E_mean_gpu.squeeze(0) # Shape becomes [d_model]\n",
        "\n",
        "def ablate_pc2_hook(\n",
        "    activation: torch.Tensor, # Shape [batch, seq_len, d_model]\n",
        "    hook: 'HookPoint'\n",
        "):\n",
        "    # Make a copy to modify\n",
        "    ablated_activation = activation.clone()\n",
        "\n",
        "    # Iterate through batch and relevant sequence positions (0 and 1 for a, b)\n",
        "    for batch_idx in range(activation.shape[0]):\n",
        "        for seq_idx in [0, 1]: # Ablate for token a and token b\n",
        "            emb = activation[batch_idx, seq_idx, :] # Should be [d_model]\n",
        "\n",
        "            # --- Subtract the squeezed 1D mean ---\n",
        "            emb_centered = emb - W_E_mean_1d_gpu # Both are [d_model], result is [d_model]\n",
        "\n",
        "            # Project - Now both inputs to torch.dot are 1D\n",
        "            score_pc1 = torch.dot(emb_centered, pc1_vec)\n",
        "            # score_pc2 = torch.dot(emb_centered, pc2_vec) # Still not needed\n",
        "\n",
        "            # Reconstruct using only PC1 contribution\n",
        "            # score_pc1 is a scalar, pc1_vec is [d_model] -> broadcasting works\n",
        "            emb_centered_ablated = score_pc1 * pc1_vec\n",
        "\n",
        "            # Add the 1D mean back\n",
        "            emb_ablated = emb_centered_ablated + W_E_mean_1d_gpu\n",
        "\n",
        "            # Put modified embedding back\n",
        "            ablated_activation[batch_idx, seq_idx, :] = emb_ablated\n",
        "\n",
        "    # Return the modified activations\n",
        "    return ablated_activation\n",
        "\n",
        "# --- How to run ---\n",
        "embedding_hook_point = \"hook_embed\" # Or \"hook_pos_embed\", check model.hook_dict\n",
        "logits = model.run_with_hooks(\n",
        "     input_tokens,\n",
        "     fwd_hooks=[(embedding_hook_point, ablate_pc2_hook)]\n",
        " )\n",
        "\n",
        "test_pairs = []\n",
        "test_labels_list = []\n",
        "for a in range(N):\n",
        "    for b in range(N):\n",
        "          test_pairs.append([a, b, equals_token_id])\n",
        "          test_labels_list.append(a + b)\n",
        "\n",
        "# Convert to tensors\n",
        "test_input_tokens = torch.tensor(test_pairs, dtype=torch.long).to(model.cfg.device)\n",
        "test_labels = torch.tensor(test_labels_list, dtype=torch.long).to(model.cfg.device)\n",
        "\n",
        "print(f\"Created test data with {test_input_tokens.shape[0]} samples.\")\n",
        "\n",
        "# --- Helper function to calculate accuracy ---\n",
        "def calculate_accuracy(logits, labels):\n",
        "    \"\"\"Calculates accuracy given logits and labels.\"\"\"\n",
        "    # Logits shape: [batch, seq_len, d_vocab]\n",
        "    # We only care about the prediction at the last position (index 2)\n",
        "    prediction_logits = logits[:, -1, :] # Shape: [batch, d_vocab]\n",
        "    predicted_tokens = torch.argmax(prediction_logits, dim=-1) # Shape: [batch]\n",
        "    correct_predictions = (predicted_tokens == labels).sum().item()\n",
        "    total_predictions = labels.shape[0]\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return accuracy, correct_predictions, total_predictions\n",
        "\n",
        "# --- 2. Calculate Original Accuracy (No Hooks) ---\n",
        "model.eval() # Set model to evaluation mode\n",
        "with torch.no_grad(): # Use no_grad for inference efficiency\n",
        "    original_logits = model(test_input_tokens)\n",
        "    original_accuracy, orig_correct, orig_total = calculate_accuracy(original_logits, test_labels)\n",
        "\n",
        "print(f\"\\nOriginal Model Performance:\")\n",
        "print(f\"  Accuracy: {original_accuracy:.4f} ({orig_correct}/{orig_total})\")\n",
        "\n",
        "# --- 3. Calculate Ablated Accuracy (With Hook) ---\n",
        "with torch.no_grad():\n",
        "    ablated_logits = model.run_with_hooks(\n",
        "        test_input_tokens,\n",
        "        fwd_hooks=[(embedding_hook_point, ablate_pc2_hook)]\n",
        "    )\n",
        "    ablated_accuracy, ablated_correct, ablated_total = calculate_accuracy(ablated_logits, test_labels)\n",
        "\n",
        "print(f\"\\nPC2 Ablated Model Performance:\")\n",
        "print(f\"  Accuracy: {ablated_accuracy:.4f} ({ablated_correct}/{ablated_total})\")\n",
        "\n",
        "# --- 4. Compare ---\n",
        "accuracy_drop = original_accuracy - ablated_accuracy\n",
        "print(f\"\\nAccuracy Drop due to PC2 Ablation: {accuracy_drop:.4f}\")"
      ],
      "metadata": {
        "id": "pzZ208_Kq3_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy drops by a lot! We go from getting it right 100% of the time, to only 42%. So the second principal component is still very important, and whatever non-linearity it's doing actually matters. Let's now look specifically at which input pairs the model fails at when we ablate the PC2."
      ],
      "metadata": {
        "id": "P2IFjLdFsSE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Get predictions from the ablated run ---\n",
        "ablated_logits_last_pos = ablated_logits[:, -1, :] # Shape: [num_samples, d_vocab]\n",
        "ablated_predicted_tokens = torch.argmax(ablated_logits_last_pos, dim=-1) # Shape: [num_samples]\n",
        "\n",
        "# --- Identify incorrect predictions ---\n",
        "incorrect_mask = (ablated_predicted_tokens != test_labels)\n",
        "incorrect_indices = torch.where(incorrect_mask)[0].cpu().numpy()\n",
        "\n",
        "# --- Map indices back to (a, b) pairs ---\n",
        "failing_pairs = []\n",
        "original_a_values = test_input_tokens[:, 0].cpu().numpy()\n",
        "original_b_values = test_input_tokens[:, 1].cpu().numpy()\n",
        "true_sums = test_labels.cpu().numpy()\n",
        "predicted_sums_ablated = ablated_predicted_tokens.cpu().numpy()\n",
        "\n",
        "for idx in incorrect_indices:\n",
        "    a = original_a_values[idx]\n",
        "    b = original_b_values[idx]\n",
        "    true_sum = true_sums[idx]\n",
        "    predicted_sum = predicted_sums_ablated[idx]\n",
        "    failing_pairs.append({\n",
        "        'a': a,\n",
        "        'b': b,\n",
        "        'True Sum (a+b)': true_sum,\n",
        "        'Predicted Sum (Ablated)': predicted_sum\n",
        "    })\n",
        "\n",
        "# --- Display the failing pairs ---\n",
        "if not failing_pairs:\n",
        "    print(\"No failures found after PC2 ablation (unexpected based on previous accuracy).\")\n",
        "else:\n",
        "    print(f\"Found {len(failing_pairs)} failures after PC2 ablation:\")\n",
        "    fail_df = pd.DataFrame(failing_pairs)\n",
        "    # Sort for potentially easier pattern spotting\n",
        "    fail_df = fail_df.sort_values(by=['True Sum (a+b)', 'a', 'b']).reset_index(drop=True)\n",
        "    print(fail_df.to_string()) # Use to_string to print the full DataFrame\n",
        "\n",
        "# Optional: Visualize failures on a heatmap\n",
        "failure_heatmap = np.zeros((N, N))\n",
        "for failure in failing_pairs:\n",
        "    failure_heatmap[failure['a'], failure['b']] = 1 # Mark failures with 1\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(failure_heatmap, cmap=\"Reds\", linewidths=.5, linecolor='gray', cbar=False, annot=False)\n",
        "plt.title(\"Failure Cases (Red) after PC2 Ablation\")\n",
        "plt.xlabel(\"Input b = F(n-1)\")\n",
        "plt.ylabel(\"Input a = F(n-2)\")\n",
        "plt.xticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.yticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eT9WHfMAstJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is interesting... There's definitely patterns, but they are not easy to interpret. Some thoughts:\n",
        "\n",
        "1.   Symmetry: Confirmed. Reinforces that a and b are treated symmetrically.\n",
        "2.   Failures involving 0: This is a huge clue. The model fails for (0, 1) through (0, 8) and (1, 0) through (8, 0). However, it succeeds for (0, 0), (0, 9), and (9, 0).\n",
        "3.   Same observation for failures involving 0. This suggests some symmetry in how the model treats small and big numbers.\n",
        "3.   Central Failures: There's definitely a cluster of failures for pairs where both a and b are \"mid-range\" (roughly 2-7).\n",
        "\n",
        "Given that the PC2 ablation dramatically impacts accuracy with a complex failure patternparticularly affecting pairs involving edge tokens (0 and 9) differently and the central diagonalit strongly suggests the issue lies not just in the MLP's internal calculation, but crucially in how the final output is derived from the MLP state. This points directly to the Unembedding layer (W_U), which performs this final mapping to output logits. Therefore, the next logical step is to analyze W_U to understand how it interprets the MLP's combined PC1/PC2 representation and why removing the PC2 component leads to these specific, non-uniform failures in decoding the sum.\n"
      ],
      "metadata": {
        "id": "S8qv09Qqtm95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hook_point_mlp_out = \"blocks.0.hook_mlp_out\"       # Output of the MLP layer\n",
        "hook_point_resid_post = \"blocks.0.hook_resid_post\"  # Output of the whole block (after skip add)\n",
        "\n",
        "# --- Shared dictionary for activation passing ---\n",
        "# This dictionary will be accessible by both hooks during the run\n",
        "captured_activations_for_run = {}\n",
        "\n",
        "# --- Hook Function to CAPTURE mlp_out ---\n",
        "def capture_mlp_out_hook(activation, hook):\n",
        "    \"\"\"Captures the mlp_out activation into the shared dictionary.\"\"\"\n",
        "    captured_activations_for_run['mlp_out'] = activation.detach()\n",
        "    # Must return the original activation to not affect the forward pass here\n",
        "    return activation\n",
        "\n",
        "# --- Hook Function to MODIFY resid_post ---\n",
        "def replace_resid_post_hook(activation, hook):\n",
        "    \"\"\"\n",
        "    Replaces the resid_post activation with the captured mlp_out\n",
        "    from the shared dictionary.\n",
        "    \"\"\"\n",
        "    if 'mlp_out' not in captured_activations_for_run:\n",
        "         print(f\"Warning: Capture hook '{hook_point_mlp_out}' did not run before modify hook '{hook.name}'\")\n",
        "         return activation # Safety fallback\n",
        "\n",
        "    mlp_out_value = captured_activations_for_run['mlp_out']\n",
        "\n",
        "    # Ensure shapes are compatible\n",
        "    if activation.shape == mlp_out_value.shape:\n",
        "        # --- IMPORTANT: Return the captured mlp_out value ---\n",
        "        # This replaces the original resid_post value (attn_out + mlp_out)\n",
        "        # with just mlp_out for the rest of the forward pass.\n",
        "        return mlp_out_value\n",
        "    else:\n",
        "        print(f\"Shape mismatch warning: {hook.name} ({activation.shape}) vs captured mlp_out ({mlp_out_value.shape})\")\n",
        "        return activation # Fallback\n",
        "\n",
        "# --- Run the Ablation ---\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Clear the shared dictionary before the run\n",
        "    captured_activations_for_run.clear()\n",
        "\n",
        "    # Run with BOTH hooks active simultaneously\n",
        "    # The library executes hooks in the order they appear in the forward pass\n",
        "    # hook_mlp_out runs before hook_resid_post, so this works.\n",
        "    print(f\"Running model with capture hook at '{hook_point_mlp_out}' and modify hook at '{hook_point_resid_post}'\")\n",
        "    skip_ablated_logits = model.run_with_hooks(\n",
        "        test_input_tokens,\n",
        "        fwd_hooks=[\n",
        "            (hook_point_mlp_out, capture_mlp_out_hook),\n",
        "            (hook_point_resid_post, replace_resid_post_hook)\n",
        "        ]\n",
        "    )\n",
        "    print(\"Model run with hooks completed.\")\n",
        "\n",
        "\n",
        "# --- Calculate and Compare Accuracy ---\n",
        "# Recalculate original accuracy\n",
        "original_logits = model(test_input_tokens) # Run without hooks\n",
        "original_accuracy, orig_correct, orig_total = calculate_accuracy(original_logits, test_labels)\n",
        "\n",
        "# Calculate ablated accuracy\n",
        "skip_ablated_accuracy, skip_ablated_correct, skip_ablated_total = calculate_accuracy(skip_ablated_logits, test_labels)\n",
        "\n",
        "print(f\"\\nOriginal Model Accuracy: {original_accuracy:.4f} ({orig_correct}/{orig_total})\")\n",
        "print(f\"Skip Conn Ablated Accuracy (MLP Only): {skip_ablated_accuracy:.4f} ({skip_ablated_correct}/{skip_ablated_total})\")\n",
        "print(f\"Accuracy Drop due to Skip Ablation: {original_accuracy - skip_ablated_accuracy:.4f}\")\n",
        "\n",
        "# Clear the dictionary just in case\n",
        "captured_activations_for_run.clear()"
      ],
      "metadata": {
        "id": "xu7b4_Fzx4aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No accuracy is lost by dropping the skip connection (the bit where you re-add the MLP input to its output before following to the output layer of the transformer). This indicates that we can effectively ignore it. Nanda had observed the same. Just to be sure, we are now going to do the same analysis but with the test loss rather than accuracy."
      ],
      "metadata": {
        "id": "RKHwwWKD9q7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "captured_activations_for_run = {}\n",
        "\n",
        "# --- Run the Ablation and Calculate Logits (same as before) ---\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Clear the shared dictionary before the run\n",
        "    captured_activations_for_run.clear()\n",
        "\n",
        "    print(f\"Running model with capture hook at '{hook_point_mlp_out}' and modify hook at '{hook_point_resid_post}'\")\n",
        "    skip_ablated_logits = model.run_with_hooks(\n",
        "        test_input_tokens,\n",
        "        fwd_hooks=[\n",
        "            (hook_point_mlp_out, capture_mlp_out_hook),\n",
        "            (hook_point_resid_post, replace_resid_post_hook)\n",
        "        ]\n",
        "    )\n",
        "    print(\"Model run with hooks completed.\")\n",
        "\n",
        "    # Calculate original logits\n",
        "    original_logits = model(test_input_tokens)\n",
        "\n",
        "# --- Calculate Accuracy AND Loss ---\n",
        "\n",
        "# Original Performance\n",
        "original_accuracy, orig_correct, orig_total = calculate_accuracy(original_logits, test_labels)\n",
        "# Select logits for the prediction position (last token)\n",
        "original_pred_logits = original_logits[:, -1, :]\n",
        "original_loss = loss_fn(original_pred_logits, test_labels)\n",
        "\n",
        "# Ablated Performance\n",
        "skip_ablated_accuracy, skip_ablated_correct, skip_ablated_total = calculate_accuracy(skip_ablated_logits, test_labels)\n",
        "# Select logits for the prediction position (last token)\n",
        "skip_ablated_pred_logits = skip_ablated_logits[:, -1, :]\n",
        "skip_ablated_loss = loss_fn(skip_ablated_pred_logits, test_labels)\n",
        "\n",
        "\n",
        "# --- Print Results ---\n",
        "print(f\"\\nOriginal Model Performance:\")\n",
        "print(f\"  Accuracy: {original_accuracy:.4f} ({orig_correct}/{orig_total})\")\n",
        "print(f\"  Loss:     {original_loss.item():.6f}\") # Use .item() to get scalar value\n",
        "\n",
        "print(f\"\\nSkip Conn Ablated Accuracy (MLP Only):\")\n",
        "print(f\"  Accuracy: {skip_ablated_accuracy:.4f} ({skip_ablated_correct}/{skip_ablated_total})\")\n",
        "print(f\"  Loss:     {skip_ablated_loss.item():.6f}\")\n",
        "\n",
        "print(f\"\\nAccuracy Drop due to Skip Ablation: {original_accuracy - skip_ablated_accuracy:.4f}\")\n",
        "print(f\"Loss Change due to Skip Ablation:   {skip_ablated_loss.item() - original_loss.item():.6f}\")\n",
        "\n",
        "# Clear the dictionary\n",
        "captured_activations_for_run.clear()"
      ],
      "metadata": {
        "id": "AHhDW0QT-HRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss increases by a few orders of magnitude, but in absolute terms is still small. I'd say this verifies our findings.\n",
        "\n",
        "We are now going to look at the difference in logits when comparing the full trained model and the model in which we have ablated away the second PCA component (the parabolic one) in the embedded representation. We hope this sheds light on what it is that this 2nd component is doing."
      ],
      "metadata": {
        "id": "vLGuQFEq_Fu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Get Original and Ablated Logits ---\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Original logits\n",
        "    original_logits = model(test_input_tokens) # Shape: [num_samples, seq_len, d_vocab]\n",
        "\n",
        "    # Ablated logits (PC2 removed from embeddings)\n",
        "    ablated_logits = model.run_with_hooks(\n",
        "        test_input_tokens,\n",
        "        fwd_hooks=[(embedding_hook_point, ablate_pc2_hook)]\n",
        "    )\n",
        "\n",
        "# --- Calculate Logit Difference ---\n",
        "logit_diff = original_logits - ablated_logits # Shape: [num_samples, seq_len, d_vocab]\n",
        "\n",
        "# Focus on the logits at the prediction position (last token)\n",
        "logit_diff_pred_pos = logit_diff[:, -1, :].cpu().numpy() # Shape: [num_samples, d_vocab]\n",
        "original_logits_pred_pos = original_logits[:, -1, :].cpu().numpy()\n",
        "ablated_logits_pred_pos = ablated_logits[:, -1, :].cpu().numpy()\n",
        "\n",
        "test_labels_np = test_labels.cpu().numpy()\n",
        "\n",
        "# --- Analyze the Difference ---\n",
        "results = []\n",
        "original_a_values = test_input_tokens[:, 0].cpu().numpy()\n",
        "original_b_values = test_input_tokens[:, 1].cpu().numpy()\n",
        "ablated_predicted_tokens = np.argmax(ablated_logits_pred_pos, axis=-1)\n",
        "\n",
        "for i in range(len(test_labels_np)):\n",
        "    a = original_a_values[i]\n",
        "    b = original_b_values[i]\n",
        "    true_label = test_labels_np[i]\n",
        "    ablated_pred = ablated_predicted_tokens[i]\n",
        "\n",
        "    # Difference for the TRUE label's logit\n",
        "    diff_for_true_label = logit_diff_pred_pos[i, true_label]\n",
        "\n",
        "    # Difference for the ABLATED MODEL'S PREDICTED label's logit\n",
        "    diff_for_ablated_pred = logit_diff_pred_pos[i, ablated_pred]\n",
        "\n",
        "    original_correct_logit = original_logits_pred_pos[i, true_label]\n",
        "    ablated_correct_logit = ablated_logits_pred_pos[i, true_label]\n",
        "\n",
        "    results.append({\n",
        "        'a': a,\n",
        "        'b': b,\n",
        "        'True Sum': true_label,\n",
        "        'Ablated Pred': ablated_pred,\n",
        "        'LogitDiff (True)': diff_for_true_label,\n",
        "        'LogitDiff (AblatedPred)': diff_for_ablated_pred,\n",
        "        'OrigLogit (True)': original_correct_logit,\n",
        "        'AblatedLogit (True)': ablated_correct_logit,\n",
        "        'Failed': true_label != ablated_pred\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# --- Display Key Information ---\n",
        "print(\"Logit Difference Analysis (Original - Ablated):\")\n",
        "print(\"Positive 'LogitDiff (True)' means PC2 boosted the correct logit.\")\n",
        "print(\"Negative 'LogitDiff (AblatedPred)' for failures means PC2 suppressed the wrongly predicted logit.\")\n",
        "\n",
        "# Show stats for the change in the correct logit\n",
        "print(\"\\nStatistics for Logit Difference of the CORRECT sum:\")\n",
        "print(results_df['LogitDiff (True)'].describe())\n",
        "\n",
        "# Show failure cases focusing on logit changes\n",
        "failures_df = results_df[results_df['Failed']].sort_values(by=['True Sum', 'a', 'b'])\n",
        "print(f\"\\nAnalysis of {len(failures_df)} Failure Cases:\")\n",
        "if not failures_df.empty:\n",
        "    print(failures_df[['a', 'b', 'True Sum', 'Ablated Pred', 'LogitDiff (True)', 'LogitDiff (AblatedPred)', 'OrigLogit (True)', 'AblatedLogit (True)']].to_string())\n",
        "\n",
        "# --- Visualize LogitDiff for the Correct Label ---\n",
        "logit_diff_heatmap = np.full((N, N), np.nan)\n",
        "for i in range(len(results_df)):\n",
        "    row = results_df.iloc[i]\n",
        "    logit_diff_heatmap[row['a'], row['b']] = row['LogitDiff (True)']\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "max_abs_val = np.nanmax(np.abs(logit_diff_heatmap))\n",
        "sns.heatmap(logit_diff_heatmap, cmap=\"coolwarm\", center=0,\n",
        "            annot=True, fmt=\".2f\", linewidths=.5, linecolor='gray',\n",
        "            vmin=-max_abs_val, vmax=max_abs_val, # Center colorbar at 0\n",
        "            mask=np.isnan(logit_diff_heatmap), square=True)\n",
        "plt.title(\"Logit Difference (Orig - Ablated) for CORRECT Sum\")\n",
        "plt.xlabel(\"Input b = F(n-1)\")\n",
        "plt.ylabel(\"Input a = F(n-2)\")\n",
        "plt.xticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.yticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OseV7BsYFW4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PC2 isn't just \"fine-tuning\". It's part of a complex encoding strategy interpreted by the Unembedding layer (W_U).\n",
        "\n",
        "It acts as a strong identifier for \"double-extreme\" inputs ((0,0), (9,9)), massively boosting their correct logits.\n",
        "\n",
        "It acts differently for \"single-extreme\" inputs ((0,k), (k,9)), where its net effect might even reduce the correct logit slightly, but likely plays a vital role in suppressing competitors.\n",
        "\n",
        "It provides a moderate general boost for central inputs.\n",
        "\n",
        "Let us now visualize by how much the ablated model is wrong, rather than just the fact that it is wrong."
      ],
      "metadata": {
        "id": "OifaSNWnHKDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_error_heatmap = np.full((N, N), np.nan)\n",
        "\n",
        "num_samples = len(test_labels_np)\n",
        "prediction_errors = np.zeros(num_samples) # Store errors for potential stats\n",
        "\n",
        "for i in range(num_samples):\n",
        "    a_val = original_a_values[i]\n",
        "    b_val = original_b_values[i]\n",
        "    ablated_pred = ablated_predicted_tokens[i]\n",
        "    true_label = test_labels_np[i]\n",
        "\n",
        "    # Calculate the arithmetic difference\n",
        "    error = float(ablated_pred - true_label) # Ensure float for calculations\n",
        "    prediction_errors[i] = error\n",
        "\n",
        "    # Place the error in the heatmap\n",
        "    prediction_error_heatmap[a_val, b_val] = error\n",
        "\n",
        "# --- Visualize Prediction Error Heatmap ---\n",
        "plt.figure(figsize=(7, 6))\n",
        "\n",
        "# Find the max absolute error for centering the color bar correctly\n",
        "max_abs_error = np.nanmax(np.abs(prediction_error_heatmap))\n",
        "# Handle case where there are no errors or all NaNs\n",
        "if np.isnan(max_abs_error):\n",
        "    max_abs_error = 1.0 # Default if no errors\n",
        "\n",
        "# Use a diverging colormap since error can be positive or negative\n",
        "cmap = \"coolwarm\" # Red for positive error (prediction > true), Blue for negative\n",
        "\n",
        "sns.heatmap(prediction_error_heatmap, cmap=cmap, center=0,\n",
        "            annot=True, fmt=\".0f\", # Show integer errors\n",
        "            linewidths=.5, linecolor='lightgray',\n",
        "            square=True,\n",
        "            vmin=-max_abs_error, vmax=max_abs_error, # Center color scale around 0\n",
        "            cbar_kws={'label': 'Ablated Prediction - True Sum'},\n",
        "            mask=np.isnan(prediction_error_heatmap)) # Mask cells not in test data\n",
        "\n",
        "plt.title(\"Prediction Error of Ablated Model (PC2 Removed)\")\n",
        "plt.xlabel(\"Input b = F(n-1)\")\n",
        "plt.ylabel(\"Input a = F(n-2)\")\n",
        "plt.xticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.yticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# --- Optional: Display Error Statistics ---\n",
        "print(\"\\nStatistics for Prediction Error (Ablated Pred - True Sum):\")\n",
        "# Create a temporary series excluding NaNs if your test set wasn't exhaustive\n",
        "valid_errors = prediction_errors[~np.isnan(prediction_errors)]\n",
        "if len(valid_errors) > 0:\n",
        "    error_series = pd.Series(valid_errors)\n",
        "    print(error_series.describe())\n",
        "    print(f\"\\nNumber of incorrect predictions (error != 0): {np.sum(valid_errors != 0)}\")\n",
        "    print(f\"Number of correct predictions (error == 0): {np.sum(valid_errors == 0)}\")\n",
        "else:\n",
        "    print(\"No valid error data found (check test set population).\")"
      ],
      "metadata": {
        "id": "QJ7T2laLgXo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some observations: It's never wrong by more than 3. And is most often wrong by 1. It's off by 3 only at 0,0 and 9,9 -- it predicts 3 for 0,0 and 15 for 9,9. It's off by positive values when at least one of the inputs is 0, off by negative values when at least one of the inputs is 9. Exception made to 0,9 and 9,0, where it is correct -- suggesting that two wrongs make a right. Further, for low-ish values of a,b it underpredicts (off by negative), for high-ish values of a,b it overpredicts (off by positive).\n",
        "\n",
        "Can we conclude anything about whatever algorithm it's implementing based on this? Let's fit the predictions of the ablated model to a linear function of the sum."
      ],
      "metadata": {
        "id": "tRvUNLDsg3sK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fitting ablated predictions to a linear function of the true sum...\")\n",
        "\n",
        "# --- Prepare Data for Linear Regression ---\n",
        "# X: Independent variable (True Sum)\n",
        "# Needs to be a 2D array for scikit-learn, even with one feature\n",
        "X_true_sum = test_labels_np.reshape(-1, 1)\n",
        "\n",
        "# y: Dependent variable (Ablated Prediction)\n",
        "y_ablated_pred = ablated_predicted_tokens\n",
        "\n",
        "# --- Fit Linear Regression Model ---\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_true_sum, y_ablated_pred)\n",
        "\n",
        "# --- Get Results ---\n",
        "# Slope (m)\n",
        "learned_m = lin_reg.coef_[0]\n",
        "# Intercept (c)\n",
        "learned_c = lin_reg.intercept_\n",
        "# Predictions from the fitted linear model\n",
        "y_linear_fit_pred = lin_reg.predict(X_true_sum)\n",
        "\n",
        "# --- Evaluate the Fit ---\n",
        "# R-squared score\n",
        "r2 = r2_score(y_ablated_pred, y_linear_fit_pred)\n",
        "# Alternatively: r2 = lin_reg.score(X_true_sum, y_ablated_pred)\n",
        "# Mean Squared Error\n",
        "mse = mean_squared_error(y_ablated_pred, y_linear_fit_pred)\n",
        "\n",
        "# --- Print Results and Comparison ---\n",
        "print(\"\\n--- Linear Fit Results: AblatedPred = m * TrueSum + c ---\")\n",
        "print(f\"Learned Slope (m): {learned_m:.4f}\")\n",
        "print(f\"Learned Intercept (c): {learned_c:.4f}\")\n",
        "print(f\"R-squared score: {r2:.4f}\")\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "# Compare to hypothesized values\n",
        "hypothesized_m = 2/3\n",
        "hypothesized_c = 3\n",
        "print(\"\\nComparison to Hypothesis (m = 2/3, c = 3):\")\n",
        "print(f\"Hypothesized m: {hypothesized_m:.4f}\")\n",
        "print(f\"Hypothesized c: {hypothesized_c:.4f}\")\n",
        "print(f\"Difference in m: {learned_m - hypothesized_m:.4f}\")\n",
        "print(f\"Difference in c: {learned_c - hypothesized_c:.4f}\")\n",
        "\n",
        "# --- Visualize the Fit ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(test_labels_np, y_ablated_pred, alpha=0.5, label='Actual Ablated Predictions')\n",
        "plt.plot(test_labels_np, y_linear_fit_pred, color='red', linewidth=2, label=f'Linear Fit (R={r2:.3f})')\n",
        "# Plot the hypothesized line\n",
        "hypothesized_line = hypothesized_m * test_labels_np + hypothesized_c\n",
        "plt.plot(test_labels_np, hypothesized_line, color='green', linestyle='--', linewidth=2, label='Hypothesized Line (m=2/3, c=3)')\n",
        "# Plot the ideal line (y=x) for reference\n",
        "plt.plot(test_labels_np, test_labels_np, color='gray', linestyle=':', linewidth=1, label='Ideal (y=x)')\n",
        "\n",
        "\n",
        "plt.title('Linear Fit: Ablated Prediction vs. True Sum')\n",
        "plt.xlabel('True Sum (a+b)')\n",
        "plt.ylabel('Ablated Model Prediction')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "# Ensure axes cover the full possible range (0 to max possible sum, e.g., 18)\n",
        "max_sum = np.max(test_labels_np) if len(test_labels_np) > 0 else 18\n",
        "max_pred = np.max(y_ablated_pred) if len(y_ablated_pred) > 0 else max_sum\n",
        "plot_max = max(max_sum, max_pred)\n",
        "plt.xlim(-0.5, plot_max + 0.5)\n",
        "plt.ylim(-0.5, plot_max + 0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yxWJPZTsi5FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: the ablated model makes different predictions for some combinations of inputs which are different but have the same sum, which is why there are multiple blue dots on the same vertical line in the plot above. This necessarily implies that we cannot perfectly fit the ablated model's predictions to a function of the sum, we need to consider a and b separately."
      ],
      "metadata": {
        "id": "ju8Rz1zVjd07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFitting ablated predictions to a linear function of a and b separately...\")\n",
        "\n",
        "# --- Prepare Data for Multiple Linear Regression ---\n",
        "# X: Independent variables (a, b)\n",
        "# Create a 2D array where columns are 'a' and 'b'\n",
        "X_inputs_ab = np.stack((original_a_values, original_b_values), axis=-1) # Shape: [num_samples, 2]\n",
        "\n",
        "# y: Dependent variable (Ablated Prediction)\n",
        "y_ablated_pred = ablated_predicted_tokens\n",
        "\n",
        "# --- Fit Linear Regression Model ---\n",
        "multi_lin_reg = LinearRegression()\n",
        "multi_lin_reg.fit(X_inputs_ab, y_ablated_pred)\n",
        "\n",
        "# --- Get Results ---\n",
        "# Coefficients (w_a, w_b)\n",
        "learned_wa, learned_wb = multi_lin_reg.coef_\n",
        "# Intercept (c)\n",
        "learned_c_multi = multi_lin_reg.intercept_\n",
        "# Predictions from the fitted linear model\n",
        "y_multilinear_fit_pred = multi_lin_reg.predict(X_inputs_ab)\n",
        "\n",
        "# --- Evaluate the Fit ---\n",
        "r2_multi = r2_score(y_ablated_pred, y_multilinear_fit_pred)\n",
        "mse_multi = mean_squared_error(y_ablated_pred, y_multilinear_fit_pred)\n",
        "\n",
        "# --- Print Results ---\n",
        "print(\"\\n--- Multiple Linear Fit Results: AblatedPred = wa*a + wb*b + c ---\")\n",
        "print(f\"Learned Weight for a (wa): {learned_wa:.4f}\")\n",
        "print(f\"Learned Weight for b (wb): {learned_wb:.4f}\")\n",
        "print(f\"Learned Intercept (c): {learned_c_multi:.4f}\")\n",
        "print(f\"R-squared score: {r2_multi:.4f}\")\n",
        "print(f\"Mean Squared Error: {mse_multi:.4f}\")\n",
        "\n",
        "# --- Compare with previous single-variable fit ---\n",
        "# Assuming 'r2', 'learned_m', 'learned_c' from the previous fit exist\n",
        "if 'r2' in locals():\n",
        "    print(f\"\\nComparison to previous fit (AblatedPred = m*(a+b) + c):\")\n",
        "    print(f\"  Previous R-squared: {r2:.4f}\")\n",
        "    print(f\"  Improvement in R-squared: {r2_multi - r2:.4f}\")\n",
        "    print(f\"  Previous m: {learned_m:.4f} (Avg of wa, wb = {(learned_wa + learned_wb)/2:.4f})\")\n",
        "    print(f\"  Previous c: {learned_c:.4f} (Current c = {learned_c_multi:.4f})\")\n",
        "\n",
        "# --- Optional: Visualize Residuals (If R-squared isn't close to 1) ---\n",
        "residuals = y_ablated_pred - y_multilinear_fit_pred\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_multilinear_fit_pred, residuals, alpha=0.5)\n",
        "plt.hlines(0, xmin=min(y_multilinear_fit_pred), xmax=max(y_multilinear_fit_pred), colors='red', linestyles='--')\n",
        "plt.title('Residual Plot for Multi-Linear Fit')\n",
        "plt.xlabel('Fitted Values (wa*a + wb*b + c)')\n",
        "plt.ylabel('Residuals (Actual - Fitted)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# --- Optional: Heatmap of Residuals ---\n",
        "residual_heatmap = np.full((N, N), np.nan)\n",
        "fitted_values_heatmap = np.full((N, N), np.nan)\n",
        "num_samples = len(y_ablated_pred)\n",
        "\n",
        "for i in range(num_samples):\n",
        "    a_val = original_a_values[i]\n",
        "    b_val = original_b_values[i]\n",
        "    residual_heatmap[a_val, b_val] = residuals[i]\n",
        "    fitted_values_heatmap[a_val, b_val] = y_multilinear_fit_pred[i] # Store fitted value\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "max_abs_resid = np.nanmax(np.abs(residual_heatmap))\n",
        "if np.isnan(max_abs_resid): max_abs_resid = 1.0\n",
        "\n",
        "cmap_resid = \"coolwarm\"\n",
        "sns.heatmap(residual_heatmap, cmap=cmap_resid, center=0,\n",
        "            annot=True, fmt=\".2f\",\n",
        "            linewidths=.5, linecolor='lightgray',\n",
        "            square=True,\n",
        "            vmin=-max_abs_resid, vmax=max_abs_resid,\n",
        "            cbar_kws={'label': 'Residual (Actual Ablated - Fitted)'},\n",
        "            mask=np.isnan(residual_heatmap))\n",
        "plt.title(\"Residuals of Multi-Linear Fit (wa*a + wb*b + c)\")\n",
        "plt.xlabel(\"Input b\")\n",
        "plt.ylabel(\"Input a\")\n",
        "plt.xticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.yticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pPwQvcX7kRyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fit to a and b individually is the same as the fit to a+b. Hence, the dependence of the ablated model's prediction depends linearly on the inputs only via their sum. Given that the fit is not perfect (R^2 of ~0.93, not 1), this indicates that there is some non-linear component contributing to the model's output. We think this is introduced by the ReLU, as we see no other sources of non-linearity.\n",
        "\n",
        "Hypothesis: note that the desired behavior is linear, i.e., just sum the inputs. The complete model implements this, albeit with a PC2 component which is non-linear. The ablated model that does not have the PC2 component fails to implement the linear behavior, and errs. Hence, it might be that the PC2 component's task is to cancel out the wrong non-linearity that the rest of the model appears to learn. We can test this by fitting the error in the ablated model's prediction to the value of the PC2 component for the same inputs."
      ],
      "metadata": {
        "id": "rPalwBgclldh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures # For potential non-linear fit\n",
        "\n",
        "# --- 1. Extract PC2 Features using EXISTING pca_scores ---\n",
        "print(\"Extracting PC2 features from existing pca_scores...\")\n",
        "try:\n",
        "    # Ensure pca_scores is available and is a tensor\n",
        "    if 'pca_scores' not in locals() or not isinstance(pca_scores, torch.Tensor):\n",
        "         raise NameError(\"Variable 'pca_scores' not found or is not a PyTorch tensor.\")\n",
        "\n",
        "    # Convert scores to numpy ONCE for use with sklearn and indexing\n",
        "    pca_scores_np = pca_scores.detach().cpu().numpy()\n",
        "    # PC2 values for all tokens in the vocabulary (index 1 is the second PC)\n",
        "    if pca_scores_np.shape[1] < 2:\n",
        "         raise ValueError(f\"pca_scores_np has shape {pca_scores_np.shape}, but need at least 2 components to extract PC2.\")\n",
        "    pc2_values_all_tokens = pca_scores_np[:, 1]\n",
        "\n",
        "    # Get PC2 value for input 'a' and 'b' for each sample in the test set\n",
        "    pc2_a_features = pc2_values_all_tokens[original_a_values]\n",
        "    pc2_b_features = pc2_values_all_tokens[original_b_values]\n",
        "\n",
        "    # Create the feature matrix for regression\n",
        "    X_pc2_features = np.stack((pc2_a_features, pc2_b_features), axis=-1) # Shape: [num_samples, 2]\n",
        "    pc2_extracted_successfully = True\n",
        "    print(\"PC2 features extracted successfully.\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Please ensure 'pca_scores' (Tensor) is computed and available.\")\n",
        "    pc2_extracted_successfully = False\n",
        "except IndexError as e:\n",
        "     print(f\"Error indexing PC2 values. Check token indices range (0-{N-1}) vs vocab size used in PCA: {e}\")\n",
        "     pc2_extracted_successfully = False\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during PC2 feature extraction: {e}\")\n",
        "    pc2_extracted_successfully = False\n",
        "print(pc2_b_features)\n",
        "# --- Proceed only if PC2 features were extracted successfully ---\n",
        "if pc2_extracted_successfully:\n",
        "    print(\"\\nFitting Ablated Model Error to PC2 Features...\")\n",
        "\n",
        "    # --- 2. Calculate the Error ---\n",
        "    y_error = test_labels_np - ablated_predicted_tokens # Correction needed\n",
        "\n",
        "    # --- 3. Perform Linear Regression ---\n",
        "    error_reg = LinearRegression()\n",
        "    error_reg.fit(X_pc2_features, y_error)\n",
        "\n",
        "    # --- Get Results ---\n",
        "    learned_w_pc2a, learned_w_pc2b = error_reg.coef_\n",
        "    learned_c_error = error_reg.intercept_\n",
        "    y_error_pred = error_reg.predict(X_pc2_features)\n",
        "\n",
        "    # --- Evaluate Fit ---\n",
        "    r2_error_fit = r2_score(y_error, y_error_pred)\n",
        "    mse_error_fit = mean_squared_error(y_error, y_error_pred)\n",
        "\n",
        "    print(\"\\n--- Linear Fit Results: Error  w_pc2a*PC2(a) + w_pc2b*PC2(b) + c ---\")\n",
        "    print(f\"Learned Weight for PC2(a): {learned_w_pc2a:.4f}\")\n",
        "    print(f\"Learned Weight for PC2(b): {learned_w_pc2b:.4f}\")\n",
        "    print(f\"Learned Intercept: {learned_c_error:.4f}\")\n",
        "    print(f\"R-squared score: {r2_error_fit:.4f}\")\n",
        "    print(f\"Mean Squared Error: {mse_error_fit:.4f}\")\n",
        "\n",
        "    # --- Optional: Try Polynomial Regression (Degree 2) ---\n",
        "    print(\"\\n--- Trying Polynomial Fit (Degree 2) ---\")\n",
        "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "    # Create interaction & quadratic features: [pc2a, pc2b, pc2a^2, pc2a*pc2b, pc2b^2]\n",
        "    X_pc2_poly_features = poly.fit_transform(X_pc2_features)\n",
        "\n",
        "    error_poly_reg = LinearRegression()\n",
        "    error_poly_reg.fit(X_pc2_poly_features, y_error)\n",
        "    y_error_poly_pred = error_poly_reg.predict(X_pc2_poly_features)\n",
        "    r2_error_poly_fit = r2_score(y_error, y_error_poly_pred)\n",
        "    mse_error_poly_fit = mean_squared_error(y_error, y_error_poly_pred)\n",
        "\n",
        "    print(f\"Polynomial Fit R-squared score: {r2_error_poly_fit:.4f}\")\n",
        "    print(f\"Polynomial Fit Mean Squared Error: {mse_error_poly_fit:.4f}\")\n",
        "    print(f\"Improvement in R-squared: {r2_error_poly_fit - r2_error_fit:.4f}\")\n",
        "\n",
        "\n",
        "    # --- Visualize Error vs Predicted Error ---\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_error, y_error_pred, alpha=0.5, label=f'Linear Fit (R={r2_error_fit:.3f})')\n",
        "    # Only plot polynomial fit if it's meaningfully better\n",
        "    if r2_error_poly_fit > r2_error_fit + 0.01:\n",
        "         plt.scatter(y_error, y_error_poly_pred, alpha=0.5, marker='x', label=f'Poly Fit (R={r2_error_poly_fit:.3f})')\n",
        "    # Ideal line y=x\n",
        "    min_val = min(np.min(y_error), np.min(y_error_pred)) - 0.5 # Add buffer\n",
        "    max_val = max(np.max(y_error), np.max(y_error_pred)) + 0.5 # Add buffer\n",
        "    #plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Ideal Fit (y=x)')\n",
        "    plt.title('Predicting Ablated Model Error using PC2 Features')\n",
        "    plt.xlabel('Actual Error (True Sum - Ablated Pred)')\n",
        "    plt.ylabel('Predicted Error (from PC2 features)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping error prediction analysis due to issues extracting PC2 features.\")"
      ],
      "metadata": {
        "id": "mN_I8gaImzql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "d_vocab = N # Or N+1 if your output includes a special token mapped within 0..N\n",
        "d_mlp = model.cfg.d_mlp # Should be 512\n",
        "num_key_neurons = 100 # Analyze the top 100 correlated neurons\n",
        "key_neuron_indices = indices_sorted_by_abs_corr[:num_key_neurons]\n",
        "\n",
        "# --- Step 1: Calculate W_L = W_U.T @ W_out.T ---\n",
        "\n",
        "# Get the weight matrices\n",
        "try:\n",
        "    # Assume stored as [d_model, d_vocab]\n",
        "    W_U_stored = model.unembed.W_U.detach().cpu()\n",
        "    W_U = W_U_stored.T # Transpose to get [d_vocab, d_model]\n",
        "except AttributeError:\n",
        "    # Assume stored as [d_model, d_vocab]\n",
        "    W_U_stored = model.W_U.detach().cpu()\n",
        "    W_U = W_U_stored.T # Transpose to get [d_vocab, d_model]\n",
        "\n",
        "# Assume stored as [d_mlp, d_model]\n",
        "W_out_stored = model.blocks[0].mlp.W_out.detach().cpu()\n",
        "W_out = W_out_stored.T # Transpose to get [d_model, d_mlp]\n",
        "\n",
        "# Calculate the effective neuron->logit map\n",
        "print(f\"Multiplying W_U shape {W_U.shape} with W_out shape {W_out.shape}\")\n",
        "# This should now be [d_vocab, d_model] @ [d_model, d_mlp]\n",
        "W_L = W_U @ W_out # Shape: [d_vocab, d_mlp]\n",
        "W_L_np = W_L.numpy()\n",
        "\n",
        "print(f\"Calculated W_L with shape: {W_L_np.shape}\") # Should be (d_vocab, d_mlp)\n",
        "\n",
        "# --- Step 2: Analyze Rows of W_L (Code remains the same) ---\n",
        "# ... (rest of the analysis code) ...\n",
        "\n",
        "print(f\"Calculated W_L with shape: {W_L_np.shape}\") # Should be (d_vocab, d_mlp), e.g., (10, 512)\n",
        "\n",
        "# --- Step 2: Analyze Rows of W_L and Neuron Contributions ---\n",
        "\n",
        "print(f\"\\nAnalyzing weights in W_L for top {num_key_neurons} sum-correlated neurons:\")\n",
        "\n",
        "analysis_results = []\n",
        "\n",
        "for k in range(d_vocab): # Iterate through each output logit (0 to N-1)\n",
        "    # Get the weights from W_L for this specific logit 'k'\n",
        "    # applied to the key neurons\n",
        "    weights_for_key_neurons = W_L_np[k, key_neuron_indices] # Shape: [num_key_neurons]\n",
        "\n",
        "    # Hypothesis Check: Contribution to correct logit\n",
        "    positivity_ratio = np.mean(weights_for_key_neurons > 0)\n",
        "\n",
        "    # Redundancy Check: Distribution statistics\n",
        "    mean_weight = np.mean(weights_for_key_neurons)\n",
        "    std_weight = np.std(weights_for_key_neurons)\n",
        "    max_abs_weight = np.max(np.abs(weights_for_key_neurons))\n",
        "    # Check sparsity: % of weights close to zero (e.g., < 1% of max abs weight)\n",
        "    sparsity_threshold = 0.01 * max_abs_weight\n",
        "    sparsity_ratio = np.mean(np.abs(weights_for_key_neurons) < sparsity_threshold)\n",
        "\n",
        "\n",
        "    print(f\"\\n--- Logit for output token k={k} ---\")\n",
        "    print(f\"  Positivity Ratio (Key Neurons): {positivity_ratio:.2f}\")\n",
        "    print(f\"  Sparsity Ratio (<{sparsity_threshold:.4f}): {sparsity_ratio:.2f}\")\n",
        "    print(f\"  Weight Stats (Key Neurons): Mean={mean_weight:.4f}, Std={std_weight:.4f}, MaxAbs={max_abs_weight:.4f}\")\n",
        "\n",
        "    analysis_results.append({\n",
        "        'k': k,\n",
        "        'positivity_ratio': positivity_ratio,\n",
        "        'sparsity_ratio': sparsity_ratio,\n",
        "        'mean_weight': mean_weight,\n",
        "        'std_weight': std_weight,\n",
        "        'max_abs_weight': max_abs_weight\n",
        "    })\n",
        "\n",
        "# Optional: Create a DataFrame for easier viewing\n",
        "analysis_df = pd.DataFrame(analysis_results)\n",
        "print(\"\\n--- Summary Table ---\")\n",
        "print(analysis_df.round(3))\n",
        "\n",
        "# Optional: Plot histogram for a specific k (e.g., k=5)\n",
        "plt.figure(figsize=(8, 4))\n",
        "k_to_plot = 5\n",
        "plt.hist(W_L_np[k_to_plot, key_neuron_indices], bins=30, alpha=0.7)\n",
        "plt.title(f\"Weight Distribution in W_L[{k_to_plot}, :] for Top {num_key_neurons} Neurons\")\n",
        "plt.xlabel(\"Weight Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(axis='y', alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "upD5rJX9KE3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That didn't work! Let's try and fit the output of the ablated model to a quadratic function of the inputs."
      ],
      "metadata": {
        "id": "OQr_D2PUu6JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFitting ablated predictions to quadratic functions of a and b...\")\n",
        "\n",
        "# --- Prepare Data ---\n",
        "# y: Dependent variable (Ablated Prediction)\n",
        "y_ablated_pred = ablated_predicted_tokens\n",
        "\n",
        "# --- Fit 1: Requested Quadratic Model: x*(a+b) + y*a^2 + z*b^2 + c ---\n",
        "print(\"\\n--- Fitting Requested Model: x*(a+b) + y*a^2 + z*b^2 + c ---\")\n",
        "\n",
        "# Construct Features: [a+b, a^2, b^2]\n",
        "sum_ab = original_a_values + original_b_values\n",
        "a_squared = original_a_values**2\n",
        "b_squared = original_b_values**2\n",
        "X_quadratic_requested = np.stack((sum_ab, a_squared, b_squared), axis=-1) # Shape: [num_samples, 3]\n",
        "\n",
        "# Fit Linear Regression Model\n",
        "quad_reg_req = LinearRegression()\n",
        "quad_reg_req.fit(X_quadratic_requested, y_ablated_pred)\n",
        "\n",
        "# Get Results\n",
        "learned_x, learned_y, learned_z = quad_reg_req.coef_\n",
        "learned_c_req = quad_reg_req.intercept_\n",
        "y_quad_req_pred = quad_reg_req.predict(X_quadratic_requested)\n",
        "\n",
        "# Evaluate Fit\n",
        "r2_quad_req = r2_score(y_ablated_pred, y_quad_req_pred)\n",
        "mse_quad_req = mean_squared_error(y_ablated_pred, y_quad_req_pred)\n",
        "\n",
        "print(f\"Learned Coefficient for (a+b) (x): {learned_x:.4f}\")\n",
        "print(f\"Learned Coefficient for a^2 (y):   {learned_y:.4f}\")\n",
        "print(f\"Learned Coefficient for b^2 (z):   {learned_z:.4f}\")\n",
        "print(f\"Learned Intercept (c):             {learned_c_req:.4f}\")\n",
        "print(f\"R-squared score:                   {r2_quad_req:.4f}\")\n",
        "print(f\"Mean Squared Error:                {mse_quad_req:.4f}\")\n",
        "\n",
        "\n",
        "# --- Fit 2: General Quadratic Model using PolynomialFeatures ---\n",
        "print(\"\\n--- Fitting General Quadratic Model (includes a*b term) ---\")\n",
        "\n",
        "# Create polynomial features (degree 2) from [a, b]\n",
        "# Includes: a, b, a^2, a*b, b^2 (bias term is handled by LinearRegression)\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_inputs_ab = np.stack((original_a_values, original_b_values), axis=-1) # Shape: [num_samples, 2]\n",
        "X_quadratic_general = poly.fit_transform(X_inputs_ab)\n",
        "# Get feature names for clarity (order might vary slightly by sklearn version)\n",
        "feature_names = poly.get_feature_names_out(['a', 'b'])\n",
        "\n",
        "\n",
        "# Fit Linear Regression Model\n",
        "quad_reg_gen = LinearRegression()\n",
        "quad_reg_gen.fit(X_quadratic_general, y_ablated_pred)\n",
        "\n",
        "# Get Results\n",
        "learned_coeffs_gen = quad_reg_gen.coef_\n",
        "learned_c_gen = quad_reg_gen.intercept_\n",
        "y_quad_gen_pred = quad_reg_gen.predict(X_quadratic_general)\n",
        "\n",
        "# Evaluate Fit\n",
        "r2_quad_gen = r2_score(y_ablated_pred, y_quad_gen_pred)\n",
        "mse_quad_gen = mean_squared_error(y_ablated_pred, y_quad_gen_pred)\n",
        "\n",
        "print(\"General Quadratic Model: Pred  w_a*a + w_b*b + w_aa*a^2 + w_ab*a*b + w_bb*b^2 + c_gen\")\n",
        "print(\"Learned Coefficients:\")\n",
        "for name, coeff in zip(feature_names, learned_coeffs_gen):\n",
        "    print(f\"  Weight for {name}: {coeff:.4f}\")\n",
        "print(f\"Learned Intercept (c_gen): {learned_c_gen:.4f}\")\n",
        "print(f\"R-squared score:           {r2_quad_gen:.4f}\")\n",
        "print(f\"Mean Squared Error:        {mse_quad_gen:.4f}\")\n",
        "\n",
        "\n",
        "# --- Compare Fits ---\n",
        "print(\"\\n--- R-squared Comparison ---\")\n",
        "if 'r2_multi' in locals(): # Check if previous linear fit results exist\n",
        "    print(f\"Linear Fit (wa*a + wb*b + c):       R = {r2_multi:.4f}\")\n",
        "else:\n",
        "    print(\"Multi-linear fit results not available for comparison.\")\n",
        "print(f\"Requested Quadratic Fit:          R = {r2_quad_req:.4f}\")\n",
        "print(f\"General Quadratic Fit (w/ a*b):   R = {r2_quad_gen:.4f}\")\n",
        "\n",
        "\n",
        "# --- Visualize Residuals for the Best Quadratic Fit ---\n",
        "best_r2_quad = max(r2_quad_req, r2_quad_gen)\n",
        "print(f\"\\nVisualizing residuals for the best quadratic fit (R={best_r2_quad:.4f})...\")\n",
        "if r2_quad_gen >= r2_quad_req:\n",
        "    residuals = y_ablated_pred - y_quad_gen_pred\n",
        "    fitted_values = y_quad_gen_pred\n",
        "    title_suffix = \"General Quadratic Fit\"\n",
        "else:\n",
        "    residuals = y_ablated_pred - y_quad_req_pred\n",
        "    fitted_values = y_quad_req_pred\n",
        "    title_suffix = \"Requested Quadratic Fit\"\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(fitted_values, residuals, alpha=0.5)\n",
        "plt.hlines(0, xmin=min(fitted_values), xmax=max(fitted_values), colors='red', linestyles='--')\n",
        "plt.title(f'Residual Plot for {title_suffix}')\n",
        "plt.xlabel('Fitted Values')\n",
        "plt.ylabel('Residuals (Actual Ablated - Fitted)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Optional: Heatmap of Residuals\n",
        "residual_heatmap = np.full((N, N), np.nan)\n",
        "num_samples = len(y_ablated_pred)\n",
        "for i in range(num_samples):\n",
        "    a_val = original_a_values[i]\n",
        "    b_val = original_b_values[i]\n",
        "    residual_heatmap[a_val, b_val] = residuals[i] # Use the correct residuals\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "max_abs_resid = np.nanmax(np.abs(residual_heatmap))\n",
        "if np.isnan(max_abs_resid) or max_abs_resid == 0: max_abs_resid = 1.0\n",
        "\n",
        "cmap_resid = \"coolwarm\"\n",
        "sns.heatmap(residual_heatmap, cmap=cmap_resid, center=0,\n",
        "            annot=True, fmt=\".2f\",\n",
        "            linewidths=.5, linecolor='lightgray',\n",
        "            square=True,\n",
        "            vmin=-max_abs_resid, vmax=max_abs_resid,\n",
        "            cbar_kws={'label': 'Residual (Actual Ablated - Fitted)'},\n",
        "            mask=np.isnan(residual_heatmap))\n",
        "plt.title(f\"Residuals of {title_suffix}\")\n",
        "plt.xlabel(\"Input b\")\n",
        "plt.ylabel(\"Input a\")\n",
        "plt.xticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.yticks(np.arange(N) + 0.5, np.arange(N))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NoUb4GyYvGYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately our hypothesis about quadratic PC2 term simply being a correction on top of the linear output was incorrect. Hence, its contribution must be more subtle. To try to understand it, we will now compare the activations of the neurons in the ablated and the full model, to see how removing the PC2 affects their firing."
      ],
      "metadata": {
        "id": "o9fJToyeyCYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Storage for Activations ---\n",
        "activation_store = {}\n",
        "\n",
        "def store_activation_hook(activation, hook, storage_key):\n",
        "    # Store activation at the final sequence position\n",
        "    # Clone and detach to prevent memory leaks and graph issues\n",
        "    activation_store[storage_key] = activation[:, -1, :].detach().clone()\n",
        "    # Return original activation to not interfere\n",
        "    return activation\n",
        "\n",
        "# --- Execute and Capture Activations ---\n",
        "a_mlp_full = None\n",
        "a_mlp_ablated = None\n",
        "mlp_activation_hook_point = utils.get_act_name(\"mlp_post\", 0)\n",
        "print(mlp_activation_hook_point)\n",
        "if mlp_activation_hook_point:\n",
        "    model.eval() # Ensure model is in evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # --- Run 1: Get Full Model Activations ---\n",
        "        print(\"Running model normally to capture full MLP activations...\")\n",
        "        activation_store.clear() # Clear previous storage\n",
        "        _ = model.run_with_hooks(\n",
        "            test_input_tokens,\n",
        "            fwd_hooks=[(mlp_activation_hook_point,\n",
        "                        lambda act, hook: store_activation_hook(act, hook, 'full'))]\n",
        "        )\n",
        "        if 'full' in activation_store:\n",
        "            a_mlp_full = activation_store['full'].cpu().numpy()\n",
        "            print(f\"Captured full activations, shape: {a_mlp_full.shape}\")\n",
        "        else:\n",
        "            print(\"Failed to capture full activations.\")\n",
        "\n",
        "        # --- Run 2: Get Ablated Model Activations ---\n",
        "        print(\"Running model with PC2 ablation to capture ablated MLP activations...\")\n",
        "        activation_store.clear() # Clear previous storage\n",
        "        _ = model.run_with_hooks(\n",
        "            test_input_tokens,\n",
        "            fwd_hooks=[(embedding_hook_point, ablate_pc2_hook), # Ablate PC2 first\n",
        "                       (mlp_activation_hook_point,\n",
        "                        lambda act, hook: store_activation_hook(act, hook, 'ablated'))]\n",
        "        )\n",
        "        if 'ablated' in activation_store:\n",
        "            a_mlp_ablated = activation_store['ablated'].cpu().numpy()\n",
        "            print(f\"Captured ablated activations, shape: {a_mlp_ablated.shape}\")\n",
        "        else:\n",
        "            print(\"Failed to capture ablated activations.\")\n",
        "\n",
        "\n",
        "# --- Calculate Difference and Analyze ---\n",
        "if a_mlp_full is not None and a_mlp_ablated is not None:\n",
        "    if a_mlp_full.shape == a_mlp_ablated.shape:\n",
        "        print(\"\\nCalculating activation differences...\")\n",
        "        delta_a_mlp = a_mlp_full - a_mlp_ablated # Shape: [num_samples, d_mlp]\n",
        "\n",
        "        # --- Analysis Focused on Failures ---\n",
        "        failure_mask = results_df['Failed'].values\n",
        "        num_failures = failure_mask.sum()\n",
        "\n",
        "        if num_failures > 0:\n",
        "            print(f\"Analyzing {num_failures} failure cases...\")\n",
        "            delta_a_mlp_failures = delta_a_mlp[failure_mask] # Shape: [num_failures, d_mlp]\n",
        "\n",
        "            # --- 1. Which neurons change the most on average during failure? ---\n",
        "            mean_abs_delta_failures = np.mean(np.abs(delta_a_mlp_failures), axis=0)\n",
        "            sorted_neuron_indices_by_delta = np.argsort(mean_abs_delta_failures)[::-1]\n",
        "\n",
        "            print(\"\\nTop 10 Neurons by Mean Absolute Activation Change During Failures:\")\n",
        "            for i in range(min(10, d_mlp)):\n",
        "                idx = sorted_neuron_indices_by_delta[i]\n",
        "                print(f\"  Neuron {idx}: Mean Abs Delta = {mean_abs_delta_failures[idx]:.4f} (Corr={neuron_sum_correlations[idx]:.3f})\")\n",
        "\n",
        "            # --- 2. Visualize Average Delta for ALL Failures ---\n",
        "            mean_delta_failures = np.mean(delta_a_mlp_failures, axis=0) # Shape: [d_mlp,]\n",
        "\n",
        "            # Sort by correlation group for visualization\n",
        "            corr_sorted_indices = np.argsort(neuron_sum_correlations)\n",
        "            mean_delta_failures_sorted = mean_delta_failures[corr_sorted_indices]\n",
        "            correlations_sorted = neuron_sum_correlations[corr_sorted_indices]\n",
        "\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            colors = ['red' if c < 0 else 'blue' for c in correlations_sorted]\n",
        "            plt.bar(range(d_mlp), mean_delta_failures_sorted, color=colors, width=1.0)\n",
        "            plt.xlabel(\"Neurons (Sorted by Correlation with Sum a+b)\")\n",
        "            plt.ylabel(\"Mean Activation Diff (Full - Ablated)\")\n",
        "            plt.title(f\"Mean MLP Activation Difference on Failure Cases ({num_failures} samples)\")\n",
        "            plt.xticks([]) # Too many neurons to label\n",
        "            plt.grid(axis='y', alpha=0.5)\n",
        "            # Add legend for color (optional, can be tricky for bar charts)\n",
        "            from matplotlib.lines import Line2D\n",
        "            legend_elements = [Line2D([0], [0], color='blue', lw=4, label='Positively Correlated Neurons'),\n",
        "                               Line2D([0], [0], color='red', lw=4, label='Negatively Correlated Neurons')]\n",
        "            plt.legend(handles=legend_elements)\n",
        "            plt.show()\n",
        "\n",
        "            # --- 3. Visualize Average Delta for Specific Failure Groups (Example: a=0 failures) ---\n",
        "            a0_failure_mask = failure_mask & (results_df['a'] == 0)\n",
        "            num_a0_failures = a0_failure_mask.sum()\n",
        "\n",
        "            if num_a0_failures > 0:\n",
        "                print(f\"\\nAnalyzing {num_a0_failures} failure cases where a=0...\")\n",
        "                delta_a_mlp_a0_failures = delta_a_mlp[a0_failure_mask]\n",
        "                mean_delta_a0_failures = np.mean(delta_a_mlp_a0_failures, axis=0)\n",
        "                mean_delta_a0_failures_sorted = mean_delta_a0_failures[corr_sorted_indices]\n",
        "\n",
        "                plt.figure(figsize=(15, 6))\n",
        "                plt.bar(range(d_mlp), mean_delta_a0_failures_sorted, color=colors, width=1.0)\n",
        "                plt.xlabel(\"Neurons (Sorted by Correlation with Sum a+b)\")\n",
        "                plt.ylabel(\"Mean Activation Diff (Full - Ablated)\")\n",
        "                plt.title(f\"Mean MLP Activation Difference on Failure Cases with a=0 ({num_a0_failures} samples)\")\n",
        "                plt.xticks([])\n",
        "                plt.grid(axis='y', alpha=0.5)\n",
        "                plt.legend(handles=legend_elements)\n",
        "                plt.show()\n",
        "            else:\n",
        "                print(\"No failure cases found where a=0.\")\n",
        "\n",
        "            # --- 4. Visualize Average Delta for Specific Failure Groups (Example: a=9 failures) ---\n",
        "            a9_failure_mask = failure_mask & (results_df['a'] == 9)\n",
        "            num_a9_failures = a9_failure_mask.sum()\n",
        "\n",
        "            if num_a9_failures > 0:\n",
        "                print(f\"\\nAnalyzing {num_a9_failures} failure cases where a=0...\")\n",
        "                delta_a_mlp_a0_failures = delta_a_mlp[a9_failure_mask]\n",
        "                mean_delta_a0_failures = np.mean(delta_a_mlp_a0_failures, axis=0)\n",
        "                mean_delta_a0_failures_sorted = mean_delta_a0_failures[corr_sorted_indices]\n",
        "\n",
        "                plt.figure(figsize=(15, 6))\n",
        "                plt.bar(range(d_mlp), mean_delta_a0_failures_sorted, color=colors, width=1.0)\n",
        "                plt.xlabel(\"Neurons (Sorted by Correlation with Sum a+b)\")\n",
        "                plt.ylabel(\"Mean Activation Diff (Full - Ablated)\")\n",
        "                plt.title(f\"Mean MLP Activation Difference on Failure Cases with a=9 ({num_a9_failures} samples)\")\n",
        "                plt.xticks([])\n",
        "                plt.grid(axis='y', alpha=0.5)\n",
        "                plt.legend(handles=legend_elements)\n",
        "                plt.show()\n",
        "            else:\n",
        "                print(\"No failure cases found where a=0.\")\n",
        "\n",
        "        else:\n",
        "            print(\"No failure cases detected to analyze activation differences.\")\n",
        "    else:\n",
        "        print(\"Error: Shape mismatch between full and ablated activations.\")\n",
        "        print(f\"Full shape: {a_mlp_full.shape}, Ablated shape: {a_mlp_ablated.shape}\")\n",
        "else:\n",
        "    print(\"Skipping activation difference analysis because activations were not captured.\")\n"
      ],
      "metadata": {
        "id": "HEtGz6boyRmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's some patterns here, in that the changes to the activations seem to be grouped by correlation values, again pointing towards the 'groups of neurons' hypothesis. Let's perform a similar analysis for the pre-ReLU values."
      ],
      "metadata": {
        "id": "OjiapcyQ1UaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "mlp_pre_activation_hook_point = \"blocks.0.mlp.hook_pre\" # Input BEFORE ReLU\n",
        "print(f\"Using MLP pre-activation hook point: {mlp_pre_activation_hook_point}\")\n",
        "\n",
        "# --- Storage ---\n",
        "pre_activation_store = {}\n",
        "\n",
        "# --- Hook Function ---\n",
        "def store_pre_activation_hook(activation, hook, storage_key):\n",
        "    # Store pre-activation at the final sequence position (usually where prediction happens)\n",
        "    pre_activation_store[storage_key] = activation[:, -1, :].detach().clone()\n",
        "    return activation\n",
        "\n",
        "# --- Execute and Capture Activations ---\n",
        "z_mlp_full = None\n",
        "z_mlp_ablated = None\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "\n",
        "    # --- Run 1: Get Full Model Pre-Activations ---\n",
        "    print(f\"\\nRunning model normally, hooking: {mlp_pre_activation_hook_point}\")\n",
        "    pre_activation_store.clear()\n",
        "    fwd_hooks_full = [(\n",
        "        mlp_pre_activation_hook_point,\n",
        "        partial(store_pre_activation_hook, storage_key='full') # Use partial\n",
        "    )]\n",
        "    _ = model.run_with_hooks(test_input_tokens, fwd_hooks=fwd_hooks_full)\n",
        "    if 'full' in pre_activation_store:\n",
        "        z_mlp_full = pre_activation_store['full'].cpu().numpy()\n",
        "        print(f\"Captured full pre-activations, shape: {z_mlp_full.shape}\")\n",
        "    else:\n",
        "        print(f\"FAILED to capture full pre-activations using hook: {mlp_pre_activation_hook_point}\")\n",
        "\n",
        "    # --- Run 2: Get Ablated Model Pre-Activations ---\n",
        "    if z_mlp_full is not None: # Proceed only if first run succeeded\n",
        "        print(f\"\\nRunning model with PC2 ablation, hooking: {mlp_pre_activation_hook_point}\")\n",
        "        pre_activation_store.clear()\n",
        "        fwd_hooks_ablated = [\n",
        "            (embedding_hook_point, ablate_pc2_hook), # Ablate PC2\n",
        "            (mlp_pre_activation_hook_point, partial(store_pre_activation_hook, storage_key='ablated')) # Store pre-activation\n",
        "        ]\n",
        "        _ = model.run_with_hooks(test_input_tokens, fwd_hooks=fwd_hooks_ablated)\n",
        "        if 'ablated' in pre_activation_store:\n",
        "            z_mlp_ablated = pre_activation_store['ablated'].cpu().numpy()\n",
        "            print(f\"Captured ablated pre-activations, shape: {z_mlp_ablated.shape}\")\n",
        "        else:\n",
        "            print(f\"FAILED to capture ablated pre-activations using hook: {mlp_pre_activation_hook_point}\")\n",
        "\n",
        "\n",
        "# --- Calculate Difference and Analyze ---\n",
        "if z_mlp_full is not None and z_mlp_ablated is not None and z_mlp_full.shape == z_mlp_ablated.shape:\n",
        "    print(\"\\nCalculating pre-activation differences (delta_z_mlp)...\")\n",
        "    delta_z_mlp = z_mlp_full - z_mlp_ablated\n",
        "\n",
        "    failure_mask = results_df['Failed'].values\n",
        "    num_failures = failure_mask.sum()\n",
        "\n",
        "    if num_failures > 0:\n",
        "        print(f\"Analyzing {num_failures} failure cases...\")\n",
        "        delta_z_mlp_failures = delta_z_mlp[failure_mask]\n",
        "\n",
        "        # --- 1. Top Changing Neurons ---\n",
        "        mean_abs_delta_z_failures = np.mean(np.abs(delta_z_mlp_failures), axis=0)\n",
        "        sorted_neuron_indices_by_delta_z = np.argsort(mean_abs_delta_z_failures)[::-1]\n",
        "        print(\"\\nTop 10 Neurons by Mean Absolute Pre-Activation Change During Failures:\")\n",
        "        for i in range(min(10, d_mlp)):\n",
        "            idx = sorted_neuron_indices_by_delta_z[i]\n",
        "            print(f\"  Neuron {idx}: Mean Abs Delta_Z = {mean_abs_delta_z_failures[idx]:.4f} (Corr={neuron_sum_correlations[idx]:.3f})\")\n",
        "\n",
        "        # --- 2. Average Delta_Z for ALL Failures ---\n",
        "        mean_delta_z_failures = np.mean(delta_z_mlp_failures, axis=0)\n",
        "        corr_sorted_indices = np.argsort(neuron_sum_correlations)\n",
        "        mean_delta_z_failures_sorted = mean_delta_z_failures[corr_sorted_indices]\n",
        "        correlations_sorted = neuron_sum_correlations[corr_sorted_indices]\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        colors = ['red' if c < 0 else 'blue' for c in correlations_sorted]\n",
        "        plt.bar(range(d_mlp), mean_delta_z_failures_sorted, color=colors, width=1.0)\n",
        "        plt.xlabel(\"Neurons (Sorted by Correlation with Sum a+b)\")\n",
        "        plt.ylabel(\"Mean Pre-Activation Diff (Full - Ablated)\")\n",
        "        plt.title(f\"Mean MLP Pre-Activation Difference (delta_z_mlp) on Failure Cases ({num_failures} samples)\")\n",
        "        plt.xticks([])\n",
        "        plt.grid(axis='y', alpha=0.5)\n",
        "        from matplotlib.lines import Line2D\n",
        "        legend_elements = [Line2D([0], [0], color='blue', lw=4, label='Positively Correlated Neurons'),\n",
        "                           Line2D([0], [0], color='red', lw=4, label='Negatively Correlated Neurons')]\n",
        "        plt.legend(handles=legend_elements)\n",
        "        plt.show()\n",
        "\n",
        "        # --- 3. Average Delta_Z for Specific Failure Groups (a=0) ---\n",
        "        a0_failure_mask = failure_mask & (results_df['a'] == 0)\n",
        "        num_a0_failures = a0_failure_mask.sum()\n",
        "        if num_a0_failures > 0:\n",
        "            print(f\"\\nAnalyzing {num_a0_failures} failure cases where a=0...\")\n",
        "            delta_z_mlp_a0_failures = delta_z_mlp[a0_failure_mask]\n",
        "            mean_delta_z_a0_failures = np.mean(delta_z_mlp_a0_failures, axis=0)\n",
        "            mean_delta_z_a0_failures_sorted = mean_delta_z_a0_failures[corr_sorted_indices]\n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.bar(range(d_mlp), mean_delta_z_a0_failures_sorted, color=colors, width=1.0)\n",
        "            plt.xlabel(\"Neurons (Sorted by Correlation with Sum a+b)\")\n",
        "            plt.ylabel(\"Mean Pre-Activation Diff (Full - Ablated)\")\n",
        "            plt.title(f\"Mean MLP Pre-Activation Difference (delta_z_mlp) on Failure Cases with a=0 ({num_a0_failures} samples)\")\n",
        "            plt.xticks([])\n",
        "            plt.grid(axis='y', alpha=0.5)\n",
        "            plt.legend(handles=legend_elements)\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"No failure cases found where a=0.\")\n",
        "\n",
        "    else:\n",
        "        print(\"No failure cases detected.\")\n",
        "else:\n",
        "    print(\"\\nAnalysis skipped: Could not capture both full and ablated pre-activations.\")\n"
      ],
      "metadata": {
        "id": "NMxPLTpJ1vTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the ablated model, are the pre-ReLU values still linear w.r.t. the sum, as is the case in the full model? We've established that this is not the case for the full model output, but not at the pre-ReLU stage. If it is, then the non-linearity must be coming from the ReLU. Otherwise, it's happening earlier on. Let's do this fit and plot it below"
      ],
      "metadata": {
        "id": "yWuTR7Dxl-Jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFitting PRE-ReLU ablated activations (z_mlp_ablated) vs. True Sum...\")\n",
        "\n",
        "# --- Check if data is available ---\n",
        "if 'z_mlp_ablated' not in locals() or z_mlp_ablated is None:\n",
        "    print(\"Error: 'z_mlp_ablated' not found. Please run the previous step first.\")\n",
        "else:\n",
        "    # --- Prepare Data for Linear Regression ---\n",
        "    # X: Independent variable (True Sum)\n",
        "    X_true_sum = test_labels_np.reshape(-1, 1)\n",
        "    num_samples = X_true_sum.shape[0]\n",
        "\n",
        "    # Array to store R-squared for each neuron's fit\n",
        "    neuron_r2_scores = np.zeros(d_mlp)\n",
        "    neuron_slopes = np.zeros(d_mlp)\n",
        "    neuron_intercepts = np.zeros(d_mlp)\n",
        "\n",
        "    # --- Fit Linear Model for Each Neuron ---\n",
        "    print(f\"Fitting linear model for {d_mlp} neurons...\")\n",
        "    for i in range(d_mlp):\n",
        "        # y: Dependent variable (Pre-ReLU activation of neuron i)\n",
        "        y_neuron_z_ablated = z_mlp_ablated[:, i]\n",
        "\n",
        "        # Check for constant activation (no variance) - skip if so\n",
        "        if np.std(y_neuron_z_ablated) < 1e-9:\n",
        "            neuron_r2_scores[i] = np.nan # Indicate no fit possible\n",
        "            neuron_slopes[i] = np.nan\n",
        "            neuron_intercepts[i] = np.nan\n",
        "            continue\n",
        "\n",
        "        # Fit model: z_i  m_i * (a+b) + c_i\n",
        "        lin_reg_neuron = LinearRegression()\n",
        "        lin_reg_neuron.fit(X_true_sum, y_neuron_z_ablated)\n",
        "\n",
        "        # Store results\n",
        "        neuron_r2_scores[i] = lin_reg_neuron.score(X_true_sum, y_neuron_z_ablated)\n",
        "        neuron_slopes[i] = lin_reg_neuron.coef_[0]\n",
        "        neuron_intercepts[i] = lin_reg_neuron.intercept_\n",
        "\n",
        "    print(\"Fitting complete.\")\n",
        "\n",
        "    # --- Analyze R-squared Distribution ---\n",
        "    valid_r2_scores = neuron_r2_scores[~np.isnan(neuron_r2_scores)]\n",
        "\n",
        "    if len(valid_r2_scores) > 0:\n",
        "        print(\"\\n--- Analysis of R-squared (z_mlp_ablated vs True Sum) ---\")\n",
        "        print(f\"Mean R:   {np.mean(valid_r2_scores):.4f}\")\n",
        "        print(f\"Median R: {np.median(valid_r2_scores):.4f}\")\n",
        "        print(f\"Min R:    {np.min(valid_r2_scores):.4f}\")\n",
        "        print(f\"Max R:    {np.max(valid_r2_scores):.4f}\")\n",
        "        print(f\"Neurons with R > 0.90: {np.sum(valid_r2_scores > 0.90)} / {len(valid_r2_scores)}\")\n",
        "        print(f\"Neurons with R > 0.95: {np.sum(valid_r2_scores > 0.95)} / {len(valid_r2_scores)}\")\n",
        "        print(f\"Neurons with R > 0.99: {np.sum(valid_r2_scores > 0.99)} / {len(valid_r2_scores)}\")\n",
        "\n",
        "        # --- Visualize R-squared Distribution ---\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(valid_r2_scores, bins=50, kde=False)\n",
        "        plt.title(\"Distribution of R: Linear Fit of Pre-ReLU Ablated Activations vs. True Sum\")\n",
        "        plt.xlabel(\"R-squared Score per Neuron\")\n",
        "        plt.ylabel(\"Number of Neurons\")\n",
        "        plt.grid(axis='y', alpha=0.5)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "_YYfdmJ-mPip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's significant amounts of neurons that are very strongly correlated with the sum! Although I am not sure how this significant number compares to the same number in the full model. Nonetheless, let's do the same analysis for the post-ReLU values. Is this ReLU killing this linear relation?"
      ],
      "metadata": {
        "id": "s2vlzuRkmjzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFitting POST-ReLU ablated activations (a_mlp_ablated) vs. True Sum...\")\n",
        "\n",
        "# --- Check if data is available ---\n",
        "if 'a_mlp_ablated' not in locals() or a_mlp_ablated is None:\n",
        "    print(\"Error: 'a_mlp_ablated' not found. Please ensure post-ReLU ablated activations were captured.\")\n",
        "elif 'neuron_r2_scores' not in locals() or neuron_r2_scores is None:\n",
        "    print(\"Error: 'neuron_r2_scores' (from pre-ReLU fit) not found. Please run the previous step first.\")\n",
        "else:\n",
        "    # --- Prepare Data ---\n",
        "    X_true_sum = test_labels_np.reshape(-1, 1)\n",
        "    num_samples = X_true_sum.shape[0]\n",
        "\n",
        "    # Array to store R-squared for the POST-ReLU fit\n",
        "    neuron_r2_scores_post = np.zeros(d_mlp)\n",
        "    neuron_slopes_post = np.zeros(d_mlp)\n",
        "    neuron_intercepts_post = np.zeros(d_mlp)\n",
        "\n",
        "    # --- Fit Linear Model for Each Neuron (POST-ReLU) ---\n",
        "    print(f\"Fitting linear model for {d_mlp} neurons (post-ReLU)...\")\n",
        "    for i in range(d_mlp):\n",
        "        # y: Dependent variable (POST-ReLU activation of neuron i)\n",
        "        y_neuron_a_ablated = a_mlp_ablated[:, i]\n",
        "\n",
        "        # Check for constant activation (e.g., always zero post-ReLU)\n",
        "        if np.std(y_neuron_a_ablated) < 1e-9:\n",
        "            neuron_r2_scores_post[i] = np.nan # Indicate no fit possible\n",
        "            neuron_slopes_post[i] = np.nan\n",
        "            neuron_intercepts_post[i] = np.nan\n",
        "            continue\n",
        "\n",
        "        # Fit model: a_i  m'_i * (a+b) + c'_i\n",
        "        lin_reg_neuron_post = LinearRegression()\n",
        "        lin_reg_neuron_post.fit(X_true_sum, y_neuron_a_ablated)\n",
        "\n",
        "        # Store results\n",
        "        neuron_r2_scores_post[i] = lin_reg_neuron_post.score(X_true_sum, y_neuron_a_ablated)\n",
        "        neuron_slopes_post[i] = lin_reg_neuron_post.coef_[0]\n",
        "        neuron_intercepts_post[i] = lin_reg_neuron_post.intercept_\n",
        "\n",
        "    print(\"Fitting complete.\")\n",
        "\n",
        "    # --- Analyze POST-ReLU R-squared Distribution ---\n",
        "    valid_r2_scores_post = neuron_r2_scores_post[~np.isnan(neuron_r2_scores_post)]\n",
        "\n",
        "    if len(valid_r2_scores_post) > 0:\n",
        "        print(\"\\n--- Analysis of R-squared (POST-ReLU a_mlp_ablated vs True Sum) ---\")\n",
        "        print(f\"Mean R:   {np.mean(valid_r2_scores_post):.4f}\")\n",
        "        print(f\"Median R: {np.median(valid_r2_scores_post):.4f}\")\n",
        "        print(f\"Min R:    {np.min(valid_r2_scores_post):.4f}\")\n",
        "        print(f\"Max R:    {np.max(valid_r2_scores_post):.4f}\")\n",
        "        print(f\"Neurons with R > 0.90: {np.sum(valid_r2_scores_post > 0.90)} / {len(valid_r2_scores_post)}\")\n",
        "        print(f\"Neurons with R > 0.95: {np.sum(valid_r2_scores_post > 0.95)} / {len(valid_r2_scores_post)}\")\n",
        "        print(f\"Neurons with R > 0.99: {np.sum(valid_r2_scores_post > 0.99)} / {len(valid_r2_scores_post)}\") # Expect fewer\n",
        "\n",
        "        # --- Visualize POST-ReLU R-squared Distribution ---\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(valid_r2_scores_post, bins=50, kde=False)\n",
        "        plt.title(\"Distribution of R: Linear Fit of POST-ReLU Ablated Activations vs. True Sum\")\n",
        "        plt.xlabel(\"R-squared Score per Neuron (Post-ReLU)\")\n",
        "        plt.ylabel(\"Number of Neurons\")\n",
        "        plt.grid(axis='y', alpha=0.5)\n",
        "        plt.show()\n",
        "\n",
        "        # --- Compare Pre-ReLU vs Post-ReLU R-squared ---\n",
        "        # Use the original neuron_r2_scores from the pre-ReLU fit\n",
        "        r2_diff = neuron_r2_scores - neuron_r2_scores_post # Pre - Post\n",
        "        valid_r2_diff = r2_diff[~np.isnan(r2_diff)] # Filter NaNs from either pre or post\n",
        "\n",
        "        print(\"\\n--- Analysis of R-squared Change (Pre-ReLU R - Post-ReLU R) ---\")\n",
        "        print(f\"Mean Difference:   {np.mean(valid_r2_diff):.4f} (Positive means ReLU reduced linearity)\")\n",
        "        print(f\"Median Difference: {np.median(valid_r2_diff):.4f}\")\n",
        "        print(f\"Min Difference:    {np.min(valid_r2_diff):.4f}\") # Could be negative if post fit is somehow better\n",
        "        print(f\"Max Difference:    {np.max(valid_r2_diff):.4f}\")\n",
        "\n",
        "        # Visualize the distribution of the DIFFERENCE\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(valid_r2_diff, bins=50, kde=False)\n",
        "        plt.title(\"Distribution of R Change (Pre-ReLU Fit R minus Post-ReLU Fit R)\")\n",
        "        plt.xlabel(\"Difference in R-squared (Positive = ReLU decreased linearity)\")\n",
        "        plt.ylabel(\"Number of Neurons\")\n",
        "        plt.grid(axis='y', alpha=0.5)\n",
        "        plt.axvline(0, color='red', linestyle='--', linewidth=1) # Mark zero difference\n",
        "        plt.show()\n",
        "\n",
        "        # Focus on neurons that were highly linear PRE-ReLU\n",
        "        highly_linear_pre_mask = neuron_r2_scores > 0.99\n",
        "        if np.any(highly_linear_pre_mask):\n",
        "            r2_diff_highly_linear = r2_diff[highly_linear_pre_mask & ~np.isnan(r2_diff)] # Also filter NaNs\n",
        "            if len(r2_diff_highly_linear) > 0:\n",
        "                 print(f\"\\n--- R Change for Neurons with Pre-ReLU R > 0.99 ({len(r2_diff_highly_linear)} neurons) ---\")\n",
        "                 print(f\"Mean Difference:   {np.mean(r2_diff_highly_linear):.4f}\")\n",
        "                 print(f\"Median Difference: {np.median(r2_diff_highly_linear):.4f}\")\n",
        "            else:\n",
        "                 print(\"\\nNo valid difference data for neurons with Pre-ReLU R > 0.99 (check for NaNs in post-fit).\")\n",
        "        else:\n",
        "            print(\"\\nNo neurons found with Pre-ReLU R > 0.99 based on previous results.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Could not calculate R-squared for any neurons post-ReLU.\")\n"
      ],
      "metadata": {
        "id": "g5n_GBhTm4pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, so the post-ReLU activations are much less strongly correlated with the sum. In fact, no neurons are now >0.9 fit with linear function of the sum. This could be just because the ReLU zeroes out the part that was negative, so you only get good correlation in the positive part, which results in overall not-so-good fit. I wonder if the same thing happens in the full model, though? Does the goodness of fit to linear function of sum change significantly pre and post ReLU?"
      ],
      "metadata": {
        "id": "s01-tD3Pnl_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFitting FULL model PRE- and POST-ReLU activations vs. True Sum...\")\n",
        "\n",
        "# --- Check if data is available ---\n",
        "if 'z_mlp_full' not in locals() or z_mlp_full is None:\n",
        "    print(\"Error: 'z_mlp_full' (full model pre-ReLU activations) not found.\")\n",
        "elif 'a_mlp_full' not in locals() or a_mlp_full is None:\n",
        "    print(\"Error: 'a_mlp_full' (full model post-ReLU activations) not found.\")\n",
        "else:\n",
        "    # --- Prepare Data ---\n",
        "    X_true_sum = test_labels_np.reshape(-1, 1)\n",
        "    num_samples = X_true_sum.shape[0]\n",
        "\n",
        "    # Arrays to store R-squared for the FULL model fits\n",
        "    neuron_r2_scores_pre_full = np.zeros(d_mlp)\n",
        "    neuron_r2_scores_post_full = np.zeros(d_mlp)\n",
        "    # Optional: store slopes/intercepts if needed later\n",
        "    # neuron_slopes_pre_full = np.zeros(d_mlp)\n",
        "    # neuron_intercepts_pre_full = np.zeros(d_mlp)\n",
        "    # neuron_slopes_post_full = np.zeros(d_mlp)\n",
        "    # neuron_intercepts_post_full = np.zeros(d_mlp)\n",
        "\n",
        "    # --- Fit Linear Model for Each Neuron (Pre & Post ReLU) ---\n",
        "    print(f\"Fitting linear models for {d_mlp} neurons (full model)...\")\n",
        "    for i in range(d_mlp):\n",
        "        # Pre-ReLU Fit\n",
        "        y_neuron_z_full = z_mlp_full[:, i]\n",
        "        if np.std(y_neuron_z_full) > 1e-9:\n",
        "            lin_reg_neuron_pre = LinearRegression()\n",
        "            lin_reg_neuron_pre.fit(X_true_sum, y_neuron_z_full)\n",
        "            neuron_r2_scores_pre_full[i] = lin_reg_neuron_pre.score(X_true_sum, y_neuron_z_full)\n",
        "        else:\n",
        "            neuron_r2_scores_pre_full[i] = np.nan\n",
        "\n",
        "        # Post-ReLU Fit\n",
        "        y_neuron_a_full = a_mlp_full[:, i]\n",
        "        if np.std(y_neuron_a_full) > 1e-9:\n",
        "            lin_reg_neuron_post = LinearRegression()\n",
        "            lin_reg_neuron_post.fit(X_true_sum, y_neuron_a_full)\n",
        "            neuron_r2_scores_post_full[i] = lin_reg_neuron_post.score(X_true_sum, y_neuron_a_full)\n",
        "        else:\n",
        "            neuron_r2_scores_post_full[i] = np.nan\n",
        "\n",
        "    print(\"Fitting complete.\")\n",
        "\n",
        "    # --- Analyze R-squared Distributions (FULL Model) ---\n",
        "    valid_r2_pre_full = neuron_r2_scores_pre_full[~np.isnan(neuron_r2_scores_pre_full)]\n",
        "    valid_r2_post_full = neuron_r2_scores_post_full[~np.isnan(neuron_r2_scores_post_full)]\n",
        "\n",
        "    if len(valid_r2_pre_full) > 0 and len(valid_r2_post_full) > 0:\n",
        "        print(\"\\n--- Analysis of R (FULL Model PRE-ReLU vs True Sum) ---\")\n",
        "        print(f\"Mean R:   {np.mean(valid_r2_pre_full):.4f}\")\n",
        "        print(f\"Median R: {np.median(valid_r2_pre_full):.4f}\")\n",
        "        print(f\"Neurons with R > 0.99: {np.sum(valid_r2_pre_full > 0.99)} / {len(valid_r2_pre_full)}\")\n",
        "\n",
        "        print(\"\\n--- Analysis of R (FULL Model POST-ReLU vs True Sum) ---\")\n",
        "        print(f\"Mean R:   {np.mean(valid_r2_post_full):.4f}\")\n",
        "        print(f\"Median R: {np.median(valid_r2_post_full):.4f}\")\n",
        "        print(f\"Neurons with R > 0.99: {np.sum(valid_r2_post_full > 0.99)} / {len(valid_r2_post_full)}\") # Compare this count\n",
        "\n",
        "        # --- Visualize Distributions ---\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
        "        sns.histplot(valid_r2_pre_full, bins=50, kde=False, ax=axes[0])\n",
        "        axes[0].set_title(\"FULL Model: Pre-ReLU Activation Linearity (R vs Sum)\")\n",
        "        axes[0].set_xlabel(\"R-squared Score per Neuron (Pre-ReLU)\")\n",
        "        axes[0].set_ylabel(\"Number of Neurons\")\n",
        "        axes[0].grid(axis='y', alpha=0.5)\n",
        "\n",
        "        sns.histplot(valid_r2_post_full, bins=50, kde=False, ax=axes[1])\n",
        "        axes[1].set_title(\"FULL Model: Post-ReLU Activation Linearity (R vs Sum)\")\n",
        "        axes[1].set_xlabel(\"R-squared Score per Neuron (Post-ReLU)\")\n",
        "        axes[1].grid(axis='y', alpha=0.5)\n",
        "        plt.suptitle(\"Comparison of Activation Linearity Before and After ReLU (FULL Model)\")\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "        # --- Compare Pre-ReLU vs Post-ReLU R-squared (FULL Model) ---\n",
        "        r2_diff_full = neuron_r2_scores_pre_full - neuron_r2_scores_post_full\n",
        "        valid_r2_diff_full = r2_diff_full[~np.isnan(r2_diff_full)]\n",
        "\n",
        "        print(\"\\n--- Analysis of R Change (Pre-ReLU R - Post-ReLU R) for FULL Model ---\")\n",
        "        print(f\"Mean Difference:   {np.mean(valid_r2_diff_full):.4f} (Positive means ReLU reduced linearity)\")\n",
        "        print(f\"Median Difference: {np.median(valid_r2_diff_full):.4f}\")\n",
        "\n",
        "        # Visualize the difference distribution\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(valid_r2_diff_full, bins=50, kde=False)\n",
        "        plt.title(\"FULL Model: Distribution of R Change (Pre-ReLU Fit R minus Post-ReLU Fit R)\")\n",
        "        plt.xlabel(\"Difference in R-squared (Positive = ReLU decreased linearity)\")\n",
        "        plt.ylabel(\"Number of Neurons\")\n",
        "        plt.grid(axis='y', alpha=0.5)\n",
        "        plt.axvline(0, color='red', linestyle='--', linewidth=1)\n",
        "        plt.show()\n",
        "\n",
        "        # --- Direct Comparison to Ablated Model Results (Optional) ---\n",
        "        if 'neuron_r2_scores' in locals() and 'neuron_r2_scores_post' in locals():\n",
        "             print(\"\\n--- Comparison to Ablated Model Linearity Drop ---\")\n",
        "             r2_diff_ablated = neuron_r2_scores - neuron_r2_scores_post\n",
        "             valid_r2_diff_ablated = r2_diff_ablated[~np.isnan(r2_diff_ablated)]\n",
        "             print(f\"Mean R Drop (Full Model):    {np.mean(valid_r2_diff_full):.4f}\")\n",
        "             print(f\"Mean R Drop (Ablated Model): {np.mean(valid_r2_diff_ablated):.4f}\")\n",
        "             print(f\"Median R Drop (Full Model):    {np.median(valid_r2_diff_full):.4f}\")\n",
        "             print(f\"Median R Drop (Ablated Model): {np.median(valid_r2_diff_ablated):.4f}\")\n",
        "        else:\n",
        "             print(\"\\nAblated model R-squared results not available for direct comparison.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Could not calculate R-squared for enough neurons in the full model.\")\n"
      ],
      "metadata": {
        "id": "yYHF8R6tn7-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, so basically the same thing happens in the full model. Hence, this does not seem the be the answer. I suspect it is indeed because the ReLU zeroes out parts of the output, which means that you do not get a good correlation overall. Where do we go from here?\n",
        "\n",
        "Let's check whether the pre-ReLU change introduced by the PC2 is a simple function (e.g., linear or quadratic) of the PC2 itself. To do this, we compute the difference in pre-ReLU values for the full and ablated model."
      ],
      "metadata": {
        "id": "tAhQ62huoGgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFitting delta_z_mlp (pre-ReLU change) vs. PC2 Features...\")\n",
        "\n",
        "# --- Check if data is available ---\n",
        "if 'delta_z_mlp' not in locals() or delta_z_mlp is None:\n",
        "    print(\"Error: 'delta_z_mlp' not found. Please calculate it first.\")\n",
        "elif 'X_pc2_features' not in locals() or X_pc2_features is None:\n",
        "    print(\"Error: 'X_pc2_features' not found. Please extract PC2 features first.\")\n",
        "else:\n",
        "    num_samples, num_neurons = delta_z_mlp.shape\n",
        "    assert num_neurons == d_mlp\n",
        "\n",
        "    # Arrays to store R-squared scores\n",
        "    neuron_r2_linear_fit = np.zeros(d_mlp)\n",
        "    neuron_r2_poly_fit = np.zeros(d_mlp)\n",
        "\n",
        "    # --- Prepare Polynomial Features ---\n",
        "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "    X_pc2_poly_features = poly.fit_transform(X_pc2_features) # Shape [num_samples, 5]\n",
        "\n",
        "    # --- Fit Models for Each Neuron ---\n",
        "    print(f\"Fitting models for {d_mlp} neurons...\")\n",
        "    for i in range(d_mlp):\n",
        "        # Target variable: pre-ReLU change for neuron i\n",
        "        y_delta_z_neuron = delta_z_mlp[:, i]\n",
        "\n",
        "        # Check for constant change (no variance)\n",
        "        if np.std(y_delta_z_neuron) < 1e-9:\n",
        "            neuron_r2_linear_fit[i] = np.nan\n",
        "            neuron_r2_poly_fit[i] = np.nan\n",
        "            continue\n",
        "\n",
        "        # --- Linear Fit ---\n",
        "        # Fit: delta_z_i  w_a*PC2(a) + w_b*PC2(b) + c\n",
        "        lin_reg_delta_z = LinearRegression()\n",
        "        lin_reg_delta_z.fit(X_pc2_features, y_delta_z_neuron)\n",
        "        neuron_r2_linear_fit[i] = lin_reg_delta_z.score(X_pc2_features, y_delta_z_neuron)\n",
        "\n",
        "        # --- Polynomial Fit ---\n",
        "        # Fit: delta_z_i  quadratic_func(PC2(a), PC2(b)) + c\n",
        "        poly_reg_delta_z = LinearRegression()\n",
        "        poly_reg_delta_z.fit(X_pc2_poly_features, y_delta_z_neuron)\n",
        "        neuron_r2_poly_fit[i] = poly_reg_delta_z.score(X_pc2_poly_features, y_delta_z_neuron)\n",
        "\n",
        "    print(\"Fitting complete.\")\n",
        "\n",
        "    # --- Analyze R-squared Distributions ---\n",
        "    valid_r2_linear = neuron_r2_linear_fit[~np.isnan(neuron_r2_linear_fit)]\n",
        "    valid_r2_poly = neuron_r2_poly_fit[~np.isnan(neuron_r2_poly_fit)]\n",
        "\n",
        "    if len(valid_r2_linear) > 0 and len(valid_r2_poly) > 0:\n",
        "        print(\"\\n--- Analysis of R (Fit delta_z_mlp vs PC2 Features) ---\")\n",
        "\n",
        "        print(\"\\n--- Linear Fit ---\")\n",
        "        print(f\"Mean R:   {np.mean(valid_r2_linear):.4f}\")\n",
        "        print(f\"Median R: {np.median(valid_r2_linear):.4f}\")\n",
        "        print(f\"Max R:    {np.max(valid_r2_linear):.4f}\")\n",
        "        print(f\"Neurons with R > 0.90: {np.sum(valid_r2_linear > 0.90)} / {len(valid_r2_linear)}\")\n",
        "        print(f\"Neurons with R > 0.99: {np.sum(valid_r2_linear > 0.99)} / {len(valid_r2_linear)}\")\n",
        "\n",
        "        print(\"\\n--- Polynomial Fit (Degree 2) ---\")\n",
        "        print(f\"Mean R:   {np.mean(valid_r2_poly):.4f}\")\n",
        "        print(f\"Median R: {np.median(valid_r2_poly):.4f}\")\n",
        "        print(f\"Max R:    {np.max(valid_r2_poly):.4f}\")\n",
        "        print(f\"Neurons with R > 0.90: {np.sum(valid_r2_poly > 0.90)} / {len(valid_r2_poly)}\")\n",
        "        print(f\"Neurons with R > 0.99: {np.sum(valid_r2_poly > 0.99)} / {len(valid_r2_poly)}\")\n",
        "\n",
        "\n",
        "        # --- Visualize Distributions ---\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
        "        sns.histplot(valid_r2_linear, bins=50, kde=False, ax=axes[0])\n",
        "        axes[0].set_title(\"R: Linear Fit of delta_z_mlp vs [PC2(a), PC2(b)]\")\n",
        "        axes[0].set_xlabel(\"R-squared Score per Neuron\")\n",
        "        axes[0].set_ylabel(\"Number of Neurons\")\n",
        "        axes[0].grid(axis='y', alpha=0.5)\n",
        "\n",
        "        sns.histplot(valid_r2_poly, bins=50, kde=False, ax=axes[1])\n",
        "        axes[1].set_title(\"R: Quadratic Fit of delta_z_mlp vs [PC2(a), PC2(b)]\")\n",
        "        axes[1].set_xlabel(\"R-squared Score per Neuron\")\n",
        "        axes[1].grid(axis='y', alpha=0.5)\n",
        "        plt.suptitle(\"How well do PC2 features predict the pre-ReLU change (delta_z_mlp)?\")\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"Could not calculate R-squared for any neurons.\")\n"
      ],
      "metadata": {
        "id": "6nKSdiHOo8SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So a linear fit is already very good, a quadratic fit improves it only slightly. There's a few neurons that have roughly ~0 R^2 -- I suspect they might be neurons that anyway do not contribute to the computation (but I don't know that for a fact).\n",
        "\n",
        "Ok, so this indicates that the change in pre-ReLU values is a simple function of PC2. What does this tell us? Can we infer anything about the algorithm from here? PC2 is somehow directly being used to change the pre-ReLU values so that the 'right' neurons fire. But what are these 'right' neurons?\n",
        "\n",
        "Could we perhaps check which neurons used to fire but don't anymore in the ablated model for one of the cases where it fails by a lot (e.g., 0,0 input, where it misses the correct output by 3)? And perhaps try to understand how the effective W_L matrix makes use of these neurons? Perhaps this can give us a clue into what's happening?\n",
        "\n",
        "More broadly, I think we still don't have a good picture of how many of these neurons are actually relevant. We have observed that tens of neurons have strong correlations with the sum, but we don't know if/how they are used by the model, as we don't know how W_L affects them. Any one of the following could hold:\n",
        "\n",
        "1. They are treated roughly equally, i.e., similar weights in W_L, indicating that the algorithm is truly distributed\n",
        "2. Only one/a few are not 'zeroed out', indicating that despite the fact that the transformer learns a distributed representation in the MLP, this is not actually used\n",
        "3. The weights are severely input-dependent, indicating that some neurons are used for given input pairs, and others for other\n",
        "4. ...\n",
        "\n",
        "Let's indeed check for neurons that change firing state in ablated model for the (0,0) input case."
      ],
      "metadata": {
        "id": "3AvBuUE_pP9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Ensure W_L is NumPy for consistency in this block ---\n",
        "# If W_L is potentially a tensor, convert it now\n",
        "if isinstance(W_L, torch.Tensor):\n",
        "    print(\"Converting W_L tensor to NumPy array...\")\n",
        "    W_L_np = W_L.detach().cpu().numpy()\n",
        "else:\n",
        "    W_L_np = np.asarray(W_L) # Ensure it's a NumPy array if not a tensor\n",
        "\n",
        "# --- Find index for input (0,0) ---\n",
        "try:\n",
        "    idx_00 = np.where((original_a_values == 0) & (original_b_values == 0))[0][0]\n",
        "    print(f\"Found sample index for input (0,0): {idx_00}\")\n",
        "except IndexError:\n",
        "    print(\"Error: Input pair (0,0) not found in the test set data used for activations.\")\n",
        "    idx_00 = None\n",
        "\n",
        "if idx_00 is not None:\n",
        "    # --- Get pre-ReLU activations for (0,0) ---\n",
        "    z_full_00 = z_mlp_full[idx_00]      # Should be NumPy\n",
        "    z_ablated_00 = z_mlp_ablated[idx_00] # Should be NumPy\n",
        "\n",
        "    # --- Identify Flipping Neurons ---\n",
        "    ablated_active_mask = z_ablated_00 > 0\n",
        "    full_active_mask = z_full_00 > 0\n",
        "\n",
        "    off_to_on_mask = (~ablated_active_mask) & full_active_mask\n",
        "    on_to_off_mask = ablated_active_mask & (~full_active_mask)\n",
        "\n",
        "    off_to_on_indices = np.where(off_to_on_mask)[0] # NumPy array of indices\n",
        "    on_to_off_indices = np.where(on_to_off_mask)[0] # NumPy array of indices\n",
        "\n",
        "    num_off_to_on = len(off_to_on_indices)\n",
        "    num_on_to_off = len(on_to_off_indices)\n",
        "    print(f\"\\nAnalysis for input (0,0) [True=0, Ablated=3]:\")\n",
        "    print(f\"  Neurons flipped OFF -> ON : {num_off_to_on}\")\n",
        "    print(f\"  Neurons flipped ON -> OFF : {num_on_to_off}\")\n",
        "\n",
        "    # --- Analyze W_L weights for flipping neurons ---\n",
        "    correct_logit_idx = 0\n",
        "    ablated_logit_idx = 3\n",
        "\n",
        "    print(\"\\n-- Neurons Turning ON (OFF -> ON) --\")\n",
        "    if num_off_to_on > 0:\n",
        "        # Slice W_L_np using NumPy indices - result is NumPy array\n",
        "        W_L_0_oto_np = W_L_np[correct_logit_idx, off_to_on_indices]\n",
        "        W_L_3_oto_np = W_L_np[ablated_logit_idx, off_to_on_indices]\n",
        "\n",
        "        if num_off_to_on == 1:\n",
        "            oto_mean_0, oto_std_0 = W_L_0_oto_np.item(), 0.0\n",
        "            oto_mean_3, oto_std_3 = W_L_3_oto_np.item(), 0.0\n",
        "            prop_correct_sign_0 = 1.0 if oto_mean_0 > 0 else 0.0\n",
        "            prop_correct_sign_3 = 1.0 if oto_mean_3 < 0 else 0.0\n",
        "            print(f\"  Weight for Neuron {off_to_on_indices[0]}:\")\n",
        "            print(f\"    W_L[{correct_logit_idx}]: {oto_mean_0:.4f}\")\n",
        "            print(f\"    W_L[{ablated_logit_idx}]: {oto_mean_3:.4f}\")\n",
        "        else:\n",
        "            oto_mean_0, oto_std_0 = np.mean(W_L_0_oto_np), np.std(W_L_0_oto_np)\n",
        "            oto_mean_3, oto_std_3 = np.mean(W_L_3_oto_np), np.std(W_L_3_oto_np)\n",
        "            prop_correct_sign_0 = np.mean(W_L_0_oto_np > 0)\n",
        "            prop_correct_sign_3 = np.mean(W_L_3_oto_np < 0)\n",
        "            print(f\"  Mean W_L weight for Logit {correct_logit_idx}: {oto_mean_0:.4f} (Std: {oto_std_0:.4f})\")\n",
        "            print(f\"  Mean W_L weight for Logit {ablated_logit_idx}: {oto_mean_3:.4f} (Std: {oto_std_3:.4f})\")\n",
        "            print(f\"  Proportion with W_L[{correct_logit_idx}] > 0 : {prop_correct_sign_0:.2f}\")\n",
        "            print(f\"  Proportion with W_L[{ablated_logit_idx}] < 0 : {prop_correct_sign_3:.2f}\")\n",
        "    else:\n",
        "        print(\"  None\")\n",
        "\n",
        "\n",
        "    print(\"\\n-- Neurons Turning OFF (ON -> OFF) --\")\n",
        "    if num_on_to_off > 0:\n",
        "        # Slice W_L_np using NumPy indices - result is NumPy array\n",
        "        W_L_0_otf_np = W_L_np[correct_logit_idx, on_to_off_indices]\n",
        "        W_L_3_otf_np = W_L_np[ablated_logit_idx, on_to_off_indices]\n",
        "\n",
        "        if num_on_to_off == 1:\n",
        "            otf_mean_0, otf_std_0 = W_L_0_otf_np.item(), 0.0\n",
        "            otf_mean_3, otf_std_3 = W_L_3_otf_np.item(), 0.0\n",
        "            prop_correct_sign_0 = 1.0 if otf_mean_0 < 0 else 0.0\n",
        "            prop_correct_sign_3 = 1.0 if otf_mean_3 > 0 else 0.0\n",
        "            print(f\"  Weight for Neuron {on_to_off_indices[0]}:\")\n",
        "            print(f\"    W_L[{correct_logit_idx}]: {otf_mean_0:.4f}\")\n",
        "            print(f\"    W_L[{ablated_logit_idx}]: {otf_mean_3:.4f}\")\n",
        "        else:\n",
        "            # These should now work correctly as input is NumPy array\n",
        "            otf_mean_0, otf_std_0 = np.mean(W_L_0_otf_np), np.std(W_L_0_otf_np)\n",
        "            otf_mean_3, otf_std_3 = np.mean(W_L_3_otf_np), np.std(W_L_3_otf_np)\n",
        "            prop_correct_sign_0 = np.mean(W_L_0_otf_np < 0)\n",
        "            prop_correct_sign_3 = np.mean(W_L_3_otf_np > 0)\n",
        "            print(f\"  Mean W_L weight for Logit {correct_logit_idx}: {otf_mean_0:.4f} (Std: {otf_std_0:.4f})\")\n",
        "            print(f\"  Mean W_L weight for Logit {ablated_logit_idx}: {otf_mean_3:.4f} (Std: {otf_std_3:.4f})\")\n",
        "            print(f\"  Proportion with W_L[{correct_logit_idx}] < 0 : {prop_correct_sign_0:.2f}\")\n",
        "            print(f\"  Proportion with W_L[{ablated_logit_idx}] > 0 : {prop_correct_sign_3:.2f}\")\n",
        "    else:\n",
        "        print(\"  None\")\n",
        "\n",
        "\n",
        "    # --- Calculate total change in logit contribution from flipping ---\n",
        "    # Ensure delta_a_00 is numpy (should be from z_mlp numpy arrays)\n",
        "    a_full_00 = np.maximum(0, z_full_00)\n",
        "    a_ablated_00 = np.maximum(0, z_ablated_00)\n",
        "    delta_a_00 = a_full_00 - a_ablated_00 # NumPy array\n",
        "\n",
        "    delta_logit_0_oto = 0\n",
        "    delta_logit_3_oto = 0\n",
        "    if num_off_to_on > 0:\n",
        "        # Use W_L_np for calculation\n",
        "        delta_logit_0_oto = np.sum(W_L_np[correct_logit_idx, off_to_on_indices] * delta_a_00[off_to_on_indices])\n",
        "        delta_logit_3_oto = np.sum(W_L_np[ablated_logit_idx, off_to_on_indices] * delta_a_00[off_to_on_indices])\n",
        "\n",
        "    delta_logit_0_otf = 0\n",
        "    delta_logit_3_otf = 0\n",
        "    if num_on_to_off > 0:\n",
        "         # Use W_L_np for calculation\n",
        "         delta_logit_0_otf = np.sum(W_L_np[correct_logit_idx, on_to_off_indices] * delta_a_00[on_to_off_indices])\n",
        "         delta_logit_3_otf = np.sum(W_L_np[ablated_logit_idx, on_to_off_indices] * delta_a_00[on_to_off_indices])\n",
        "\n",
        "    print(\"\\n--- Approx. Logit Change from Flipping Neurons ---\")\n",
        "    print(\"  (Change = Sum[ W_L[logit, neuron] * (a_full - a_ablated) ] for flipping neurons)\")\n",
        "    print(f\"  From OFF->ON : dLogit[{correct_logit_idx}]={delta_logit_0_oto:.3f}, dLogit[{ablated_logit_idx}]={delta_logit_3_oto:.3f}\")\n",
        "    print(f\"  From ON->OFF : dLogit[{correct_logit_idx}]={delta_logit_0_otf:.3f}, dLogit[{ablated_logit_idx}]={delta_logit_3_otf:.3f}\")\n",
        "    print(f\"  Total Flip Contribution: dLogit[{correct_logit_idx}]={delta_logit_0_oto + delta_logit_0_otf:.3f}, dLogit[{ablated_logit_idx}]={delta_logit_3_oto + delta_logit_3_otf:.3f}\")\n"
      ],
      "metadata": {
        "id": "rQoCC1eiqrBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to interpret the results. We are looking at input (0,0). The expected output is 0, which the full model correctly outputs. The ablated model outputs 3. We are looking at which neurons change their activation status after ablating. So either they fire in the full model, but not in the ablated one, or vice-versa. We find 25 such neurons, 24 of which go from firing to not firing. The one that turns on does not change the relevant logits (i.e., for 0 and for 3) at all. The others due! Mostly by massively increasing the logit for 0, and decreasing for 3. Wait, shouldn't the signs be the other way around?\n",
        "\n",
        "Also, perhaps it makes sense to look at logit variation for all possible outputs due to these neurons? Perhaps this gives a clearer picture of the algorithm being implemented. Let's do so."
      ],
      "metadata": {
        "id": "iISEoB5GrbmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if idx_00 is not None and 'z_mlp_full' in locals() and 'z_mlp_ablated' in locals() and 'W_L_np' in locals():\n",
        "    print(f\"\\nCalculating full logit change profile for input (0,0)...\")\n",
        "\n",
        "    # --- Calculate Post-ReLU activation change for (0,0) ---\n",
        "    z_full_00 = z_mlp_full[idx_00]\n",
        "    z_ablated_00 = z_mlp_ablated[idx_00]\n",
        "    a_full_00 = np.maximum(0, z_full_00)\n",
        "    a_ablated_00 = np.maximum(0, z_ablated_00)\n",
        "    delta_a_00 = a_full_00 - a_ablated_00 # Shape: [d_mlp,]\n",
        "\n",
        "    # --- Calculate induced change for each logit ---\n",
        "    # Matrix multiplication: (num_outputs, d_mlp) @ (d_mlp,) -> (num_outputs,)\n",
        "    delta_logits_00 = W_L_np @ delta_a_00\n",
        "\n",
        "    # --- Visualize the Logit Change Profile ---\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    output_indices = np.arange(num_outputs)\n",
        "    colors = ['green' if i == 0 else 'red' if i == 3 else 'gray' for i in output_indices] # Highlight 0 and 3\n",
        "    bars = plt.bar(output_indices, delta_logits_00, color=colors)\n",
        "\n",
        "    plt.xlabel(\"Output Logit Index (k)\")\n",
        "    plt.ylabel(\"Approx. Change in Logit (Full - Ablated)\")\n",
        "    plt.title(\"Approx. Logit Change Profile for Input (0,0) due to PC2\")\n",
        "    plt.xticks(output_indices)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    # Add legend manually\n",
        "    from matplotlib.lines import Line2D\n",
        "    legend_elements = [Line2D([0], [0], color='green', lw=4, label=f'Logit {0} (Correct)'),\n",
        "                       Line2D([0], [0], color='red', lw=4, label=f'Logit {3} (Ablated Pred)'),\n",
        "                       Line2D([0], [0], color='gray', lw=4, label='Other Logits')]\n",
        "    plt.legend(handles=legend_elements)\n",
        "    plt.axhline(0, color='black', linewidth=0.5) # Zero line\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nLogit Change Values for (0,0):\")\n",
        "    for k, delta in enumerate(delta_logits_00):\n",
        "        print(f\"  Logit {k}: {delta:+.3f}\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping full logit change analysis due to missing data.\")\n"
      ],
      "metadata": {
        "id": "cEplXDOKtCCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is fascinating! The difference in logits looks like a cosine of period roughly k/2! This structure cannot be coincidental.\n",
        "\n",
        "Let's now also look at what the logits are for the full model and the ablated model. This tells us what we're subtracting a cosine from, and what we obtain in result."
      ],
      "metadata": {
        "id": "ap03WOzfyt5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Check if data is available ---\n",
        "if 'original_logits_pred_pos' not in locals() or \\\n",
        "   'ablated_logits_pred_pos' not in locals() or \\\n",
        "   'delta_logits_00' not in locals() or \\\n",
        "   idx_00 is None:\n",
        "    print(\"Error: Missing necessary logit data or index for (0,0).\")\n",
        "    print(\"Please ensure the initial model runs and the previous delta_logits calculation were successful.\")\n",
        "else:\n",
        "    # --- Extract Logit Vectors for (0,0) ---\n",
        "    logits_full_00 = original_logits_pred_pos[idx_00]\n",
        "    logits_ablated_00 = ablated_logits_pred_pos[idx_00]\n",
        "\n",
        "    # Optional: Verify the difference calculation\n",
        "    # print(\"Sanity Check: Max difference between calculated delta and (full - ablated):\",\n",
        "    #       np.max(np.abs(delta_logits_00 - (logits_full_00 - logits_ablated_00))))\n",
        "\n",
        "    # --- Create the Plot ---\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    output_indices = np.arange(num_outputs)\n",
        "\n",
        "    # Plot the three series\n",
        "    plt.plot(output_indices, logits_ablated_00, marker='o', linewidth=4, color='orange', label='Ablated Logits (Input 0,0)')\n",
        "    # plt.plot(output_indices, delta_logits_00, marker='x', linestyle=':', color='purple', label='Delta Logits (Full - Ablated)')\n",
        "    plt.plot(output_indices, logits_full_00, marker='s', linewidth=4, color='blue', label='Full Model Logits (Input 0,0)')\n",
        "\n",
        "    # Add vertical lines for key logits\n",
        "    plt.axvline(0, color='green', linestyle='-.', linewidth=1, label='Correct Logit (0)')\n",
        "    plt.axvline(3, color='red', linestyle='-.', linewidth=1, label='Ablated Prediction (3)')\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel(\"Output Logit Index (k)\")\n",
        "    plt.ylabel(\"Logit Value\")\n",
        "    plt.title(\"Logit Comparison for Input (0,0)\")\n",
        "    plt.xticks(output_indices)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.axhline(0, color='black', linewidth=0.5) # Zero line\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "fKRWoibBzKGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is less obviously a clear function shape, but let us try to describe it.\n",
        "\n",
        "I'd say the shape is roughly still similar for both models, and some semblanbce of periodicity is still present -- although maybe the periodic part is just me squinting too hard.\n",
        "\n",
        "They both have a dip at logit 9.\n",
        "\n",
        "They both go to zero at logit 19.\n",
        "\n",
        "The full model starts with its highest peak at 0, goes down until 5, has another brief peak 7, goes down to 9, and rises again from there to peak at 15, goes down to 18... This looks like maybe an attenuated sinusoidal? Maybe a sum of a few of them?\n",
        "\n",
        "The ablated model also looks like something along these lines, but it's starting at roughly 0 instead of having its peak there...\n",
        "\n",
        "Intriguing! Not sure where to go from here. Perhaps we can identify for the full model what the top W_L * neuron_activation contributions are, and plot these contributions to the logit separately, again for the (0,0) case? I am hypothesizing that what we are seeing is a sum of sinusoidals that is weighed in such a way to peak at the right places. By removing the PC2 we kill some of them, in a poorer approximation."
      ],
      "metadata": {
        "id": "rg74U4ylzljT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Check if data is available ---\n",
        "if 'a_full_00' not in locals() or a_full_00 is None:\n",
        "    print(\"Error: 'a_full_00' (full model activations for (0,0)) not found.\")\n",
        "elif 'W_L_np' not in locals() or W_L_np is None:\n",
        "    print(\"Error: 'W_L_np' (NumPy version of W_L) not found.\")\n",
        "else:\n",
        "    print(f\"\\nCalculating neuron contributions to logits for input (0,0)...\")\n",
        "\n",
        "    # --- 1. Calculate individual neuron contributions ---\n",
        "    # Element-wise multiplication with broadcasting:\n",
        "    # (num_outputs, d_mlp) * (d_mlp,) -> (num_outputs, d_mlp)\n",
        "    neuron_logit_contributions_00 = W_L_np * a_full_00 # Note: uses broadcasting\n",
        "\n",
        "    # Verify shape\n",
        "    # print(f\"Shape of W_L_np: {W_L_np.shape}\")\n",
        "    # print(f\"Shape of a_full_00: {a_full_00.shape}\")\n",
        "    # print(f\"Shape of contributions: {neuron_logit_contributions_00.shape}\") # Should be [num_outputs, d_mlp]\n",
        "\n",
        "    # Sanity check: Sum of contributions should approximate the final logits\n",
        "    # approx_logits_full_00 = np.sum(neuron_logit_contributions_00, axis=1)\n",
        "    # if 'logits_full_00' in locals(): # Compare if original logits are available\n",
        "    #      print(\"Max difference between sum of contributions and original logits:\",\n",
        "    #            np.max(np.abs(approx_logits_full_00 - logits_full_00)))\n",
        "\n",
        "\n",
        "    # --- 2. Identify top contributing neurons ---\n",
        "    # Sum absolute contributions across all logits for each neuron\n",
        "    total_abs_contribution = np.sum(np.abs(neuron_logit_contributions_00), axis=0) # Sum over outputs -> shape [d_mlp,]\n",
        "    top_contributing_indices = np.argsort(total_abs_contribution)[::-1] # Sort descending\n",
        "\n",
        "    num_top_to_plot = 5 # Adjust how many top neurons to visualize\n",
        "    print(f\"\\nTop {num_top_to_plot} contributing neurons for input (0,0):\")\n",
        "    for i in range(num_top_to_plot):\n",
        "        idx = top_contributing_indices[i]\n",
        "        print(f\"  Neuron {idx}: Total Abs Contribution = {total_abs_contribution[idx]:.4f}\")\n",
        "        if 'neuron_sum_correlations' in locals():\n",
        "            print(f\"     (Correlation w/ sum: {neuron_sum_correlations[idx]:.3f})\")\n",
        "\n",
        "\n",
        "    # --- 3. Plot contribution profiles ---\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    output_indices = np.arange(num_outputs)\n",
        "\n",
        "    # Plot overall full logit for reference\n",
        "    # if 'logits_full_00' in locals():\n",
        "    #     plt.plot(output_indices, logits_full_00, marker='', linestyle='-', color='black', linewidth=2.5, label='Total Logits (Full Model)')\n",
        "\n",
        "    # Plot contributions of top neurons\n",
        "    for i in range(num_top_to_plot):\n",
        "        neuron_idx = top_contributing_indices[i]\n",
        "        contribution_profile = neuron_logit_contributions_00[:, neuron_idx] # Shape [num_outputs,]\n",
        "        plt.plot(output_indices, contribution_profile, marker='.', linestyle='--',\n",
        "                 label=f'Neuron {neuron_idx} Contribution (AbsSum={total_abs_contribution[neuron_idx]:.2f})')\n",
        "\n",
        "\n",
        "    plt.xlabel(\"Output Logit Index (k)\")\n",
        "    plt.ylabel(\"Contribution to Logit Value (W_L[k, i] * a_full_00[i])\")\n",
        "    plt.title(f\"Contribution Profiles of Top {num_top_to_plot} Neurons for Input (0,0)\")\n",
        "    plt.xticks(output_indices)\n",
        "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.axhline(0, color='black', linewidth=0.5) # Zero line\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1RqarFQQ1K8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, this does not quite support my hypothesis. Of the top 20 neurons, 19 of them have a function shape that looks quite similar to the output logit -- the shape we described earlier and seems kind of sinusoidal if you squint but... not quite! Then there's one of this top 20 that seems to just be screwing things up, and actually peaks at 5 rather than 0, and starts negative at zero. That's neuron 398 I believe. The top 2 neurons are significanlt 'stronger' than the rest and are 357 and 184. Are these maybe also (among) the top 2 neurons for other inputs?\n",
        "\n",
        "This seems indeed very distributed. I wonder the following:\n",
        "\n",
        "1. Do we see the same shape for other inputs? If we do, then we can maybe hypothesize what the function is that it is learning and try to fit it? Although it kind of looks like a non-trivial function...\n",
        "2. What do the separate neuron contributions look like for the ablated model?\n",
        "\n",
        "Let's below plot the logits for inputs (4,4), to investigate question 1."
      ],
      "metadata": {
        "id": "egklLAcM1v_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_a, target_b = 9, 9\n",
        "target_sum = target_a + target_b\n",
        "\n",
        "try:\n",
        "    idx_target = np.where((original_a_values == target_a) & (original_b_values == target_b))[0][0]\n",
        "    print(f\"Found sample index for input ({target_a},{target_b}): {idx_target}\")\n",
        "\n",
        "    logits_full_target = original_logits_pred_pos[idx_target]\n",
        "\n",
        "    # --- Create the Comparison Plot ---\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    output_indices = np.arange(num_outputs)\n",
        "\n",
        "    plt.plot(output_indices, logits_full_00, marker='s', linestyle='-', color='blue', label=f'Logits for (0,0) -> Target 0')\n",
        "    plt.plot(output_indices, logits_full_target, marker='o', linestyle='--', color='green', label=f'Logits for ({target_a},{target_b}) -> Target {target_sum}')\n",
        "\n",
        "    plt.axvline(0, color='blue', linestyle='-.', linewidth=1, label='Target for (0,0)')\n",
        "    plt.axvline(target_sum, color='green', linestyle='-.', linewidth=1, label=f'Target for ({target_a},{target_b})')\n",
        "\n",
        "    plt.xlabel(\"Output Logit Index (k)\")\n",
        "    plt.ylabel(\"Logit Value (Full Model)\")\n",
        "    plt.title(\"Logit Shape Comparison for Different Inputs\")\n",
        "    plt.xticks(output_indices)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.axhline(0, color='black', linewidth=0.5) # Zero line\n",
        "    plt.show()\n",
        "\n",
        "except IndexError:\n",
        "    print(f\"Error: Input pair ({target_a},{target_b}) not found in the test set data used.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "6QqVN4IO2yEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's hard to tell whether it's the same shape. Seems like a non-trivial function. I wouldn't know what to fit it to. I am still confused about why the difference in logits looks like a cosine, but the logits themselves don't obviously map to a sinusoidal. Or could it be that each W_L * activation is already such a complex sinusoidal, with this being learnt somewhere earlier in the process? What is the easiest way to check this hypothesis? Maybe fit the logits of the full model to a sum of sines/cosines? Fourier basis? Let's perform Fourier analysis on the full model logits for input (0,0) to try to verify this."
      ],
      "metadata": {
        "id": "2J0Np2uP3xus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Check if data is available ---\n",
        "if 'logits_full_00' not in locals() or logits_full_00 is None:\n",
        "    print(\"Error: 'logits_full_00' not found.\")\n",
        "else:\n",
        "    print(f\"\\nPerforming Fourier Analysis on Full Model Logits for Input (0,0)...\")\n",
        "\n",
        "    # --- 1. Compute DFT ---\n",
        "    # Use rfft for real-valued input, gives positive frequencies only\n",
        "    fourier_coeffs = np.fft.rfft(logits_full_00)\n",
        "    # Frequencies corresponding to the coefficients\n",
        "    frequencies = np.fft.rfftfreq(num_outputs) # Length matches output of rfft\n",
        "\n",
        "    # Get magnitudes\n",
        "    fourier_magnitudes = np.abs(fourier_coeffs)\n",
        "\n",
        "    # Normalize magnitudes (optional, for easier comparison)\n",
        "    # fourier_magnitudes /= np.max(fourier_magnitudes)\n",
        "\n",
        "    # --- 2. Analyze Magnitudes ---\n",
        "    print(\"\\nFourier Coefficient Magnitudes:\")\n",
        "    # Sort by magnitude to find dominant frequencies\n",
        "    sorted_indices = np.argsort(fourier_magnitudes)[::-1]\n",
        "    num_top_freqs_to_show = 10\n",
        "    print(f\"  Top {num_top_freqs_to_show} Frequencies by Magnitude:\")\n",
        "    for i in range(min(num_top_freqs_to_show, len(frequencies))):\n",
        "        idx = sorted_indices[i]\n",
        "        freq = frequencies[idx]\n",
        "        mag = fourier_magnitudes[idx]\n",
        "        print(f\"    Freq Index {idx} (Freq  {freq:.3f}): Magnitude = {mag:.4f}\")\n",
        "\n",
        "    # --- 3. Visualize ---\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(range(len(fourier_magnitudes)), fourier_magnitudes, width=0.9)\n",
        "    plt.xlabel(\"Frequency Index (k)\")\n",
        "    plt.ylabel(\"Magnitude of Fourier Coefficient\")\n",
        "    plt.title(\"Fourier Transform Magnitudes for Full Model Logits [Input (0,0)]\")\n",
        "    # Optional: Add frequency values to x-axis if helpful, can get crowded\n",
        "    # plt.xticks(range(len(fourier_magnitudes)), [f\"{f:.2f}\" for f in frequencies], rotation=45, ha='right')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "    # --- 4. Optional: Variance Explained by Top K Frequencies ---\n",
        "    k_to_check = 6 # How many top frequencies to use for reconstruction\n",
        "    print(f\"\\nChecking variance explained by top {k_to_check} frequencies...\")\n",
        "\n",
        "    # Create a filtered coefficient array (zeros except for top k)\n",
        "    filtered_coeffs = np.zeros_like(fourier_coeffs)\n",
        "    # Keep DC component (index 0) + top k non-DC frequencies\n",
        "    top_k_indices = sorted_indices[:k_to_check+1] # Get indices including potential DC\n",
        "    # Ensure we don't double-count or miss components if top k includes index 0\n",
        "    indices_to_keep = np.union1d([0], top_k_indices) # Always keep DC, add others\n",
        "\n",
        "    filtered_coeffs[indices_to_keep] = fourier_coeffs[indices_to_keep]\n",
        "\n",
        "    # Inverse transform to reconstruct the signal\n",
        "    reconstructed_logits = np.fft.irfft(filtered_coeffs, n=num_outputs)\n",
        "\n",
        "    # Calculate variance explained (R-squared like measure)\n",
        "    variance_original = np.var(logits_full_00)\n",
        "    variance_residual = np.var(logits_full_00 - reconstructed_logits)\n",
        "    if variance_original > 1e-9: # Avoid division by zero\n",
        "        variance_explained = 1 - (variance_residual / variance_original)\n",
        "        print(f\"  Variance explained by top {k_to_check} frequencies (incl. DC): {variance_explained:.4f}\")\n",
        "    else:\n",
        "        print(\"  Cannot calculate variance explained (original variance is zero).\")\n",
        "\n",
        "    # Optional: Plot reconstruction\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    output_indices = np.arange(num_outputs)\n",
        "    plt.plot(output_indices, logits_full_00, marker='o', linestyle='-', label='Original Logits')\n",
        "    plt.plot(output_indices, reconstructed_logits, marker='x', linestyle='--', label=f'Reconstructed (Top {k_to_check} Freqs)')\n",
        "    plt.title(f\"Logit Reconstruction using Top {k_to_check} Fourier Frequencies [Input (0,0)]\")\n",
        "    plt.xlabel(\"Output Logit Index (k)\")\n",
        "    plt.ylabel(\"Logit Value\")\n",
        "    plt.xticks(output_indices)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Jv_oJ4h94qrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reconstruction using the top 3 frequencies looks quite similar already! Although it would actually produce the wrong output, as 0 goes down slightly, and 1 and 2 both come up slightly, ending above it! 9 is also less pronouncedly negative.  Adding more frequencies progressively improves, and with 6 we get the correct output. Let's see if we obtain similar results for other inputs, for example (4,4)."
      ],
      "metadata": {
        "id": "ssS3tDsx8aI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_a, target_b = 4, 4\n",
        "target_sum = target_a + target_b\n",
        "\n",
        "idx_target = np.where((original_a_values == target_a) & (original_b_values == target_b))[0][0]\n",
        "print(f\"Found sample index for input ({target_a},{target_b}): {idx_target}\")\n",
        "\n",
        "logits_full_target = original_logits_pred_pos[idx_target]\n",
        "\n",
        "print(f\"\\nPerforming Fourier Analysis on Full Model Logits for Input (4,4)...\")\n",
        "\n",
        "# --- 1. Compute DFT ---\n",
        "# Use rfft for real-valued input, gives positive frequencies only\n",
        "fourier_coeffs = np.fft.rfft(logits_full_target)\n",
        "# Frequencies corresponding to the coefficients\n",
        "frequencies = np.fft.rfftfreq(num_outputs) # Length matches output of rfft\n",
        "\n",
        "# Get magnitudes\n",
        "fourier_magnitudes = np.abs(fourier_coeffs)\n",
        "\n",
        "# Normalize magnitudes (optional, for easier comparison)\n",
        "# fourier_magnitudes /= np.max(fourier_magnitudes)\n",
        "\n",
        "# --- 2. Analyze Magnitudes ---\n",
        "print(\"\\nFourier Coefficient Magnitudes:\")\n",
        "# Sort by magnitude to find dominant frequencies\n",
        "sorted_indices = np.argsort(fourier_magnitudes)[::-1]\n",
        "num_top_freqs_to_show = 10\n",
        "print(f\"  Top {num_top_freqs_to_show} Frequencies by Magnitude:\")\n",
        "for i in range(min(num_top_freqs_to_show, len(frequencies))):\n",
        "    idx = sorted_indices[i]\n",
        "    freq = frequencies[idx]\n",
        "    mag = fourier_magnitudes[idx]\n",
        "    print(f\"    Freq Index {idx} (Freq  {freq:.3f}): Magnitude = {mag:.4f}\")\n",
        "\n",
        "# --- 3. Visualize ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(len(fourier_magnitudes)), fourier_magnitudes, width=0.9)\n",
        "plt.xlabel(\"Frequency Index (k)\")\n",
        "plt.ylabel(\"Magnitude of Fourier Coefficient\")\n",
        "plt.title(\"Fourier Transform Magnitudes for Full Model Logits [Input (0,0)]\")\n",
        "# Optional: Add frequency values to x-axis if helpful, can get crowded\n",
        "# plt.xticks(range(len(fourier_magnitudes)), [f\"{f:.2f}\" for f in frequencies], rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# --- 4. Optional: Variance Explained by Top K Frequencies ---\n",
        "k_to_check = 6 # How many top frequencies to use for reconstruction\n",
        "print(f\"\\nChecking variance explained by top {k_to_check} frequencies...\")\n",
        "\n",
        "# Create a filtered coefficient array (zeros except for top k)\n",
        "filtered_coeffs = np.zeros_like(fourier_coeffs)\n",
        "# Keep DC component (index 0) + top k non-DC frequencies\n",
        "top_k_indices = sorted_indices[:k_to_check+1] # Get indices including potential DC\n",
        "# Ensure we don't double-count or miss components if top k includes index 0\n",
        "indices_to_keep = np.union1d([0], top_k_indices) # Always keep DC, add others\n",
        "\n",
        "filtered_coeffs[indices_to_keep] = fourier_coeffs[indices_to_keep]\n",
        "\n",
        "# Inverse transform to reconstruct the signal\n",
        "reconstructed_logits = np.fft.irfft(filtered_coeffs, n=num_outputs)\n",
        "\n",
        "# Calculate variance explained (R-squared like measure)\n",
        "variance_original = np.var(logits_full_target)\n",
        "variance_residual = np.var(logits_full_target - reconstructed_logits)\n",
        "if variance_original > 1e-9: # Avoid division by zero\n",
        "    variance_explained = 1 - (variance_residual / variance_original)\n",
        "    print(f\"  Variance explained by top {k_to_check} frequencies (incl. DC): {variance_explained:.4f}\")\n",
        "else:\n",
        "    print(\"  Cannot calculate variance explained (original variance is zero).\")\n",
        "\n",
        "# Optional: Plot reconstruction\n",
        "plt.figure(figsize=(12, 6))\n",
        "output_indices = np.arange(num_outputs)\n",
        "plt.plot(output_indices, logits_full_target, marker='o', linestyle='-', label='Original Logits')\n",
        "plt.plot(output_indices, reconstructed_logits, marker='x', linestyle='--', label=f'Reconstructed (Top {k_to_check} Freqs)')\n",
        "plt.title(f\"Logit Reconstruction using Top {k_to_check} Fourier Frequencies [Input (0,0)]\")\n",
        "plt.xlabel(\"Output Logit Index (k)\")\n",
        "plt.ylabel(\"Logit Value\")\n",
        "plt.xticks(output_indices)\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zux0czpK56qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We hypothesize that the network implements a Fourier-like algorithm where the final logits are synthesized by summing trigonometric basis functions (sines/cosines) weighted by MLP neuron activations. To test the readout part of this hypothesis, we will analyze the effective neuron-logit weight matrix, W_L. If the hypothesis is correct, the columns of W_L (each corresponding to a single neuron's influence on all output logits) should themselves resemble discrete sine or cosine waves of specific frequencies. Therefore, we will compute the Discrete Fourier Transform (DFT) for each column (neuron) of W_L. We expect to find that for many neurons, especially those relevant to the computation, the power in their Fourier spectrum is concentrated at just one or a few low-frequency indices, indicating they represent specific Fourier basis components.\n"
      ],
      "metadata": {
        "id": "z09AzLK48Xbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Check if data is available ---\n",
        "if 'W_L_np' not in locals() or W_L_np is None:\n",
        "    print(\"Error: 'W_L_np' (NumPy version of W_L) not found.\")\n",
        "elif 'neuron_sum_correlations' not in locals():\n",
        "     print(\"Warning: 'neuron_sum_correlations' not found. Heatmap columns will not be sorted.\")\n",
        "     # Create a dummy sorter if correlations are missing\n",
        "     neuron_sum_correlations = np.arange(d_mlp)\n",
        "else:\n",
        "    print(f\"\\nPerforming Fourier Analysis on columns (neurons) of W_L...\")\n",
        "\n",
        "    # --- Compute DFT for each column ---\n",
        "    # Use rfft for real-valued input. Output length is n//2 + 1\n",
        "    num_freqs = np.fft.rfftfreq(num_outputs).shape[0]\n",
        "    neuron_fourier_mags = np.zeros((num_freqs, d_mlp))\n",
        "\n",
        "    print(f\"Calculating DFT for {d_mlp} neurons...\")\n",
        "    for i in range(d_mlp):\n",
        "        w_col = W_L_np[:, i] # Get weights for neuron i across all logits\n",
        "        fourier_coeffs = np.fft.rfft(w_col)\n",
        "        neuron_fourier_mags[:, i] = np.abs(fourier_coeffs)\n",
        "    print(\"DFT calculation complete.\")\n",
        "\n",
        "    # --- Normalize magnitudes per neuron (optional but recommended for visualization) ---\n",
        "    # This makes peaks comparable across neurons with different overall weight norms\n",
        "    max_mags_per_neuron = np.max(neuron_fourier_mags, axis=0, keepdims=True)\n",
        "    # Avoid division by zero for neurons with all zero weights\n",
        "    max_mags_per_neuron[max_mags_per_neuron < 1e-9] = 1.0\n",
        "    neuron_fourier_mags_normalized = neuron_fourier_mags / max_mags_per_neuron\n",
        "\n",
        "\n",
        "    # --- Visualize as Heatmap (Sorted by Neuron Correlation) ---\n",
        "    # Sort neurons by correlation\n",
        "    corr_sorted_indices = np.argsort(neuron_sum_correlations)\n",
        "    heatmap_data = neuron_fourier_mags_normalized[:, corr_sorted_indices]\n",
        "    frequencies = np.fft.rfftfreq(num_outputs) # For labeling y-axis\n",
        "\n",
        "    print(\"\\nGenerating heatmap of normalized Fourier magnitudes...\")\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    sns.heatmap(heatmap_data, cmap='viridis', cbar=True,\n",
        "                cbar_kws={'label': 'Normalized Magnitude (Max=1 per Neuron)'})\n",
        "                # Use robust=True instead of normalization if preferred:\n",
        "                # sns.heatmap(neuron_fourier_mags[:, corr_sorted_indices], cmap='viridis', robust=True, cbar=True)\n",
        "    plt.title(\"Normalized Fourier Magnitudes of W_L Columns (Neurons)\")\n",
        "    plt.xlabel(\"Neurons (Sorted by Correlation with Sum a+b)\")\n",
        "    plt.ylabel(\"Frequency Index (k)\")\n",
        "    # Label y-axis with frequency index k\n",
        "    plt.yticks(np.arange(num_freqs) + 0.5, labels=np.arange(num_freqs), rotation=0)\n",
        "    # Optional: Label y-axis with actual frequencies (can be dense)\n",
        "    # plt.yticks(np.arange(num_freqs)[::2] + 0.5, labels=[f\"{f:.2f}\" for f in frequencies[::2]], rotation=0)\n",
        "    plt.xticks([]) # Hide dense neuron labels\n",
        "    plt.show()\n",
        "\n",
        "    # --- Optional: Plot DFT for specific example neurons ---\n",
        "    # Example: Top 2 contributors from previous analysis (if available) + maybe one from each corr group\n",
        "    example_neuron_indices = []\n",
        "    if 'top_contributing_indices' in locals():\n",
        "         example_neuron_indices.extend(top_contributing_indices[:2].tolist())\n",
        "    # Add indices from specific correlation groups if desired (find manually or via sorting)\n",
        "    # example_neuron_indices.extend([idx_neg_group, idx_pos_group_low_corr])\n",
        "    # Ensure unique indices\n",
        "    example_neuron_indices = sorted(list(set(example_neuron_indices)))\n",
        "\n",
        "    if example_neuron_indices:\n",
        "        print(f\"\\nPlotting DFT magnitudes for example neurons: {example_neuron_indices}\")\n",
        "        n_examples = len(example_neuron_indices)\n",
        "        fig, axes = plt.subplots(1, n_examples, figsize=(5 * n_examples, 4), sharey=True)\n",
        "        if n_examples == 1: axes = [axes] # Make axes iterable if only one plot\n",
        "\n",
        "        for i, neuron_idx in enumerate(example_neuron_indices):\n",
        "            ax = axes[i]\n",
        "            mags = neuron_fourier_mags[:, neuron_idx] # Use non-normalized for scale\n",
        "            ax.bar(range(num_freqs), mags, width=0.9)\n",
        "            ax.set_title(f\"Neuron {neuron_idx} W_L DFT Mags\")\n",
        "            ax.set_xlabel(\"Freq Index (k)\")\n",
        "            if i == 0: ax.set_ylabel(\"Magnitude\")\n",
        "            ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "            # Highlight dominant frequency?\n",
        "            # peak_freq_idx = np.argmax(mags)\n",
        "            # ax.patches[peak_freq_idx].set_facecolor('red')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "j1Bm8D2X8gh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the heat map, frequency indices 1, 2 and 3 (roughly in that order) are the ones with highest normalized magnitude for basically all neurons studied. Frequency zero is largely irrelevant. Frequency indices greater than 3 are progressively less important\n",
        "\n",
        "Moving to the bar plots for two example neurons, the important frequencies are similar. 0 does not matter. 1 and 2 are the biggest (although 2 is biggest for 184 and 1 for 357).\n",
        "\n",
        "Let us now see if there is frequency-specific structure in how the model represents inputs or reads out outputs. This is similar to Figure 3 in Neel's paper.\n",
        "\n",
        "So we will analyze the frequency components of the model's key weight matrices: the embedding matrix (W_E) and the effective neuron-logit map (W_L = W_U @ W_out). For W_E, we compute the Discrete Fourier Transform (DFT) along the input token dimension (0-9) for each embedding feature and calculate the norm of these Fourier coefficients across the embedding dimension for each frequency. This reveals if certain frequencies are preferentially used to represent the input tokens. For W_L, we compute the DFT along the output logit dimension (0-18) for each neuron's weights and calculate the norm across the neuron dimension for each frequency. This reveals if the readout mechanism is structured to preferentially operate on specific output frequencies. Observing sparsity (only a few frequencies with high magnitude) in either plot would suggest the use of a Fourier-like basis, complementing our PCA findings.\n",
        "\n"
      ],
      "metadata": {
        "id": "vjBrJM1JC9iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Ensure matrices are NumPy ---\n",
        "if isinstance(W_E, torch.Tensor):\n",
        "    W_E_np = W_E.detach().cpu().numpy()\n",
        "else:\n",
        "    W_E_np = np.asarray(W_E)\n",
        "\n",
        "if isinstance(W_L, torch.Tensor):\n",
        "    W_L_np = W_L.detach().cpu().numpy()\n",
        "else:\n",
        "    W_L_np = np.asarray(W_L)\n",
        "\n",
        "# --- Analysis 1: Fourier Components of Embedding Matrix (W_E) ---\n",
        "print(f\"\\nAnalyzing Fourier components of W_E ({W_E_np.shape})...\")\n",
        "\n",
        "# Calculate DFT along the token dimension (axis 0) for each feature\n",
        "# Result shape: [num_input_freqs, d_model]\n",
        "w_e_fft_coeffs = np.fft.rfft(W_E_np, axis=0)\n",
        "w_e_fft_mags = np.abs(w_e_fft_coeffs)\n",
        "\n",
        "# Calculate L2 norm across the embedding dimension (d_model) for each frequency\n",
        "# Result shape: [num_input_freqs,]\n",
        "w_e_freq_norms = np.linalg.norm(w_e_fft_mags, axis=1)\n",
        "input_frequencies = np.fft.rfftfreq(N) # Frequencies for input size N\n",
        "\n",
        "print(\"W_E analysis complete.\")\n",
        "\n",
        "# --- Analysis 2: Fourier Components of Neuron-Logit Map (W_L) ---\n",
        "print(f\"\\nAnalyzing Fourier components of W_L ({W_L_np.shape})...\")\n",
        "\n",
        "# Calculate DFT along the logit dimension (axis 0) for each neuron's weights\n",
        "# Result shape: [num_output_freqs, d_mlp]\n",
        "w_l_fft_coeffs = np.fft.rfft(W_L_np, axis=0)\n",
        "w_l_fft_mags = np.abs(w_l_fft_coeffs)\n",
        "\n",
        "# Calculate L2 norm across the neuron dimension (d_mlp) for each frequency\n",
        "# Result shape: [num_output_freqs,]\n",
        "w_l_freq_norms = np.linalg.norm(w_l_fft_mags, axis=1)\n",
        "output_frequencies = np.fft.rfftfreq(num_outputs) # Frequencies for output size num_outputs\n",
        "\n",
        "print(\"W_L analysis complete.\")\n",
        "\n",
        "\n",
        "# --- Plotting (Similar to Nanda et al. Figure 3) ---\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot for W_E\n",
        "axes[0].bar(range(len(w_e_freq_norms)), w_e_freq_norms, width=0.9)\n",
        "axes[0].set_title(\"Norm of Fourier Components of Embedding Matrix (W_E)\")\n",
        "axes[0].set_xlabel(f\"Input Frequency Index (k), N={N}\")\n",
        "axes[0].set_ylabel(\"Norm across d_model\")\n",
        "axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "# axes[0].set_xticks(range(len(w_e_freq_norms))) # Only if few freqs\n",
        "\n",
        "# Plot for W_L\n",
        "axes[1].bar(range(len(w_l_freq_norms)), w_l_freq_norms, width=0.9)\n",
        "axes[1].set_title(\"Norm of Fourier Components of Neuron-Logit Map (W_L)\")\n",
        "axes[1].set_xlabel(f\"Output Frequency Index (k), P={num_outputs}\")\n",
        "axes[1].set_ylabel(\"Norm across d_mlp\")\n",
        "axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "# axes[1].set_xticks(range(len(w_l_freq_norms))) # Only if few freqs\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Sa01tzuxCylg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've seen that the influence of PC2 on the pre-ReLU activations (delta_z_mlp) is quite simple (linear/quadratic in PC2 features), but the baseline pre-ReLU activations without PC2 (z_mlp_ablated) were only imperfectly linear with the target sum. To get a clearer picture of the computation before the ReLU non-linearity in the full model, we will now attempt to directly model the complete pre-ReLU activations (z_mlp_full) as a function of both PC1 and PC2 features extracted from the input tokens a and b. Specifically, we'll fit a degree-2 polynomial model (z_mlp_full  Poly2(PC1(a), PC1(b), PC2(a), PC2(b))) for each neuron. If this polynomial fit yields high R-squared values across most neurons, it suggests the MLP's computation before the ReLU is relatively straightforward and mathematically describable, isolating the main complexity to the ReLU step and the W_L readout.\n"
      ],
      "metadata": {
        "id": "HT1J_ep3Il7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFitting FULL model pre-ReLU activations (z_mlp_full) vs. Polynomial(PC1, PC2 features)...\")\n",
        "\n",
        "# --- Check if data is available ---\n",
        "if 'pca_scores_np' not in locals() or pca_scores_np is None or pca_scores_np.shape[1] < 2:\n",
        "    print(\"Error: 'pca_scores_np' with at least 2 components not found.\")\n",
        "elif 'z_mlp_full' not in locals() or z_mlp_full is None:\n",
        "    print(\"Error: 'z_mlp_full' not found. Please ensure full pre-ReLU activations were captured.\")\n",
        "else:\n",
        "    # --- 1. Extract PC1 and PC2 Features for all tokens ---\n",
        "    pc1_values_all_tokens = pca_scores_np[:, 0] # PC1 is index 0\n",
        "    pc2_values_all_tokens = pca_scores_np[:, 1] # PC2 is index 1\n",
        "\n",
        "    # --- 2. Map features to samples ---\n",
        "    p1a = pc1_values_all_tokens[original_a_values]\n",
        "    p1b = pc1_values_all_tokens[original_b_values]\n",
        "    p2a = pc2_values_all_tokens[original_a_values]\n",
        "    p2b = pc2_values_all_tokens[original_b_values]\n",
        "\n",
        "    # --- 3. Create base feature matrix ---\n",
        "    # Using individual features seems more general than pre-combining\n",
        "    X_features = np.stack((p1a, p1b, p2a, p2b), axis=-1) # Shape: [num_samples, 4]\n",
        "\n",
        "    # --- 4. Generate Polynomial Features (Degree 2) ---\n",
        "    # include_bias=True adds a column of 1s for the intercept term automatically\n",
        "    poly = PolynomialFeatures(degree=2, include_bias=True)\n",
        "    X_poly_features = poly.fit_transform(X_features)\n",
        "    # Note: This will include constant, linear terms (p1a,p1b,p2a,p2b), squares, and cross-terms.\n",
        "    # print(f\"Shape of polynomial features: {X_poly_features.shape}\") # Debugging shape\n",
        "    # print(f\"Polynomial features names: {poly.get_feature_names_out(['p1a', 'p1b', 'p2a', 'p2b'])}\") # Debugging names\n",
        "\n",
        "    # Array to store R-squared scores\n",
        "    neuron_r2_scores_z_full_fit = np.zeros(d_mlp)\n",
        "\n",
        "    # --- 5. Regression Loop ---\n",
        "    print(f\"Fitting polynomial model for {d_mlp} neurons...\")\n",
        "    for i in range(d_mlp):\n",
        "        # Target variable: full pre-ReLU activation for neuron i\n",
        "        y_neuron_z_full = z_mlp_full[:, i]\n",
        "\n",
        "        # Check for constant activation (no variance)\n",
        "        if np.std(y_neuron_z_full) < 1e-9:\n",
        "            neuron_r2_scores_z_full_fit[i] = np.nan\n",
        "            continue\n",
        "\n",
        "        # Fit model: z_full_i  Poly2(p1a, p1b, p2a, p2b)\n",
        "        # Use fit_intercept=False because PolynomialFeatures(include_bias=True) added it\n",
        "        poly_reg_z_full = LinearRegression(fit_intercept=False)\n",
        "        poly_reg_z_full.fit(X_poly_features, y_neuron_z_full)\n",
        "        neuron_r2_scores_z_full_fit[i] = poly_reg_z_full.score(X_poly_features, y_neuron_z_full)\n",
        "\n",
        "    print(\"Fitting complete.\")\n",
        "\n",
        "    # --- 6. Analysis ---\n",
        "    valid_r2_scores = neuron_r2_scores_z_full_fit[~np.isnan(neuron_r2_scores_z_full_fit)]\n",
        "\n",
        "    if len(valid_r2_scores) > 0:\n",
        "        print(\"\\n--- Analysis of R-squared (Fit z_mlp_full vs Poly2(PC1, PC2 features)) ---\")\n",
        "        print(f\"Mean R:   {np.mean(valid_r2_scores):.4f}\")\n",
        "        print(f\"Median R: {np.median(valid_r2_scores):.4f}\")\n",
        "        print(f\"Min R:    {np.min(valid_r2_scores):.4f}\")\n",
        "        print(f\"Max R:    {np.max(valid_r2_scores):.4f}\")\n",
        "        print(f\"Neurons with R > 0.95: {np.sum(valid_r2_scores > 0.95)} / {len(valid_r2_scores)}\")\n",
        "        print(f\"Neurons with R > 0.99: {np.sum(valid_r2_scores > 0.99)} / {len(valid_r2_scores)}\")\n",
        "        print(f\"Neurons with R > 0.999: {np.sum(valid_r2_scores > 0.999)} / {len(valid_r2_scores)}\") # Add higher threshold\n",
        "\n",
        "        # --- 7. Visualization ---\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(valid_r2_scores, bins=50, kde=False)\n",
        "        plt.title(\"R Distribution: Fit of Pre-ReLU Activations (z_full) vs. Poly2(PC1, PC2 Features)\")\n",
        "        plt.xlabel(\"R-squared Score per Neuron\")\n",
        "        plt.ylabel(\"Number of Neurons\")\n",
        "        plt.grid(axis='y', alpha=0.5)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Could not calculate R-squared for any neurons.\")\n"
      ],
      "metadata": {
        "id": "8XUis9PUImr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What This Conclusively Tells Us:**\n",
        "\n",
        "1.  **MLP Pre-ReLU Computation is Simple and Solved:** The core transformation performed by the MLP *before* the ReLU activation function (`z_mlp = MLP_Linear(PC1, PC2) + Bias`) is **extremely well-described** as a simple degree-2 polynomial function of the PC1 and PC2 features derived from the inputs `a` and `b`. There are no significant hidden complexities or dependencies on other obscure features happening *before* the ReLU.\n",
        "2.  **PC1 and PC2 Suffice:** The top 2 Principal Components of the embeddings contain virtually all the information the MLP uses to calculate its pre-activations. The remaining embedding dimensions or PCs contribute negligible information to this stage.\n",
        "3.  **Complexity Localized to ReLU and Readout:** This pinpoints exactly where the remaining complexity or \"magic\" must lie:\n",
        "    *   **ReLU:** The piecewise linear `max(0, ...)` operation acting on these smooth polynomial pre-activations (`z_mlp_full`) is the primary source of non-linearity that makes the *post-ReLU* activations (`a_mlp_full`) and the final output (`k_ablated`) harder to model directly.\n",
        "    *   **`W_L` Readout:** The structure of the `W_L` matrix (which we found uses a distributed, mixed low-frequency Fourier basis) determines how these post-ReLU activations `a_mlp_full` are interpreted and summed to form the final logits.\n",
        "\n",
        "**Updated Concrete Algorithm Description:**\n",
        "\n",
        "We can now state the algorithm with much higher confidence and specificity:\n",
        "\n",
        "1.  **Embedding & Feature Extraction:**\n",
        "    *   Map inputs `a, b` to embeddings `W_E(a), W_E(b)`.\n",
        "    *   Extract the scalar features `PC1(a), PC1(b), PC2(a), PC2(b)` representing the projections onto the first two principal components (linear and quadratic features).\n",
        "\n",
        "2.  **MLP Pre-Activation (Polynomial Computation):**\n",
        "    *   For *each neuron `i`* in the MLP layer:\n",
        "        `z_mlp_full[i] = Degree2Poly_i(PC1(a), PC1(b), PC2(a), PC2(b)) + Bias_i`\n",
        "    *   Where `Degree2Poly_i` is a specific, learned quadratic polynomial function (with coefficients determined by the MLP's `W_in` weights for neuron `i`) involving terms up to degree 2 of the four input features (e.g., `PC1(a)`, `PC1(b)`, `PC2(a)`, `PC2(b)`, `PC1(a)^2`, `PC1(a)*PC1(b)`, `PC1(a)*PC2(a)`, etc.).\n",
        "    *   **(This step is now known with high confidence).**\n",
        "\n",
        "3.  **MLP Activation (ReLU):**\n",
        "    *   `a_mlp_full[i] = max(0, z_mlp_full[i])`\n",
        "\n",
        "4.  **Unembedding/Readout (Distributed Fourier Synthesis):**\n",
        "    *   `Logits[k] = Sum_i ( W_L[k, i] * a_mlp_full[i] )`\n",
        "    *   Where the columns `W_L[:, i]` represent combinations of low-frequency (k=1, 2, 3...) Fourier basis vectors. The dot product synthesizes the final logit shape.\n",
        "\n",
        "Let's now look at what the coefficients of this polynomial actually are. Are they roughly the same for most/all neurons? Exactly *how* does the MLP make use of these linear and quadratic features?"
      ],
      "metadata": {
        "id": "j3DmrCNxJaiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAnalyzing coefficients of the Poly2 fit for z_mlp_full...\")\n",
        "\n",
        "# --- Check if data is available ---\n",
        "if 'X_poly_features' not in locals() or X_poly_features is None:\n",
        "    print(\"Error: 'X_poly_features' not found.\")\n",
        "elif 'z_mlp_full' not in locals() or z_mlp_full is None:\n",
        "    print(\"Error: 'z_mlp_full' not found.\")\n",
        "elif 'poly' not in locals() or poly is None:\n",
        "    print(\"Error: 'poly' (PolynomialFeatures object) not found.\")\n",
        "elif 'neuron_sum_correlations' not in locals() or neuron_sum_correlations is None:\n",
        "    print(\"Error: 'neuron_sum_correlations' not found.\")\n",
        "else:\n",
        "    num_samples, num_poly_feats = X_poly_features.shape\n",
        "    num_samples_z, num_neurons = z_mlp_full.shape\n",
        "    assert num_neurons == d_mlp\n",
        "    assert num_samples == num_samples_z\n",
        "\n",
        "    # --- Get Feature Names ---\n",
        "    # Define base feature names used to create polynomials\n",
        "    base_feature_names = ['p1a', 'p1b', 'p2a', 'p2b']\n",
        "    poly_feature_names = poly.get_feature_names_out(base_feature_names)\n",
        "    print(f\"Polynomial features ({num_poly_feats}): {poly_feature_names}\")\n",
        "\n",
        "    # --- Fit Models and Extract Coefficients ---\n",
        "    print(f\"Fitting models and extracting coefficients for {d_mlp} neurons...\")\n",
        "    learned_poly_coeffs = np.zeros((d_mlp, num_poly_feats))\n",
        "    neuron_fit_successful = np.zeros(d_mlp, dtype=bool)\n",
        "\n",
        "    for i in range(d_mlp):\n",
        "        y_neuron_z_full = z_mlp_full[:, i]\n",
        "        if np.std(y_neuron_z_full) < 1e-9:\n",
        "            learned_poly_coeffs[i, :] = np.nan # Mark as NaN if no variance\n",
        "            continue\n",
        "\n",
        "        # Fit model using pre-calculated polynomial features\n",
        "        poly_reg_z_full = LinearRegression(fit_intercept=False) # Bias term is feature '1'\n",
        "        poly_reg_z_full.fit(X_poly_features, y_neuron_z_full)\n",
        "        learned_poly_coeffs[i, :] = poly_reg_z_full.coef_\n",
        "        neuron_fit_successful[i] = True\n",
        "\n",
        "    print(\"Coefficient extraction complete.\")\n",
        "\n",
        "    # Filter out neurons where fit failed (NaNs)\n",
        "    valid_coeffs = learned_poly_coeffs[neuron_fit_successful]\n",
        "    valid_correlations = neuron_sum_correlations[neuron_fit_successful]\n",
        "    num_valid_neurons = valid_coeffs.shape[0]\n",
        "    print(f\"Analyzing {num_valid_neurons} neurons with successful fits.\")\n",
        "\n",
        "    # --- Overall Analysis: Mean and Std of Coefficients ---\n",
        "    mean_coeffs = np.nanmean(valid_coeffs, axis=0)\n",
        "    std_coeffs = np.nanstd(valid_coeffs, axis=0)\n",
        "\n",
        "    print(\"\\n--- Overall Mean Polynomial Coefficients (across all valid neurons) ---\")\n",
        "    coeffs_df_overall = pd.DataFrame({\n",
        "        'Feature': poly_feature_names,\n",
        "        'MeanCoeff': mean_coeffs,\n",
        "        'StdCoeff': std_coeffs,\n",
        "        'MeanAbsCoeff': np.nanmean(np.abs(valid_coeffs), axis=0) # Look at avg magnitude too\n",
        "    }).sort_values('MeanAbsCoeff', ascending=False)\n",
        "    print(coeffs_df_overall.to_string())\n",
        "\n",
        "    # --- Visualize Overall Mean Coefficients ---\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    indices = np.arange(num_poly_feats)\n",
        "    plt.bar(indices, mean_coeffs, yerr=std_coeffs, capsize=5, alpha=0.7, ecolor='gray')\n",
        "    plt.axhline(0, color='black', linewidth=0.5)\n",
        "    plt.xticks(indices, poly_feature_names, rotation=45, ha='right')\n",
        "    plt.ylabel(\"Coefficient Value\")\n",
        "    plt.title(\"Mean Polynomial Coefficients (+/- std dev) across All Neurons\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # --- Group Analysis: Based on Correlation ---\n",
        "    print(\"\\n--- Mean Coefficients by Neuron Correlation Group ---\")\n",
        "    # Define correlation groups (adjust thresholds as needed based on your data)\n",
        "    groups = {\n",
        "        \"High Pos (>=0.9)\": valid_correlations >= 0.9,\n",
        "        \"Mid Pos (0.5-0.9)\": (valid_correlations >= 0.5) & (valid_correlations < 0.9),\n",
        "         #\"Low Mag (<0.5)\": np.abs(valid_correlations) < 0.5, # Optional\n",
        "        \"Mid Neg (-0.9 - -0.5)\": (valid_correlations < -0.5) & (valid_correlations >= -0.9),\n",
        "        \"High Neg (<=-0.9)\": valid_correlations < -0.9,\n",
        "    }\n",
        "\n",
        "    group_coeffs_mean = {}\n",
        "    for name, mask in groups.items():\n",
        "        group_mask_in_valid = mask[neuron_fit_successful] # Align mask with valid coeffs\n",
        "        if np.sum(group_mask_in_valid) > 0:\n",
        "            group_coeffs_mean[name] = np.nanmean(valid_coeffs[group_mask_in_valid], axis=0)\n",
        "        else:\n",
        "             group_coeffs_mean[name] = np.full(num_poly_feats, np.nan) # Handle empty groups\n",
        "\n",
        "    group_coeffs_df = pd.DataFrame(group_coeffs_mean, index=poly_feature_names)\n",
        "    print(group_coeffs_df.to_string(float_format=\"%.4f\"))\n",
        "\n",
        "    # --- Visualize Group Coefficients (Heatmap) ---\n",
        "    # Transpose for better heatmap layout (features as rows, groups as columns)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(group_coeffs_df.T, annot=True, fmt=\".3f\", cmap=\"coolwarm\", center=0,\n",
        "                linewidths=.5, linecolor='lightgray')\n",
        "    plt.title(\"Mean Polynomial Coefficients for Different Neuron Correlation Groups\")\n",
        "    plt.xlabel(\"Polynomial Feature\")\n",
        "    plt.ylabel(\"Neuron Correlation Group\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "uAZ2i_40KS7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dominance of Linear PC1: The linear terms for PC1 (p1a, p1b) have the largest mean absolute coefficients (after the bias '1'). This confirms PC1 (related to the sum a+b) is a primary driver.\n",
        "\n",
        "Symmetry (a vs b): The coefficients for p1a and p1b are extremely similar (both overall and within groups). Likewise for p2a vs p2b, p1a^2 vs p1b^2, p2a^2 vs p2b^2, etc. This strongly suggests the MLP treats a and b symmetrically, consistent with the addition task. The computation depends primarily on combinations like PC1(a)+PC1(b) and PC2(a)+PC2(b), not the individual values in an asymmetric way.\n",
        "\n",
        "PC1 Sign Flipping: This is a major finding from the group analysis. Neurons with positive correlation (High Pos, Mid Pos) have large negative coefficients for the linear PC1 terms (p1a, p1b). Neurons with negative correlation (Mid Neg) have large positive coefficients for these terms. This seems counter-intuitive at first glance, but likely interacts with the bias and other terms. It clearly shows opposing strategies for neurons that correlate positively vs negatively with the sum.\n",
        "\n",
        "PC2 Linear Term Consistency: The coefficients for the linear PC2 terms (p2a, p2b) are consistently negative across the groups where they are significant. This suggests PC2 linearly contributes a suppressing effect on average to the pre-activation, regardless of the neuron's correlation group.\n",
        "Interaction Terms: The PC1 cross-term (p1a p1b) has a noticeable negative coefficient overall. The PC1xPC2 cross-terms (p1a p2a, p1a p2b, etc.) have smaller average magnitudes but do show sign differences between positive and negative correlation groups, suggesting PC2's influence might be partially exerted through interaction with the PC1 signal.\n",
        "\n",
        "Quadratic Terms Minor (on average): Pure quadratic terms (p1a^2, p2a^2, etc.) have the smallest coefficients on average, suggesting they play a less dominant role than linear or interaction terms in the average recipe.\n",
        "\n",
        "\n",
        "standard deviations are high compared to means, so i presume the learned representations are not the same for all neurons? can we perhaps isolate this a bit more? perhaps look at e.g. only the high-correlation group, or even just at one or two high-correlation neurons"
      ],
      "metadata": {
        "id": "Dc90XpcKKvZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out neurons where fit failed (NaNs)\n",
        "valid_coeffs = learned_poly_coeffs[neuron_fit_successful]\n",
        "valid_correlations = neuron_sum_correlations[neuron_fit_successful]\n",
        "valid_indices = np.where(neuron_fit_successful)[0] # Original indices of valid neurons\n",
        "num_valid_neurons = valid_coeffs.shape[0]\n",
        "\n",
        "# --- 1. Analyze High Positive Correlation Group (>0.9) ---\n",
        "high_pos_mask_valid = valid_correlations >= 0.9\n",
        "num_high_pos = np.sum(high_pos_mask_valid)\n",
        "\n",
        "if num_high_pos > 0:\n",
        "    coeffs_high_pos = valid_coeffs[high_pos_mask_valid]\n",
        "    mean_coeffs_high_pos = np.mean(coeffs_high_pos, axis=0)\n",
        "    std_coeffs_high_pos = np.std(coeffs_high_pos, axis=0)\n",
        "\n",
        "    print(f\"\\n--- Mean Polynomial Coefficients for High Positive Correlation Neurons (>{num_high_pos}) ---\")\n",
        "    coeffs_df_high_pos = pd.DataFrame({\n",
        "        'Feature': poly_feature_names,\n",
        "        'MeanCoeff': mean_coeffs_high_pos,\n",
        "        'StdCoeff': std_coeffs_high_pos,\n",
        "        'MeanAbsCoeff': np.mean(np.abs(coeffs_high_pos), axis=0)\n",
        "    }).sort_values('MeanAbsCoeff', ascending=False)\n",
        "    print(coeffs_df_high_pos.to_string())\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    indices = np.arange(len(poly_feature_names))\n",
        "    plt.bar(indices, mean_coeffs_high_pos, yerr=std_coeffs_high_pos, capsize=5, alpha=0.7, ecolor='gray')\n",
        "    plt.axhline(0, color='black', linewidth=0.5)\n",
        "    plt.xticks(indices, poly_feature_names, rotation=45, ha='right')\n",
        "    plt.ylabel(\"Coefficient Value\")\n",
        "    plt.title(f\"Mean Polynomial Coefficients (+/- std dev) for High Positive Neurons (N={num_high_pos})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo neurons found in the High Positive Correlation group (>=0.9).\")\n",
        "\n",
        "\n",
        "# --- 2. Analyze Specific Neurons (e.g., 357, 184) ---\n",
        "# Check if these indices exist and had successful fits\n",
        "neuron_ids_to_check = []\n",
        "if 'top_contributing_indices' in locals() and len(top_contributing_indices) >= 2:\n",
        "     neuron_ids_to_check = top_contributing_indices[:2] # Use the previously found top 2\n",
        "else:\n",
        "     # Fallback if top contributors aren't available, find highest correlation ones\n",
        "     print(\"Warning: top_contributing_indices not found, using highest correlation neurons instead.\")\n",
        "     if num_valid_neurons >= 2:\n",
        "        sorted_corr_indices_valid = np.argsort(np.abs(valid_correlations))[::-1]\n",
        "        neuron_ids_to_check = valid_indices[sorted_corr_indices_valid[:2]] # Get original indices\n",
        "\n",
        "\n",
        "if len(neuron_ids_to_check) > 0:\n",
        "    print(f\"\\n--- Coefficients for Specific Neurons: {neuron_ids_to_check} ---\")\n",
        "    specific_coeffs_dict = {}\n",
        "    for neuron_idx in neuron_ids_to_check:\n",
        "         # Find the index within the valid_coeffs array\n",
        "         valid_idx_location = np.where(valid_indices == neuron_idx)[0]\n",
        "         if len(valid_idx_location) > 0:\n",
        "             valid_idx = valid_idx_location[0]\n",
        "             specific_coeffs_dict[f'Neuron {neuron_idx}'] = learned_poly_coeffs[valid_idx]\n",
        "         else:\n",
        "              print(f\"Warning: Neuron {neuron_idx} did not have a successful fit or is not in valid indices.\")\n",
        "\n",
        "    if specific_coeffs_dict:\n",
        "         specific_coeffs_df = pd.DataFrame(specific_coeffs_dict, index=poly_feature_names)\n",
        "         print(specific_coeffs_df.to_string(float_format=\"%.4f\"))\n",
        "\n",
        "         # Visualize comparison\n",
        "         specific_coeffs_df.plot(kind='bar', figsize=(14, 7), alpha=0.7)\n",
        "         plt.title(f\"Polynomial Coefficients for Specific Neurons\")\n",
        "         plt.ylabel(\"Coefficient Value\")\n",
        "         plt.xticks(rotation=45, ha='right')\n",
        "         plt.axhline(0, color='black', linewidth=0.5)\n",
        "         plt.tight_layout()\n",
        "         plt.show()\n"
      ],
      "metadata": {
        "id": "Tm3w29FmLHIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that the quadratic terms in our previous degree-2 polynomial fit had relatively small average coefficients, we now test if the pre-ReLU activations (z_mlp_full) can be accurately modelled using only a linear combination of the PC1 and PC2 features from inputs a and b. We will fit z_mlp_full  w1*PC1(a) + w2*PC1(b) + w3*PC2(a) + w4*PC2(b) + Bias for each neuron. Comparing the R-squared values from this linear fit to the previous quadratic fit will reveal the specific contribution of the quadratic terms to the pre-activation computation.\n"
      ],
      "metadata": {
        "id": "ejHlzhpSMaU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFitting z_mlp_full vs. LINEAR PC1 & PC2 Features...\")\n",
        "\n",
        "# --- Check if data is available ---\n",
        "if 'X_features' not in locals() or X_features is None or X_features.shape[1] != 4:\n",
        "    print(\"Error: 'X_features' with 4 columns [p1a, p1b, p2a, p2b] not found.\")\n",
        "elif 'z_mlp_full' not in locals() or z_mlp_full is None:\n",
        "    print(\"Error: 'z_mlp_full' not found.\")\n",
        "else:\n",
        "    num_samples, num_neurons = z_mlp_full.shape\n",
        "    assert num_neurons == d_mlp\n",
        "\n",
        "    # Array to store R-squared scores\n",
        "    neuron_r2_linear_only_fit = np.zeros(d_mlp)\n",
        "\n",
        "    # --- Fit Models for Each Neuron ---\n",
        "    print(f\"Fitting linear model for {d_mlp} neurons...\")\n",
        "    for i in range(d_mlp):\n",
        "        y_neuron_z_full = z_mlp_full[:, i]\n",
        "        if np.std(y_neuron_z_full) < 1e-9:\n",
        "            neuron_r2_linear_only_fit[i] = np.nan\n",
        "            continue\n",
        "\n",
        "        # Fit model: z_full_i  w1*p1a + w2*p1b + w3*p2a + w4*p2b + Bias\n",
        "        lin_reg_z_full = LinearRegression(fit_intercept=True) # Fit intercept separately now\n",
        "        lin_reg_z_full.fit(X_features, y_neuron_z_full)\n",
        "        neuron_r2_linear_only_fit[i] = lin_reg_z_full.score(X_features, y_neuron_z_full)\n",
        "\n",
        "    print(\"Fitting complete.\")\n",
        "\n",
        "    # --- Analysis ---\n",
        "    valid_r2_scores = neuron_r2_linear_only_fit[~np.isnan(neuron_r2_linear_only_fit)]\n",
        "    valid_r2_poly_prev = neuron_r2_scores_z_full_fit[~np.isnan(neuron_r2_linear_only_fit)] # Align masks\n",
        "\n",
        "    if len(valid_r2_scores) > 0:\n",
        "        print(\"\\n--- Analysis of R-squared (Fit z_mlp_full vs Linear(PC1, PC2 features)) ---\")\n",
        "        print(f\"Mean R:   {np.mean(valid_r2_scores):.4f}\")\n",
        "        print(f\"Median R: {np.median(valid_r2_scores):.4f}\")\n",
        "        print(f\"Min R:    {np.min(valid_r2_scores):.4f}\")\n",
        "        print(f\"Max R:    {np.max(valid_r2_scores):.4f}\")\n",
        "        print(f\"Neurons with R > 0.95: {np.sum(valid_r2_scores > 0.95)} / {len(valid_r2_scores)}\")\n",
        "        print(f\"Neurons with R > 0.99: {np.sum(valid_r2_scores > 0.99)} / {len(valid_r2_scores)}\")\n",
        "\n",
        "        # Compare to Poly2 fit\n",
        "        if 'neuron_r2_scores_z_full_fit' in locals():\n",
        "            avg_r2_drop = np.mean(valid_r2_poly_prev - valid_r2_scores)\n",
        "            median_r2_drop = np.median(valid_r2_poly_prev - valid_r2_scores)\n",
        "            print(f\"\\n--- Comparison to Poly2 Fit ---\")\n",
        "            print(f\"Mean R drop by removing quadratics:   {avg_r2_drop:.5f}\")\n",
        "            print(f\"Median R drop by removing quadratics: {median_r2_drop:.5f}\")\n",
        "        else:\n",
        "            print(\"\\nCannot compare to Poly2 fit R-squared (previous results not found).\")\n",
        "\n",
        "        # Visualize\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(valid_r2_scores, bins=50, kde=False)\n",
        "        plt.title(\"R Distribution: Fit of Pre-ReLU (z_full) vs. Linear(PC1, PC2 Features)\")\n",
        "        plt.xlabel(\"R-squared Score per Neuron\")\n",
        "        plt.ylabel(\"Number of Neurons\")\n",
        "        plt.grid(axis='y', alpha=0.5)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Could not calculate R-squared for any neurons.\")\n"
      ],
      "metadata": {
        "id": "2HGQvHjzMc-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ok, it gets significantly worse! so we cannot analyze just linearly. however, maybe we can enforce that the coefficients be the same for a and b, and simplify in that way?"
      ],
      "metadata": {
        "id": "MhQ8vv3yMqhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFitting z_mlp_full vs. Symmetric Quadratic Features derived from PC1 & PC2...\")\n",
        "\n",
        "# --- Check if data is available ---\n",
        "if 'p1a' not in locals() or 'p1b' not in locals() or 'p2a' not in locals() or 'p2b' not in locals():\n",
        "     print(\"Error: Base PC features (p1a, p1b, p2a, p2b) not found.\")\n",
        "elif 'z_mlp_full' not in locals() or z_mlp_full is None:\n",
        "    print(\"Error: 'z_mlp_full' not found.\")\n",
        "else:\n",
        "    # --- 1. Create Symmetric Quadratic Features ---\n",
        "    p1_sum = p1a + p1b\n",
        "    p2_sum = p2a + p2b\n",
        "    p1_sum_sq = p1_sum**2\n",
        "    p2_sum_sq = p2_sum**2\n",
        "    p1_p2_interaction = p1_sum * p2_sum\n",
        "\n",
        "    # Feature matrix: [p1_sum, p2_sum, p1_sum^2, p2_sum^2, p1_sum*p2_sum]\n",
        "    X_symm_quad_features = np.stack(\n",
        "        (p1_sum, p2_sum, p1_sum_sq, p2_sum_sq, p1_p2_interaction),\n",
        "        axis=-1\n",
        "    ) # Shape: [num_samples, 5]\n",
        "    symm_quad_feature_names = ['p1_sum', 'p2_sum', 'p1_sum_sq', 'p2_sum_sq', 'p1_sum*p2_sum']\n",
        "\n",
        "    num_samples, num_neurons = z_mlp_full.shape\n",
        "    assert num_neurons == d_mlp\n",
        "\n",
        "    # Array to store R-squared scores\n",
        "    neuron_r2_symm_quad_fit = np.zeros(d_mlp)\n",
        "    # Optional: Store coefficients if needed\n",
        "    # neuron_coeffs_symm_quad = np.zeros((d_mlp, 5))\n",
        "    # neuron_intercepts_symm_quad = np.zeros(d_mlp)\n",
        "    neuron_fit_successful_symm_quad = np.zeros(d_mlp, dtype=bool)\n",
        "\n",
        "    # --- Fit Models for Each Neuron ---\n",
        "    print(f\"Fitting symmetric quadratic model for {d_mlp} neurons...\")\n",
        "    for i in range(d_mlp):\n",
        "        y_neuron_z_full = z_mlp_full[:, i]\n",
        "        if np.std(y_neuron_z_full) < 1e-9:\n",
        "            neuron_r2_symm_quad_fit[i] = np.nan\n",
        "            continue\n",
        "\n",
        "        # Fit model: z_full_i  w1*p1_sum + w2*p2_sum + w3*p1_sum^2 + w4*p2_sum^2 + w5*(p1s*p2s) + Bias\n",
        "        symm_quad_reg_z_full = LinearRegression(fit_intercept=True)\n",
        "        symm_quad_reg_z_full.fit(X_symm_quad_features, y_neuron_z_full)\n",
        "        neuron_r2_symm_quad_fit[i] = symm_quad_reg_z_full.score(X_symm_quad_features, y_neuron_z_full)\n",
        "        # Optional: Store coeffs/intercept\n",
        "        # neuron_coeffs_symm_quad[i, :] = symm_quad_reg_z_full.coef_\n",
        "        # neuron_intercepts_symm_quad[i] = symm_quad_reg_z_full.intercept_\n",
        "        neuron_fit_successful_symm_quad[i] = True\n",
        "\n",
        "    print(\"Fitting complete.\")\n",
        "\n",
        "    # --- Analysis ---\n",
        "    valid_r2_symm_quad = neuron_r2_symm_quad_fit[neuron_fit_successful_symm_quad]\n",
        "    # Align masks for comparison\n",
        "    valid_r2_poly_prev = neuron_r2_scores_z_full_fit[neuron_fit_successful_symm_quad]\n",
        "\n",
        "    if len(valid_r2_symm_quad) > 0:\n",
        "        print(\"\\n--- Analysis of R-squared (Fit z_mlp_full vs Symmetric Quadratic Features) ---\")\n",
        "        print(f\"Mean R:   {np.mean(valid_r2_symm_quad):.4f}\")\n",
        "        print(f\"Median R: {np.median(valid_r2_symm_quad):.4f}\")\n",
        "        print(f\"Min R:    {np.min(valid_r2_symm_quad):.4f}\")\n",
        "        print(f\"Max R:    {np.max(valid_r2_symm_quad):.4f}\")\n",
        "        print(f\"Neurons with R > 0.95: {np.sum(valid_r2_symm_quad > 0.95)} / {len(valid_r2_symm_quad)}\")\n",
        "        print(f\"Neurons with R > 0.99: {np.sum(valid_r2_symm_quad > 0.99)} / {len(valid_r2_symm_quad)}\")\n",
        "        print(f\"Neurons with R > 0.999: {np.sum(valid_r2_symm_quad > 0.999)} / {len(valid_r2_symm_quad)}\")\n",
        "\n",
        "\n",
        "        # Compare to full Poly2 fit\n",
        "        if 'neuron_r2_scores_z_full_fit' in locals():\n",
        "            avg_r2_diff = np.mean(valid_r2_poly_prev - valid_r2_symm_quad)\n",
        "            median_r2_diff = np.median(valid_r2_poly_prev - valid_r2_symm_quad)\n",
        "            print(f\"\\n--- Comparison to Full Poly2 Fit ---\")\n",
        "            print(f\"Mean R Difference (Full Poly2 - Symmetric Quad):   {avg_r2_diff:.6f}\")\n",
        "            print(f\"Median R Difference (Full Poly2 - Symmetric Quad): {median_r2_diff:.6f}\")\n",
        "            # Expect this difference to be very small if symmetry simplification holds\n",
        "        else:\n",
        "            print(\"\\nCannot compare to Full Poly2 fit R-squared (previous results not found).\")\n",
        "\n",
        "        # Visualize R^2 comparison\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.scatter(valid_r2_poly_prev, valid_r2_symm_quad, alpha=0.3)\n",
        "        min_both = min(np.min(valid_r2_poly_prev), np.min(valid_r2_symm_quad))\n",
        "        plt.plot([min_both, 1], [min_both, 1], color='red', linestyle='--', label='y=x')\n",
        "        plt.xlabel(\"R using Full Poly2 (14 features + bias)\")\n",
        "        plt.ylabel(\"R using Symmetric Quad (5 features + bias)\")\n",
        "        plt.title(\"R Comparison: Full vs Symmetric Quadratic Features\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        # Adjust limits slightly if needed, but should be close to 1\n",
        "        plt.xlim(min_both - 0.001, 1.001)\n",
        "        plt.ylim(min_both - 0.001, 1.001)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Could not calculate R-squared for any neurons.\")\n"
      ],
      "metadata": {
        "id": "6WCVoR2LMrRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fit gets worse, but not by a lot. The vast majority of neurons still have R^2 above 0.95. I would tentatively interpret this as \"the model learns that the function it has to implement is symmetric, but does not get all of the way there\". I am not sure where to go from here. Perhaps we can recap what we do know:\n",
        "\n",
        "1. The embedding is quadratic. There is one linear term and one quadratic term. It's also highly symmetric, effectively the same for a and b.\n",
        "2. Both of those features of the embedding are used by the MLP going into its final layer. Further, the input to the ReLU is a quadratic function of these two features.\n",
        "3. This function is again very symmetric, with the coefficients for the a and b parts of the embedding being almost exactly the same. Enforcing this in the fit drops fit quality only slightly.\n",
        "4. ReLU happens. We understand the mechanics of this.\n",
        "5. Then we get to the unembedding matrix. This is a lot less clear to me. It seems to add neuron activations in a non-trivial way. I am not sure what it does, exactly.\n",
        "\n",
        "Plan: Identify the top ~20-30 active neurons (a_mlp[i] > threshold) for input (0,0) and for input (4,4). Then, plot the DFT magnitudes of their corresponding W_L[:, i] columns side-by-side or overlaid. Do the active neurons for (0,0) tend to have W_L columns rich in the specific frequencies needed for the target=0 shape? Do the active neurons for (4,4) have W_L columns rich in the frequencies needed for the target=8 shape? This compares the \"ingredients\" (W_L structure) selected by the network (a_mlp activation) for different tasks.\n",
        "\n",
        "We hypothesize that the network produces the correct logit shape (peaking at a+b) by activating a specific set of MLP neurons (a_mlp). Each active neuron i then contributes its corresponding \"output basis vector\" (W_L[:, i]) scaled by its activation. Our Fourier analysis suggests these W_L columns represent combinations of low-frequency components. To test this further, we will now identify the neurons most highly activated (a_mlp post-ReLU) for two different target sums: 0 (input (0,0)) and 8 (input (4,4)). We will then compute the average Discrete Fourier Transform (DFT) magnitude spectrum of the W_L columns corresponding to the top active neurons for each case. If the hypothesis holds, we expect the average spectrum for the neurons active for input (0,0) to potentially differ (in the balance of frequencies 1, 2, 3...) from the average spectrum for neurons active for input (4,4), reflecting the different frequency mixes needed to synthesize peaks at 0 versus 8.\n",
        "\n"
      ],
      "metadata": {
        "id": "IJZfwTC3M_fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "k_top_neurons = 30 # How many most active neurons to consider\n",
        "input_pair_1 = (0, 0)\n",
        "input_pair_2 = (4, 4)\n",
        "target_sum_1 = input_pair_1[0] + input_pair_1[1]\n",
        "target_sum_2 = input_pair_2[0] + input_pair_2[1]\n",
        "\n",
        "# --- Check if data is available ---\n",
        "data_missing = False\n",
        "if 'a_mlp_full' not in locals() or a_mlp_full is None:\n",
        "    print(\"Error: 'a_mlp_full' not found.\")\n",
        "    data_missing = True\n",
        "if 'W_L_np' not in locals() or W_L_np is None:\n",
        "    print(\"Error: 'W_L_np' not found.\")\n",
        "    data_missing = True\n",
        "if 'original_a_values' not in locals() or 'original_b_values' not in locals():\n",
        "    print(\"Error: Input value arrays not found.\")\n",
        "    data_missing = True\n",
        "\n",
        "if not data_missing:\n",
        "    # --- Find indices for target input pairs ---\n",
        "    try:\n",
        "        idx_1 = np.where((original_a_values == input_pair_1[0]) & (original_b_values == input_pair_1[1]))[0][0]\n",
        "        print(f\"Found index for input {input_pair_1}: {idx_1}\")\n",
        "    except IndexError:\n",
        "        print(f\"Error: Input pair {input_pair_1} not found in the test set data.\")\n",
        "        idx_1 = None\n",
        "\n",
        "    try:\n",
        "        idx_2 = np.where((original_a_values == input_pair_2[0]) & (original_b_values == input_pair_2[1]))[0][0]\n",
        "        print(f\"Found index for input {input_pair_2}: {idx_2}\")\n",
        "    except IndexError:\n",
        "        print(f\"Error: Input pair {input_pair_2} not found in the test set data.\")\n",
        "        idx_2 = None\n",
        "\n",
        "    if idx_1 is not None and idx_2 is not None:\n",
        "        # --- Get activation vectors ---\n",
        "        a_full_1 = a_mlp_full[idx_1] # Activations for input 1\n",
        "        a_full_2 = a_mlp_full[idx_2] # Activations for input 2\n",
        "\n",
        "        # --- Identify top active neurons ---\n",
        "        top_indices_1 = np.argsort(a_full_1)[::-1][:k_top_neurons]\n",
        "        top_indices_2 = np.argsort(a_full_2)[::-1][:k_top_neurons]\n",
        "        print(f\"\\nTop {k_top_neurons} active neuron indices for {input_pair_1}: {top_indices_1[:5]}...\")\n",
        "        print(f\"Top {k_top_neurons} active neuron indices for {input_pair_2}: {top_indices_2[:5]}...\")\n",
        "\n",
        "        # --- Helper function to get average DFT magnitude spectrum ---\n",
        "        def get_avg_dft_mags(neuron_indices, W_L_matrix, num_outputs_fft):\n",
        "            all_mags = []\n",
        "            if len(neuron_indices) == 0:\n",
        "                return np.zeros(num_outputs_fft // 2 + 1) # Return zeros if no neurons\n",
        "\n",
        "            for i in neuron_indices:\n",
        "                w_col = W_L_matrix[:, i]\n",
        "                fft_coeffs = np.fft.rfft(w_col)\n",
        "                all_mags.append(np.abs(fft_coeffs))\n",
        "            # Average across the selected neurons\n",
        "            return np.mean(np.array(all_mags), axis=0)\n",
        "\n",
        "        # --- Calculate average spectra ---\n",
        "        print(f\"\\nCalculating average W_L DFT spectra for top {k_top_neurons} active neurons...\")\n",
        "        avg_spectrum_1 = get_avg_dft_mags(top_indices_1, W_L_np, num_outputs)\n",
        "        avg_spectrum_2 = get_avg_dft_mags(top_indices_2, W_L_np, num_outputs)\n",
        "        num_freqs = len(avg_spectrum_1)\n",
        "        frequencies = np.fft.rfftfreq(num_outputs) # For reference\n",
        "\n",
        "        # --- Analyze Top Frequencies in Average Spectra ---\n",
        "        print(f\"\\nAnalysis for Input {input_pair_1} (Target {target_sum_1}):\")\n",
        "        sorted_freq_indices_1 = np.argsort(avg_spectrum_1)[::-1]\n",
        "        for i in range(min(5, num_freqs)):\n",
        "            f_idx = sorted_freq_indices_1[i]\n",
        "            print(f\"  Rank {i+1}: Freq Index {f_idx} (Mag: {avg_spectrum_1[f_idx]:.4f})\")\n",
        "\n",
        "        print(f\"\\nAnalysis for Input {input_pair_2} (Target {target_sum_2}):\")\n",
        "        sorted_freq_indices_2 = np.argsort(avg_spectrum_2)[::-1]\n",
        "        for i in range(min(5, num_freqs)):\n",
        "            f_idx = sorted_freq_indices_2[i]\n",
        "            print(f\"  Rank {i+1}: Freq Index {f_idx} (Mag: {avg_spectrum_2[f_idx]:.4f})\")\n",
        "\n",
        "\n",
        "        # --- Visualize the average spectra ---\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        bar_width = 0.35\n",
        "        indices = np.arange(num_freqs)\n",
        "\n",
        "        plt.bar(indices - bar_width/2, avg_spectrum_1, bar_width, label=f'Avg Spectrum for Input {input_pair_1} (Top {k_top_neurons})')\n",
        "        plt.bar(indices + bar_width/2, avg_spectrum_2, bar_width, label=f'Avg Spectrum for Input {input_pair_2} (Top {k_top_neurons})')\n",
        "\n",
        "        plt.xlabel(\"Frequency Index (k)\")\n",
        "        plt.ylabel(\"Average Magnitude of Fourier Coefficient\")\n",
        "        plt.title(f\"Average W_L Column DFT Magnitudes for Top {k_top_neurons} Active Neurons\")\n",
        "        plt.xticks(indices) # Label all frequency indices\n",
        "        plt.legend()\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"\\nSkipping analysis as one or both input pairs were not found.\")\n",
        "else:\n",
        "    print(\"\\nSkipping analysis due to missing activation or W_L data.\")\n"
      ],
      "metadata": {
        "id": "knVIxaqiOrQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAnalyzing W_L Column DFTs for Neurons Flipping State for Input (0,0)...\")\n",
        "\n",
        "# --- Helper function to get average DFT magnitude spectrum ---\n",
        "def get_avg_dft_mags(neuron_indices, W_L_matrix, num_outputs_fft):\n",
        "    all_mags = []\n",
        "    if len(neuron_indices) == 0:\n",
        "        return np.zeros(num_outputs_fft // 2 + 1), [] # Return zeros and empty list\n",
        "\n",
        "    for i in neuron_indices:\n",
        "        w_col = W_L_matrix[:, i]\n",
        "        fft_coeffs = np.fft.rfft(w_col)\n",
        "        all_mags.append(np.abs(fft_coeffs))\n",
        "    # Average across the selected neurons\n",
        "    if not all_mags: # Should not happen if len > 0, but safe check\n",
        "        return np.zeros(num_outputs_fft // 2 + 1), []\n",
        "    return np.mean(np.array(all_mags), axis=0), np.array(all_mags)\n",
        "\n",
        "# --- Calculate average spectra ---\n",
        "avg_spectrum_oto, _ = get_avg_dft_mags(off_to_on_indices, W_L_np, num_outputs)\n",
        "avg_spectrum_otf, all_mags_otf = get_avg_dft_mags(on_to_off_indices, W_L_np, num_outputs)\n",
        "num_freqs = len(avg_spectrum_oto)\n",
        "\n",
        "# --- Visualize the average spectra ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "bar_width = 0.35\n",
        "indices = np.arange(num_freqs)\n",
        "\n",
        "# Plot average spectrum for OFF->ON neurons (if any)\n",
        "if len(off_to_on_indices) > 0:\n",
        "    plt.bar(indices - bar_width/2, avg_spectrum_oto, bar_width, label=f'Avg Spectrum OFF->ON (N={len(off_to_on_indices)})', color='cyan', alpha=0.8)\n",
        "\n",
        "# Plot average spectrum for ON->OFF neurons (if any)\n",
        "if len(on_to_off_indices) > 0:\n",
        "    plt.bar(indices + bar_width/2, avg_spectrum_otf, bar_width, label=f'Avg Spectrum ON->OFF (N={len(on_to_off_indices)})', color='magenta', alpha=0.8)\n",
        "\n",
        "    # Optional: Add individual lines for ON->OFF neurons to show variance\n",
        "    # for i, mags in enumerate(all_mags_otf):\n",
        "    #     plt.plot(indices + bar_width/2, mags, color='magenta', alpha=0.1) # Plot individual spectra lightly\n",
        "\n",
        "\n",
        "plt.xlabel(\"Frequency Index (k)\")\n",
        "plt.ylabel(\"Average Magnitude of Fourier Coefficient\")\n",
        "plt.title(\"Average W_L Column DFT Magnitudes for Flipping Neurons [Input (0,0)]\")\n",
        "plt.xticks(indices)\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# --- Analyze Top Frequencies ---\n",
        "print(\"\\n--- Top Frequencies in Average Spectra for Flipping Neurons ---\")\n",
        "if len(off_to_on_indices) > 0:\n",
        "    print(f\"OFF -> ON (N={len(off_to_on_indices)}):\")\n",
        "    sorted_freq_indices_oto = np.argsort(avg_spectrum_oto)[::-1]\n",
        "    for i in range(min(5, num_freqs)):\n",
        "        f_idx = sorted_freq_indices_oto[i]\n",
        "        print(f\"  Rank {i+1}: Freq Index {f_idx} (Mag: {avg_spectrum_oto[f_idx]:.4f})\")\n",
        "else:\n",
        "    print(\"OFF -> ON: None\")\n",
        "\n",
        "if len(on_to_off_indices) > 0:\n",
        "     print(f\"\\nON -> OFF (N={len(on_to_off_indices)}):\")\n",
        "     sorted_freq_indices_otf = np.argsort(avg_spectrum_otf)[::-1]\n",
        "     for i in range(min(5, num_freqs)):\n",
        "         f_idx = sorted_freq_indices_otf[i]\n",
        "         print(f\"  Rank {i+1}: Freq Index {f_idx} (Mag: {avg_spectrum_otf[f_idx]:.4f})\")\n",
        "else:\n",
        "    print(\"ON -> OFF: None\")\n"
      ],
      "metadata": {
        "id": "pagYBr0QP6yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Fourier analysis hasn't been as illuminating as we had hoped - many frequencies (6+) are required to get correct predictions, which given the simplicity of the problem is a lot. Let's turn back to the polynomial stuff we've observed.\n",
        "\n",
        "Having established that the pre-ReLU activations (z_mlp) are extremely well modelled by a degree-2 polynomial of the input PC1 and PC2 features (R > 0.99 for all neurons), we will now test if this understanding is sufficient to reconstruct the model's final output. We will simulate the rest of the forward pass: first, we calculate the predicted z_mlp for all inputs using the learned polynomial coefficients for each neuron. Second, we apply the standard ReLU activation function to these predicted pre-activations. Third, we multiply the resulting post-ReLU activations by the effective unembedding matrix W_L. Finally, we compare these simulated logits to the actual logits produced by the full model. A close match would validate our complete algorithmic description: Logits  W_L @ ReLU(Poly2(PC1, PC2)).\n"
      ],
      "metadata": {
        "id": "rwUjwLdYRpBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSimulating Logits using Poly2(PC1, PC2) fit for pre-ReLU activations...\")\n",
        "\n",
        "# --- Check if data is available ---\n",
        "if 'learned_poly_coeffs' not in locals() or learned_poly_coeffs is None:\n",
        "     print(\"Error: Learned polynomial coefficients ('learned_poly_coeffs') not found.\")\n",
        "elif 'X_poly_features' not in locals() or X_poly_features is None:\n",
        "    print(\"Error: Polynomial features ('X_poly_features') not found.\")\n",
        "elif 'W_L_np' not in locals() or W_L_np is None:\n",
        "    print(\"Error: 'W_L_np' not found.\")\n",
        "elif 'original_logits_pred_pos' not in locals() or original_logits_pred_pos is None:\n",
        "     print(\"Error: Actual original logits ('original_logits_pred_pos') not found.\")\n",
        "else:\n",
        "    # --- 1. Predict z_mlp using polynomial fits ---\n",
        "    # We have X_poly_features (shape [num_samples, num_poly_feats])\n",
        "    # We have learned_poly_coeffs (shape [d_mlp, num_poly_feats])\n",
        "    # learned_poly_coeffs[i, :] contains the coefficients INCLUDING the bias term coefficient\n",
        "\n",
        "    z_mlp_pred = np.zeros_like(z_mlp_full) # Initialize prediction array\n",
        "    fit_indices = np.where(neuron_fit_successful)[0]\n",
        "\n",
        "    print(f\"Predicting z_mlp for {len(fit_indices)} successfully fitted neurons...\")\n",
        "\n",
        "    # The prediction is simply the dot product of features and coefficients\n",
        "    # (num_samples, num_poly_feats) @ (num_poly_feats, d_mlp) -> (num_samples, d_mlp)\n",
        "    # Transpose learned_poly_coeffs\n",
        "    z_mlp_pred = X_poly_features @ learned_poly_coeffs[fit_indices, :].T # Use only valid coeffs\n",
        "\n",
        "    # If some neurons failed the fit, their columns in z_mlp_pred will be zero.\n",
        "    # This is usually acceptable, or could be filled with mean if needed.\n",
        "    print(\"Predicted z_mlp shape:\", z_mlp_pred.shape)\n",
        "\n",
        "    # --- 2. Apply ReLU ---\n",
        "    a_mlp_pred = np.maximum(0, z_mlp_pred)\n",
        "    print(\"Applied ReLU. Predicted a_mlp shape:\", a_mlp_pred.shape)\n",
        "\n",
        "    # --- 3. Apply W_L ---\n",
        "    # Logits[sample, k] = Sum_i W_L[k, i] * a_mlp_pred[sample, i]\n",
        "    # Matrix multiplication: (num_samples, d_mlp) @ (d_mlp, num_outputs) -> (num_samples, num_outputs)\n",
        "    # Need to transpose W_L_np\n",
        "    logits_pred = a_mlp_pred @ W_L_np.T\n",
        "    print(\"Calculated predicted logits. Shape:\", logits_pred.shape)\n",
        "\n",
        "    # --- 4. Compare Predicted vs Actual Logits ---\n",
        "    print(\"\\n--- Comparing Predicted Logits vs Actual Logits ---\")\n",
        "\n",
        "    # Overall Metrics\n",
        "    mse_logits = mean_squared_error(original_logits_pred_pos.flatten(), logits_pred.flatten())\n",
        "    # Calculate R-squared manually for the whole set of logits\n",
        "    mean_actual_logits = np.mean(original_logits_pred_pos)\n",
        "    total_variance = np.sum((original_logits_pred_pos - mean_actual_logits)**2)\n",
        "    residual_variance = np.sum((original_logits_pred_pos - logits_pred)**2)\n",
        "    r2_logits = 1 - (residual_variance / total_variance) if total_variance > 1e-9 else np.nan\n",
        "\n",
        "    print(f\"Overall Mean Squared Error between predicted and actual logits: {mse_logits:.4f}\")\n",
        "    print(f\"Overall R-squared between predicted and actual logits:        {r2_logits:.4f}\")\n",
        "\n",
        "    # Per-Sample Comparison (Example: input (0,0))\n",
        "    if 'idx_00' in locals() and idx_00 is not None:\n",
        "        logits_pred_00 = logits_pred[idx_00]\n",
        "        logits_actual_00 = original_logits_pred_pos[idx_00]\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        output_indices = np.arange(num_outputs)\n",
        "        plt.plot(output_indices, logits_actual_00, marker='o', linestyle='-', color='blue', label='Actual Logits (0,0)')\n",
        "        plt.plot(output_indices, logits_pred_00, marker='x', linestyle='--', color='red', label=f'Predicted Logits (R={r2_logits:.3f})')\n",
        "        plt.title(\"Comparison of Actual vs Predicted Logits for Input (0,0)\")\n",
        "        plt.xlabel(\"Output Logit Index (k)\")\n",
        "        plt.ylabel(\"Logit Value\")\n",
        "        plt.xticks(output_indices)\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        plt.show()\n",
        "\n",
        "    # Scatter plot of predicted vs actual (for all logits)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.scatter(original_logits_pred_pos.flatten(), logits_pred.flatten(), alpha=0.1)\n",
        "    # Add y=x line\n",
        "    min_val = min(np.min(original_logits_pred_pos), np.min(logits_pred)) - 1\n",
        "    max_val = max(np.max(original_logits_pred_pos), np.max(logits_pred)) + 1\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='y=x')\n",
        "    plt.xlabel(\"Actual Logit Value\")\n",
        "    plt.ylabel(\"Predicted Logit Value (from Poly2 -> ReLU -> W_L)\")\n",
        "    plt.title(f\"Predicted vs Actual Logits (Overall R = {r2_logits:.4f})\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.axis('equal') # Ensure axes have same scale\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "WLfK4kjOR21X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This confirms the observation that the model learns to use the 2 PCs, and use them quadratically. Replacing our fit and running the rest of the model (ReLU + W_L multiplication) keeps performance effectively the same. Let us now look into whether the post-ReLU also looks like a quadratic function of the 2 PCs. We expect not, because of the piecewise nature of the ReLU, which is hard to approximate with just a quadratic."
      ],
      "metadata": {
        "id": "NKnrvYLgU7Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFitting FULL model POST-ReLU activations (a_mlp_full) vs. Poly2(PC1, PC2 features)...\")\n",
        "\n",
        "# --- Check if data is available ---\n",
        "if 'X_poly_features' not in locals() or X_poly_features is None:\n",
        "    print(\"Error: 'X_poly_features' not found.\")\n",
        "elif 'a_mlp_full' not in locals() or a_mlp_full is None:\n",
        "    print(\"Error: 'a_mlp_full' not found.\")\n",
        "else:\n",
        "    num_samples, num_poly_feats = X_poly_features.shape\n",
        "    num_samples_a, num_neurons = a_mlp_full.shape\n",
        "    assert num_neurons == d_mlp\n",
        "    assert num_samples == num_samples_a\n",
        "\n",
        "    # Array to store R-squared scores\n",
        "    neuron_r2_scores_a_full_fit = np.zeros(d_mlp)\n",
        "    neuron_fit_successful_a = np.zeros(d_mlp, dtype=bool)\n",
        "\n",
        "    # --- Fit Models for Each Neuron ---\n",
        "    print(f\"Fitting polynomial model for {d_mlp} neurons...\")\n",
        "    for i in range(d_mlp):\n",
        "        # Target variable: full POST-ReLU activation for neuron i\n",
        "        y_neuron_a_full = a_mlp_full[:, i]\n",
        "\n",
        "        # Check for constant activation (e.g., always zero)\n",
        "        if np.std(y_neuron_a_full) < 1e-9:\n",
        "            neuron_r2_scores_a_full_fit[i] = np.nan\n",
        "            continue\n",
        "\n",
        "        # Fit model: a_full_i  Poly2(PC1, PC2 features)\n",
        "        poly_reg_a_full = LinearRegression(fit_intercept=False) # Bias included in X_poly_features\n",
        "        poly_reg_a_full.fit(X_poly_features, y_neuron_a_full)\n",
        "        neuron_r2_scores_a_full_fit[i] = poly_reg_a_full.score(X_poly_features, y_neuron_a_full)\n",
        "        neuron_fit_successful_a[i] = True\n",
        "\n",
        "    print(\"Fitting complete.\")\n",
        "\n",
        "    # --- Analysis ---\n",
        "    valid_r2_scores_a = neuron_r2_scores_a_full_fit[neuron_fit_successful_a]\n",
        "    valid_r2_pre_prev = neuron_r2_scores_z_full_fit[neuron_fit_successful_a] # Align masks\n",
        "\n",
        "    if len(valid_r2_scores_a) > 0:\n",
        "        print(\"\\n--- Analysis of R-squared (Fit a_mlp_full vs Poly2(PC1, PC2 features)) ---\")\n",
        "        print(f\"Mean R:   {np.mean(valid_r2_scores_a):.4f}\")\n",
        "        print(f\"Median R: {np.median(valid_r2_scores_a):.4f}\")\n",
        "        print(f\"Min R:    {np.min(valid_r2_scores_a):.4f}\")\n",
        "        print(f\"Max R:    {np.max(valid_r2_scores_a):.4f}\")\n",
        "        print(f\"Neurons with R > 0.95: {np.sum(valid_r2_scores_a > 0.95)} / {len(valid_r2_scores_a)}\")\n",
        "        print(f\"Neurons with R > 0.99: {np.sum(valid_r2_scores_a > 0.99)} / {len(valid_r2_scores_a)}\")\n",
        "\n",
        "        # --- Visualize R-squared Distribution ---\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(valid_r2_scores_a, bins=50, kde=False)\n",
        "        plt.title(\"R Distribution: Fit of Post-ReLU Activations (a_full) vs. Poly2(PC1, PC2 Features)\")\n",
        "        plt.xlabel(\"R-squared Score per Neuron\")\n",
        "        plt.ylabel(\"Number of Neurons\")\n",
        "        plt.grid(axis='y', alpha=0.5)\n",
        "        plt.show()\n",
        "\n",
        "        # --- Compare Pre vs Post R-squared using Poly2 fit ---\n",
        "        if 'neuron_r2_scores_z_full_fit' in locals():\n",
        "            avg_r2_drop_poly = np.mean(valid_r2_pre_prev - valid_r2_scores_a)\n",
        "            median_r2_drop_poly = np.median(valid_r2_pre_prev - valid_r2_scores_a)\n",
        "            print(f\"\\n--- Comparison to Pre-ReLU Poly2 Fit ---\")\n",
        "            print(f\"Mean R drop (Poly2(z_full) - Poly2(a_full)):   {avg_r2_drop_poly:.4f}\")\n",
        "            print(f\"Median R drop (Poly2(z_full) - Poly2(a_full)): {median_r2_drop_poly:.4f}\")\n",
        "\n",
        "            # Visualize the drop\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.scatter(valid_r2_pre_prev, valid_r2_scores_a, alpha=0.3)\n",
        "            min_both = min(np.min(valid_r2_pre_prev), np.min(valid_r2_scores_a))\n",
        "            max_both = max(np.max(valid_r2_pre_prev), np.max(valid_r2_scores_a))\n",
        "            plt.plot([min_both, max_both], [min_both, max_both], color='red', linestyle='--', label='y=x')\n",
        "            plt.xlabel(\"R for Pre-ReLU (z_full) vs Poly2(PCs)\")\n",
        "            plt.ylabel(\"R for Post-ReLU (a_full) vs Poly2(PCs)\")\n",
        "            plt.title(\"Impact of ReLU on Poly2 Fit Accuracy\")\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.legend()\n",
        "            plt.xlim(min_both - 0.01, max_both + 0.01)\n",
        "            plt.ylim(min_both - 0.01, max_both + 0.01)\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"\\nCannot compare to Pre-ReLU Poly2 fit R-squared (previous results not found).\")\n",
        "    else:\n",
        "        print(\"Could not calculate R-squared for any neurons.\")\n"
      ],
      "metadata": {
        "id": "EDyOBa6SU55n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "it actually does not get so much worse! we still have the vast majority of neurons with ~0.95 quality fit. this is somewhat unexpected. shall we try to use this fit as input to the W_L and compare the resulting simulated logits with the actual model logits? we now expect this to work decently well. and if it does, we can fully focus on the unembedding."
      ],
      "metadata": {
        "id": "Ot_UcGH2VlzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSimulating Logits using Poly2(PCs) fit for POST-ReLU activations (a_mlp_full)...\")\n",
        "\n",
        "# --- Check if data is available ---\n",
        "# (Add checks similar to previous blocks if running standalone)\n",
        "if 'a_mlp_full' not in locals() or 'X_poly_features' not in locals() or 'W_L_np' not in locals() or 'original_logits_pred_pos' not in locals():\n",
        "     print(\"Error: Missing necessary data. Please run previous steps.\")\n",
        "else:\n",
        "    # --- 1. Fit a_mlp_full and get coefficients ---\n",
        "    # (Re-running fit just to ensure coeffs are available here, can be optimized)\n",
        "    print(f\"Fitting polynomial model for {d_mlp} neurons to get coefficients for a_mlp_full...\")\n",
        "    learned_poly_coeffs_a = np.zeros((d_mlp, X_poly_features.shape[1]))\n",
        "    neuron_fit_successful_a = np.zeros(d_mlp, dtype=bool) # Recreate mask locally if needed\n",
        "\n",
        "    for i in range(d_mlp):\n",
        "        y_neuron_a_full = a_mlp_full[:, i]\n",
        "        if np.std(y_neuron_a_full) < 1e-9:\n",
        "            learned_poly_coeffs_a[i, :] = np.nan\n",
        "            continue\n",
        "        poly_reg_a_full = LinearRegression(fit_intercept=False)\n",
        "        poly_reg_a_full.fit(X_poly_features, y_neuron_a_full)\n",
        "        learned_poly_coeffs_a[i, :] = poly_reg_a_full.coef_\n",
        "        neuron_fit_successful_a[i] = True\n",
        "\n",
        "    valid_a_fit_indices = np.where(neuron_fit_successful_a)[0]\n",
        "    print(f\"Predicting a_mlp for {len(valid_a_fit_indices)} successfully fitted neurons...\")\n",
        "\n",
        "    # --- 2. Predict a_mlp using polynomial fits ---\n",
        "    # Prediction is dot product: (num_samples, num_poly_feats) @ (num_poly_feats, d_mlp)\n",
        "    a_mlp_pred_from_poly = X_poly_features @ learned_poly_coeffs_a[valid_a_fit_indices, :].T\n",
        "    # Create full d_mlp array, filling non-fitted with zeros\n",
        "    a_mlp_pred_full_dim = np.zeros((num_samples, d_mlp))\n",
        "    a_mlp_pred_full_dim[:, valid_a_fit_indices] = a_mlp_pred_from_poly\n",
        "\n",
        "    print(\"Predicted a_mlp shape:\", a_mlp_pred_full_dim.shape)\n",
        "\n",
        "    # --- 3. Apply W_L ---\n",
        "    # Logits[sample, k] = Sum_i W_L[k, i] * a_mlp_pred[sample, i]\n",
        "    logits_pred_from_poly_a = a_mlp_pred_full_dim @ W_L_np.T\n",
        "    print(\"Calculated predicted logits (from poly fit of a_mlp). Shape:\", logits_pred_from_poly_a.shape)\n",
        "\n",
        "    # --- 4. Compare Predicted vs Actual Logits ---\n",
        "    print(\"\\n--- Comparing Logits [Actual vs. Poly2(a_mlp) Sim vs. ReLU(Poly2(z_mlp)) Sim] ---\")\n",
        "\n",
        "    # Overall Metrics for Poly2(a_mlp) simulation\n",
        "    mse_logits_poly_a = mean_squared_error(original_logits_pred_pos.flatten(), logits_pred_from_poly_a.flatten())\n",
        "    mean_actual_logits = np.mean(original_logits_pred_pos)\n",
        "    total_variance = np.sum((original_logits_pred_pos - mean_actual_logits)**2)\n",
        "    residual_variance_poly_a = np.sum((original_logits_pred_pos - logits_pred_from_poly_a)**2)\n",
        "    r2_logits_poly_a = 1 - (residual_variance_poly_a / total_variance) if total_variance > 1e-9 else np.nan\n",
        "\n",
        "    print(f\"Simulation using Poly2(a_mlp) fit:\")\n",
        "    print(f\"  Overall Mean Squared Error: {mse_logits_poly_a:.4f}\")\n",
        "    print(f\"  Overall R-squared:          {r2_logits_poly_a:.4f}\")\n",
        "\n",
        "    # Compare to previous simulation using ReLU(Poly2(z_mlp))\n",
        "    if 'logits_pred' in locals() and logits_pred is not None:\n",
        "         mse_logits_relu = mean_squared_error(original_logits_pred_pos.flatten(), logits_pred.flatten())\n",
        "         r2_logits_relu = 1 - (np.sum((original_logits_pred_pos - logits_pred)**2) / total_variance) if total_variance > 1e-9 else np.nan\n",
        "         print(f\"\\nSimulation using ReLU(Poly2(z_mlp)) fit (Previous):\")\n",
        "         print(f\"  Overall Mean Squared Error: {mse_logits_relu:.4f}\")\n",
        "         print(f\"  Overall R-squared:          {r2_logits_relu:.4f}\")\n",
        "         print(f\"\\nDifference in R (ReLU Sim - Poly(a_mlp) Sim): {r2_logits_relu - r2_logits_poly_a:.6f}\")\n",
        "    else:\n",
        "        print(\"\\nPrevious simulation results ('logits_pred') not available for comparison.\")\n",
        "\n",
        "\n",
        "# --- Per-Sample Comparison (Example: input (0,0)) ---\n",
        "if 'idx_00' in locals() and idx_00 is not None \\\n",
        "   and 'original_logits_pred_pos' in locals() \\\n",
        "   and 'logits_pred_from_poly_a' in locals(): # Check the result of the current simulation\n",
        "\n",
        "    logits_pred_poly_a_00 = logits_pred_from_poly_a[idx_00]\n",
        "    logits_actual_00 = original_logits_pred_pos[idx_00]\n",
        "\n",
        "    # --- Fix for AttributeError ---\n",
        "    logits_pred_relu_00 = None # Default to None\n",
        "    if 'logits_pred' in locals() and isinstance(logits_pred, np.ndarray):\n",
        "        # Check if the index is valid for the array\n",
        "        if idx_00 < logits_pred.shape[0]:\n",
        "             logits_pred_relu_00 = logits_pred[idx_00]\n",
        "        else:\n",
        "             print(f\"Warning: idx_00 ({idx_00}) is out of bounds for logits_pred shape {logits_pred.shape}\")\n",
        "    # --- End Fix ---\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    output_indices = np.arange(num_outputs)\n",
        "    plt.plot(output_indices, logits_actual_00, marker='o', linestyle='-', color='blue', label='Actual Logits (0,0)')\n",
        "    plt.plot(output_indices, logits_pred_poly_a_00, marker='s', linestyle=':', color='green', alpha=0.8, label=f'Pred from Poly(a_mlp) (R={r2_logits_poly_a:.3f})')\n",
        "\n",
        "    # Plot the previous simulation only if available\n",
        "    if logits_pred_relu_00 is not None and 'r2_logits_relu' in locals():\n",
        "         plt.plot(output_indices, logits_pred_relu_00, marker='x', linestyle='--', color='red', alpha=0.8, label=f'Pred from ReLU(Poly(z_mlp)) (R={r2_logits_relu:.3f})')\n",
        "    elif 'r2_logits_relu' in locals():\n",
        "        # Add legend entry even if plotting fails, indicating R^2\n",
        "         plt.plot([], [], marker='x', linestyle='--', color='red', alpha=0.8, label=f'Pred from ReLU(Poly(z_mlp)) (R={r2_logits_relu:.3f})')\n",
        "\n",
        "\n",
        "    plt.title(\"Comparison of Actual vs Predicted Logits for Input (0,0)\")\n",
        "    plt.xlabel(\"Output Logit Index (k)\")\n",
        "    plt.ylabel(\"Logit Value\")\n",
        "    plt.xticks(output_indices)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.show()\n",
        "\n",
        "# --- Scatter plot of predicted vs actual (for Poly2(a_mlp) fit) ---\n",
        "if 'logits_pred_from_poly_a' in locals() and 'original_logits_pred_pos' in locals() and 'r2_logits_poly_a' in locals():\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.scatter(original_logits_pred_pos.flatten(), logits_pred_from_poly_a.flatten(), alpha=0.1)\n",
        "    min_val = min(np.min(original_logits_pred_pos), np.min(logits_pred_from_poly_a)) - 1\n",
        "    max_val = max(np.max(original_logits_pred_pos), np.max(logits_pred_from_poly_a)) + 1\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='y=x')\n",
        "    plt.xlabel(\"Actual Logit Value\")\n",
        "    plt.ylabel(\"Predicted Logit Value (from Poly2(a_mlp) -> W_L)\")\n",
        "    plt.title(f\"Predicted vs Actual Logits using Poly(a_mlp) (Overall R = {r2_logits_poly_a:.4f})\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.axis('equal')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping plots due to missing data.\")\n"
      ],
      "metadata": {
        "id": "uB1lQEzIV83_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the shape of the function is effectively identical! it follows the same shape, just slightly deviating from the original a tad bit more than the previous simulation. the relative ordering is also kept i think, so i suspect that this would result in the same predictions.\n",
        "\n",
        "it appears that the main mystery is in the unembedding. how can we best analyze this? can we do PCA like we did for the embedding?\n",
        "\n",
        "To further understand the structure embedded within the unembedding matrix W_L ([num_outputs, d_mlp]), we will now analyze its columns using Principal Component Analysis (PCA). Each column W_L[:, i] represents the pattern of influence neuron i has across all output logits (0 to num_outputs-1). By treating these d_mlp columns as our data points, PCA will identify the principal axes of variation, revealing the most common \"shapes\" or \"basis vectors\" that these neuron influence patterns are composed of. Based on our Fourier analysis, we hypothesize that the first few principal components will correspond to low-frequency sine and cosine waves, confirming that these periodic patterns are the dominant structural elements learned within W_L.\n"
      ],
      "metadata": {
        "id": "YwsSJ45iW1ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity # For comparing vectors\n",
        "\n",
        "# --- Assume previous steps ran and these variables exist ---\n",
        "# W_L_np: Numpy array [num_outputs, d_mlp] (NumPy version of W_L matrix)\n",
        "# num_outputs: Number of output logits (e.g., 19)\n",
        "# d_mlp: Dimension of MLP\n",
        "# --- End assumptions ---\n",
        "\n",
        "# --- Check if data is available ---\n",
        "if 'W_L_np' not in locals() or W_L_np is None:\n",
        "    print(\"Error: 'W_L_np' not found.\")\n",
        "else:\n",
        "    print(f\"\\nPerforming PCA on the COLUMNS of W_L (shape: {W_L_np.shape})...\")\n",
        "    print(\"Treating each neuron's output weight vector as a sample.\")\n",
        "\n",
        "    # --- 1. Prepare Data for PCA ---\n",
        "    # We want to find patterns across the output dimension (length num_outputs).\n",
        "    # Each neuron's column is a sample. PCA expects samples as rows. So, TRANSPOSE W_L.\n",
        "    X_pca = W_L_np.T # Shape: [d_mlp, num_outputs]\n",
        "\n",
        "    # --- 2. Center the Data ---\n",
        "    # Subtract the mean neuron output vector\n",
        "    mean_vector = np.mean(X_pca, axis=0) # Mean across neurons\n",
        "    X_centered = X_pca - mean_vector\n",
        "\n",
        "    # --- 3. Perform SVD ---\n",
        "    # U: [d_mlp, K], S: [K,], Vt: [K, num_outputs] where K = min(d_mlp, num_outputs)\n",
        "    # Need full_matrices=False (or True, Vt shape depends on it, but components are the same)\n",
        "    try:\n",
        "         U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
        "         print(f\"SVD complete. Vt shape: {Vt.shape}\") # Should be [~19, 19]\n",
        "         num_components = Vt.shape[0] # Number of components found\n",
        "    except Exception as e:\n",
        "         print(f\"SVD failed: {e}\")\n",
        "         Vt = None\n",
        "\n",
        "    if Vt is not None:\n",
        "        # --- 4. Principal Components & Variance Explained ---\n",
        "        # Principal components are the ROWS of Vt\n",
        "        principal_components = Vt # Shape: [num_components, num_outputs]\n",
        "\n",
        "        # Explained variance ratio\n",
        "        explained_variance = S**2 / np.sum(S**2)\n",
        "        cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "        print(f\"\\nVariance Explained by Principal Components:\")\n",
        "        for i in range(min(10, num_components)): # Show top 10\n",
        "            print(f\"  PC {i+1}: {explained_variance[i]:.4f} (Cumulative: {cumulative_variance[i]:.4f})\")\n",
        "\n",
        "        # --- 5. Scree Plot (Explained Variance) ---\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(range(1, num_components + 1), explained_variance, alpha=0.7, align='center', label='Individual explained variance')\n",
        "        plt.step(range(1, num_components + 1), cumulative_variance, where='mid', label='Cumulative explained variance')\n",
        "        plt.ylabel('Explained variance ratio')\n",
        "        plt.xlabel('Principal component index')\n",
        "        plt.title('PCA Explained Variance for W_L Columns')\n",
        "        plt.xticks(range(1, num_components + 1))\n",
        "        plt.legend(loc='best')\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.ylim(0, 1.1) # Set y-axis limit\n",
        "        # Add threshold line, e.g., at 95% variance\n",
        "        try:\n",
        "            idx_95 = np.where(cumulative_variance >= 0.95)[0][0]\n",
        "            plt.axvline(idx_95 + 1, color='red', linestyle=':', label=f'{(idx_95+1)} Components for 95% Var')\n",
        "            plt.legend(loc='best') # Update legend\n",
        "        except IndexError:\n",
        "            pass # If 95% is never reached\n",
        "        plt.show()\n",
        "\n",
        "        # --- 6. Visualize Top Principal Components vs Fourier Basis ---\n",
        "        num_pcs_to_plot = 4 # Plot PC1 to PC4\n",
        "        output_indices_k = np.arange(num_outputs)\n",
        "\n",
        "        fig, axes = plt.subplots(num_pcs_to_plot, 1, figsize=(12, 3 * num_pcs_to_plot), sharex=True)\n",
        "        if num_pcs_to_plot == 1: axes = [axes] # Make iterable\n",
        "\n",
        "        P = num_outputs # Period for Fourier basis\n",
        "\n",
        "        for i in range(num_pcs_to_plot):\n",
        "            ax = axes[i]\n",
        "            pc_vector = principal_components[i, :]\n",
        "\n",
        "            # Plot the PC vector\n",
        "            ax.plot(output_indices_k, pc_vector, marker='o', linestyle='-', label=f'PC {i+1} (Var: {explained_variance[i]:.3f})')\n",
        "\n",
        "            # Compare with low-frequency cosine/sine\n",
        "            freq_to_compare = (i // 2) + 1 # Compare PC1/2 with freq 1, PC3/4 with freq 2, etc.\n",
        "            cos_basis = np.cos(2 * np.pi * freq_to_compare * output_indices_k / P)\n",
        "            sin_basis = np.sin(2 * np.pi * freq_to_compare * output_indices_k / P)\n",
        "\n",
        "            # Normalize basis vectors for comparison scaling (optional)\n",
        "            # cos_basis /= np.linalg.norm(cos_basis)\n",
        "            # sin_basis /= np.linalg.norm(sin_basis)\n",
        "            # pc_vector_norm = pc_vector / np.linalg.norm(pc_vector) # Normalize PC too if normalizing basis\n",
        "\n",
        "            # Calculate Cosine Similarity\n",
        "            sim_cos = cosine_similarity(pc_vector.reshape(1, -1), cos_basis.reshape(1, -1))[0, 0]\n",
        "            sim_sin = cosine_similarity(pc_vector.reshape(1, -1), sin_basis.reshape(1, -1))[0, 0]\n",
        "\n",
        "            # Plot the most similar basis vector (scaled arbitrarily for visualization)\n",
        "            if abs(sim_cos) > abs(sim_sin):\n",
        "                 scale = np.dot(pc_vector, cos_basis) / np.dot(cos_basis, cos_basis) # Project PC onto basis\n",
        "                 ax.plot(output_indices_k, scale * cos_basis, marker='.', linestyle=':', color='red', label=f'Scaled Cos(k={freq_to_compare}), Sim={sim_cos:.2f}')\n",
        "            else:\n",
        "                 scale = np.dot(pc_vector, sin_basis) / np.dot(sin_basis, sin_basis) # Project PC onto basis\n",
        "                 ax.plot(output_indices_k, scale * sin_basis, marker='.', linestyle=':', color='green', label=f'Scaled Sin(k={freq_to_compare}), Sim={sim_sin:.2f}')\n",
        "\n",
        "\n",
        "            ax.set_ylabel(\"Component Value\")\n",
        "            ax.set_title(f\"Principal Component {i+1} of W_L Columns\")\n",
        "            ax.grid(True, linestyle='--', alpha=0.6)\n",
        "            ax.axhline(0, color='black', linewidth=0.5)\n",
        "            ax.legend()\n",
        "\n",
        "        axes[-1].set_xlabel(\"Output Logit Index (k)\")\n",
        "        fig.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "P6IvPRegYDex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ultra Low-Dimensional Structure: The fact that just TWO principal components explain 98.3% of the variance in the W_L columns is remarkable. It means that despite having 512 neurons, the patterns they use to influence the 20 output logits are overwhelmingly constrained to a simple 2D subspace. All 512 neuron output vectors (W_L[:, i]) are effectively just different linear combinations of these two primary basis vectors (PC1 and PC2 of W_L).\n",
        "\n",
        "\n",
        "Simplification: This drastically simplifies the unembedding step conceptually. Instead of thinking about 512 complex columns, we know the readout is dominated by combining just two fundamental output shapes (likely sin(k=1) and cos(k=1)).\n",
        "\n",
        "Revisiting the Whole Algorithm Fit:\n",
        "Your idea to fit the entire model output (final logits) using the PC1/PC2 features of the input makes perfect sense now. We tried fitting z_mlp and a_mlp to Poly2(InputPCs). Given the W_L structure is also ultra-simple (dominated by 2 components likely related to output frequency k=1), perhaps the entire chain InputPCs -> Poly2 -> ReLU -> W_L(Dominated by 2 PCs) can be effectively approximated by a relatively simple function mapping input PCs to output logits.\n",
        "\n",
        "Hypothesis: The final Logits[k] for a given k can be approximated by a function of the input features PC1(a), PC1(b), PC2(a), PC2(b), potentially a polynomial, leveraging the low-dimensional structure we found everywhere.\n",
        "\n",
        "Before checking this hypothesis, we'll check in on the two principal components of the unembedding matrix.\n"
      ],
      "metadata": {
        "id": "trIBGX2YZGdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Visualize Top 2 Principal Components ---\n",
        "num_pcs_to_plot = 2\n",
        "output_indices_k = np.arange(num_outputs)\n",
        "\n",
        "print(f\"\\nVisualizing the shapes of PC1 and PC2 of W_L columns:\")\n",
        "\n",
        "fig, axes = plt.subplots(num_pcs_to_plot, 1, figsize=(12, 3 * num_pcs_to_plot), sharex=True)\n",
        "if num_pcs_to_plot == 1: axes = [axes] # Make iterable\n",
        "\n",
        "for i in range(num_pcs_to_plot):\n",
        "    ax = axes[i]\n",
        "    pc_vector = principal_components[i, :] # Get the i-th PC (PC{i+1})\n",
        "\n",
        "    # Plot the PC vector shape\n",
        "    ax.plot(output_indices_k, pc_vector, marker='o', linestyle='-', label=f'PC {i+1} (Var: {explained_variance[i]:.3f})')\n",
        "\n",
        "    ax.set_ylabel(\"Component Value\")\n",
        "    ax.set_title(f\"Principal Component {i+1} of W_L Columns (Neuron Output Patterns)\")\n",
        "    ax.grid(True, linestyle='--', alpha=0.6)\n",
        "    ax.axhline(0, color='black', linewidth=0.5)\n",
        "    ax.legend()\n",
        "\n",
        "axes[-1].set_xlabel(\"Output Logit Index (k)\")\n",
        "axes[-1].set_xticks(output_indices_k) # Show all logit indices if not too many\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rMQsd6rUCtM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFitting FINAL LOGITS vs. Polynomial(PC1, PC2 features)...\")\n",
        "\n",
        "# --- Check if data is available ---\n",
        "if 'X_poly_features' not in locals() or X_poly_features is None:\n",
        "    print(\"Error: 'X_poly_features' not found.\")\n",
        "elif 'original_logits_pred_pos' not in locals() or original_logits_pred_pos is None:\n",
        "    print(\"Error: 'original_logits_pred_pos' not found.\")\n",
        "else:\n",
        "    num_samples, num_poly_feats = X_poly_features.shape\n",
        "    num_samples_l, num_logits = original_logits_pred_pos.shape\n",
        "    assert num_logits == num_outputs\n",
        "    assert num_samples == num_samples_l\n",
        "\n",
        "    # Array to store R-squared scores for each logit's fit\n",
        "    logit_r2_scores_fit = np.zeros(num_outputs)\n",
        "    logit_fit_successful = np.zeros(num_outputs, dtype=bool)\n",
        "\n",
        "    # --- Fit Models for Each Logit ---\n",
        "    print(f\"Fitting polynomial model for {num_outputs} output logits...\")\n",
        "    for k in range(num_outputs):\n",
        "        # Target variable: actual logit value for index k\n",
        "        y_logit_k = original_logits_pred_pos[:, k]\n",
        "\n",
        "        # Check for constant value (might happen for far-off logits)\n",
        "        if np.std(y_logit_k) < 1e-9:\n",
        "            logit_r2_scores_fit[k] = np.nan\n",
        "            continue\n",
        "\n",
        "        # Fit model: Logit[k]  Poly2(p1a, p1b, p2a, p2b)\n",
        "        poly_reg_logit_k = LinearRegression(fit_intercept=False) # Bias included in X_poly_features\n",
        "        poly_reg_logit_k.fit(X_poly_features, y_logit_k)\n",
        "        logit_r2_scores_fit[k] = poly_reg_logit_k.score(X_poly_features, y_logit_k)\n",
        "        logit_fit_successful[k] = True\n",
        "\n",
        "    print(\"Fitting complete.\")\n",
        "\n",
        "    # --- Analysis ---\n",
        "    valid_r2_scores = logit_r2_scores_fit[logit_fit_successful]\n",
        "\n",
        "    if len(valid_r2_scores) > 0:\n",
        "        print(\"\\n--- Analysis of R-squared (Fit Logit[k] vs Poly2(PC1, PC2 features)) ---\")\n",
        "        print(f\"Mean R across logits:   {np.nanmean(logit_r2_scores_fit):.4f}\")\n",
        "        print(f\"Median R across logits: {np.nanmedian(logit_r2_scores_fit):.4f}\")\n",
        "        print(f\"Min R across logits:    {np.nanmin(logit_r2_scores_fit):.4f}\")\n",
        "        print(f\"Max R across logits:    {np.max(logit_r2_scores_fit):.4f}\")\n",
        "        print(f\"Logits with R > 0.95: {np.sum(logit_r2_scores_fit[logit_fit_successful] > 0.95)} / {len(valid_r2_scores)}\")\n",
        "        print(f\"Logits with R > 0.99: {np.sum(logit_r2_scores_fit[logit_fit_successful] > 0.99)} / {len(valid_r2_scores)}\")\n",
        "\n",
        "        # --- Visualize R-squared per Logit ---\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        output_indices = np.arange(num_outputs)\n",
        "        plt.bar(output_indices[logit_fit_successful], valid_r2_scores, width=0.9)\n",
        "        plt.title(\"R Score per Output Logit (Fit Logit[k] vs. Poly2(PC1, PC2 Features))\")\n",
        "        plt.xlabel(\"Output Logit Index (k)\")\n",
        "        plt.ylabel(\"R-squared Score\")\n",
        "        plt.xticks(output_indices)\n",
        "        plt.ylim(min(0.9, np.nanmin(logit_r2_scores_fit)-0.01), 1.01) # Zoom in on high values\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Could not calculate R-squared for any logits.\")\n"
      ],
      "metadata": {
        "id": "Cf18arV5ZPhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "\n",
        "Quadratic Sufficiency: Yes, you can absolutely conclude that a degree-2 polynomial function of the input PC1/PC2 features is sufficient to approximate the final output logits with very high accuracy (R > 0.96 for all logits, mean ~0.975).\n",
        "\n",
        "Effective Algorithm IS Simple: While the mechanistic steps involve ReLU and a Fourier-structured W_L, the net functional mapping from the dominant input features to the final logit values behaves as if it were a relatively simple quadratic function. The complexities introduced by ReLU and the specific W_L structure largely cancel out or combine in a way that is well-approximated polynomially.\n",
        "\n",
        "Let's find the logit k with the highest R (~0.9906) from your results and look at its specific polynomial coefficients. This shows the \"recipe\" used to calculate the score for that specific potential sum.\n"
      ],
      "metadata": {
        "id": "mauVi2q3Z2UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAnalyzing coefficients for the logit with the highest R-squared fit...\")\n",
        "\n",
        "# --- Find best-fit logit ---\n",
        "# Handle potential NaNs if any fits failed\n",
        "valid_logit_indices = np.where(~np.isnan(logit_r2_scores_fit))[0]\n",
        "if len(valid_logit_indices) == 0:\n",
        "    print(\"Error: No valid R-squared scores found for any logit.\")\n",
        "else:\n",
        "    best_logit_k_idx_in_valid = np.argmax(logit_r2_scores_fit[valid_logit_indices])\n",
        "    best_logit_k = valid_logit_indices[best_logit_k_idx_in_valid] # Get the actual logit index k\n",
        "    best_r2 = logit_r2_scores_fit[best_logit_k]\n",
        "    print(f\"Best fit found for Logit {best_logit_k} with R = {best_r2:.4f}\")\n",
        "\n",
        "    # --- Re-fit just this logit to get coefficients ---\n",
        "    y_best_logit = original_logits_pred_pos[:, best_logit_k]\n",
        "\n",
        "    # Fit model: Logit[k_best]  Poly2(p1a, p1b, p2a, p2b)\n",
        "    poly_reg_best_logit = LinearRegression(fit_intercept=False) # Bias included in X_poly_features\n",
        "    poly_reg_best_logit.fit(X_poly_features, y_best_logit)\n",
        "\n",
        "    # --- Get and display coefficients ---\n",
        "    coeffs_best_logit = poly_reg_best_logit.coef_\n",
        "    poly_feature_names = poly.get_feature_names_out(['p1a', 'p1b', 'p2a', 'p2b'])\n",
        "\n",
        "    print(f\"\\n--- Coefficients for Logit {best_logit_k} (R={best_r2:.4f}) ---\")\n",
        "    coeffs_df_best = pd.DataFrame({\n",
        "        'Feature': poly_feature_names,\n",
        "        'Coefficient': coeffs_best_logit\n",
        "    })\n",
        "    # Add absolute value for sorting by importance\n",
        "    coeffs_df_best['AbsCoefficient'] = np.abs(coeffs_df_best['Coefficient'])\n",
        "    print(coeffs_df_best.sort_values('AbsCoefficient', ascending=False).to_string(index=False, float_format=\"%.4f\"))\n"
      ],
      "metadata": {
        "id": "illU6S-SZ9r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found that the final logits are extremely well predicted by a degree-2 polynomial of the input PC features. To simplify the interpretation of the specific polynomial learned for the best-fitting logit (k=2, R0.9906), we will leverage the observed symmetry between a and b features. We refit Logit[2] using only symmetric quadratic features: p1_sum = PC1(a)+PC1(b), p2_sum = PC2(a)+PC2(b), p1_sum^2, p2_sum^2, and p1_sum * p2_sum. Analyzing the coefficients of this simpler 5-feature model will make the core computational relationship between the combined input features and the logit value clearer, while also quantifying any small loss in accuracy due to enforcing perfect symmetry.\n"
      ],
      "metadata": {
        "id": "G9PgCdm2abi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nFitting Logit {best_logit_k} vs. SYMMETRIC Quadratic Features...\")\n",
        "\n",
        "# --- Check if data is available ---\n",
        "if 'X_symm_quad_features' not in locals() or X_symm_quad_features is None:\n",
        "     print(\"Error: Symmetric quadratic features ('X_symm_quad_features') not found.\")\n",
        "elif 'original_logits_pred_pos' not in locals() or original_logits_pred_pos is None:\n",
        "     print(\"Error: Actual original logits ('original_logits_pred_pos') not found.\")\n",
        "elif 'best_logit_k' not in locals() or best_logit_k is None:\n",
        "     print(\"Error: Index of best logit ('best_logit_k') not found.\")\n",
        "else:\n",
        "    # --- Prepare Target Data ---\n",
        "    y_best_logit = original_logits_pred_pos[:, best_logit_k]\n",
        "\n",
        "    # --- Fit Symmetric Model ---\n",
        "    # Fit model: Logit[k_best]  SymmQuadPoly(p1s, p2s) + Bias\n",
        "    symm_quad_reg_logit_k = LinearRegression(fit_intercept=True)\n",
        "    symm_quad_reg_logit_k.fit(X_symm_quad_features, y_best_logit)\n",
        "\n",
        "    # --- Get Results ---\n",
        "    coeffs_symm_quad = symm_quad_reg_logit_k.coef_\n",
        "    intercept_symm_quad = symm_quad_reg_logit_k.intercept_\n",
        "    r2_symm_quad = symm_quad_reg_logit_k.score(X_symm_quad_features, y_best_logit)\n",
        "\n",
        "    print(f\"\\n--- Coefficients for Logit {best_logit_k} using Symmetric Features (R={r2_symm_quad:.4f}) ---\")\n",
        "    print(f\"Intercept: {intercept_symm_quad:.4f}\")\n",
        "    coeffs_df_symm = pd.DataFrame({\n",
        "        'Feature': symm_quad_feature_names,\n",
        "        'Coefficient': coeffs_symm_quad\n",
        "    })\n",
        "    # Add absolute value for sorting\n",
        "    coeffs_df_symm['AbsCoefficient'] = np.abs(coeffs_df_symm['Coefficient'])\n",
        "    print(coeffs_df_symm.sort_values('AbsCoefficient', ascending=False).to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "    # --- Compare R ---\n",
        "    if 'best_r2' in locals():\n",
        "        r2_drop = best_r2 - r2_symm_quad\n",
        "        print(f\"\\nComparison to Full Poly2 Fit R ({best_r2:.4f}):\")\n",
        "        print(f\"  R drop from enforcing symmetry: {r2_drop:.6f}\")\n",
        "    else:\n",
        "        print(\"\\nCannot compare R (previous best R value not found).\")\n"
      ],
      "metadata": {
        "id": "1an5fvIZadl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Summary: Reverse-Engineering Addition in a Small Transformer**\n",
        "\n",
        "**Goal:** Understand the algorithm learned by a simple 1-layer Transformer trained to compute `c = a + b`, where `a` and `b` are single digits (0-9) and the output `c` ranges from 0-18. This was initially conceived as a simplified Fibonacci step.\n",
        "\n",
        "**1. Input Representation (Embeddings `W_E`):**\n",
        "\n",
        "*   **Method:** Principal Component Analysis (PCA) on the learned token embeddings (0-9).\n",
        "*   **Findings:** The embeddings exist predominantly on a 2D manifold.\n",
        "    *   **PC1 (~73% variance):** Correlates linearly with the token value (`i`).\n",
        "    *   **PC2 (~25% variance, total ~99.4%):** Correlates quadratically/parabolically with the token value (`i^2`).\n",
        "    *   **Symmetry:** The embedding structure treats `a` and `b` sources symmetrically.\n",
        "*   **Frequency View:** Fourier analysis of `W_E` confirmed this, showing dominant power at frequency indices k=1 (corresponding to linear PC1) and k=2 (corresponding to quadratic PC2) across the input token dimension (0-9).\n",
        "*   **Interpretation:** The model *learns* to represent input numbers primarily using their linear value and a quadratic/boundary-sensitive feature.\n",
        "\n",
        "**2. The Necessity of PC2 (Ablation Studies):**\n",
        "\n",
        "*   **Method:** Ablated (removed) the PC2 component from the embeddings before feeding them into the model.\n",
        "*   **Findings:** Model accuracy plummeted from ~100% to ~42%. Failures were systematic:\n",
        "    *   Inputs involving 0 (e.g., `(0,x)`) caused over-prediction.\n",
        "    *   Inputs involving 9 (e.g., `(9,x)`) caused under-prediction.\n",
        "    *   Diagonal inputs `(x,x)` often failed.\n",
        "    *   Extreme asymmetry `(0,9)` and `(9,0)` were predicted *correctly*.\n",
        "    *   Ablating the MLP skip connection had negligible impact.\n",
        "*   **Interpretation:** PC2 is causally crucial for correct computation, specifically for handling boundary conditions. The linear PC1 feature alone is insufficient for the network (with ReLU and `W_L`) to generalize addition correctly.\n",
        "\n",
        "**3. MLP Computation (Pre-ReLU `z_mlp`):**\n",
        "\n",
        "*   **Method:** Fitted the pre-ReLU activations of the full model (`z_mlp_full`) to functions of the input PC features.\n",
        "*   **Findings:**\n",
        "    *   `z_mlp_full` is **extremely well-explained (mean R  0.998 for all 512 neurons)** by a **degree-2 polynomial** of the four input features: `PC1(a), PC1(b), PC2(a), PC2(b)`.\n",
        "    *   The fit using only *linear* terms of these features was significantly worse (mean R  0.75), confirming quadratic terms are essential pre-ReLU.\n",
        "    *   The learned polynomials exhibited strong **symmetry** between `a` and `b` features. A fit using only 5 symmetric quadratic features (like `PC1(a)+PC1(b)`, `(PC1(a)+PC1(b))^2`, etc.) still achieved high accuracy (mean R  0.98).\n",
        "    *   The *change* induced by PC2 (`delta_z_mlp = z_full - z_ablated`) was well-fit by a simple linear/quadratic function of `PC2(a), PC2(b)`.\n",
        "*   **Interpretation:** The MLP's core computation *before* the non-linearity is surprisingly simple and mathematically characterizable: it computes a specific, symmetric quadratic polynomial of the dominant input features (PC1, PC2) for each neuron.\n",
        "\n",
        "**4. MLP Activation & ReLU Impact:**\n",
        "\n",
        "*   **Method:** Applied ReLU (`a_mlp = max(0, z_mlp)`). Fitted `a_mlp` vs inputs. Compared pre/post ReLU linearity. Analyzed neuron state flips caused by PC2.\n",
        "*   **Findings:**\n",
        "    *   Fitting `a_mlp_full` with `Poly2(PCs)` still yields a good fit (mean R  0.975), but noticeably worse than the near-perfect pre-ReLU fit.\n",
        "    *   ReLU significantly reduces the direct linear correlation between activations and the sum `a+b` (both for the full and ablated models, with a similar R drop).\n",
        "    *   Analysis of neuron flips (e.g., for input (0,0)) showed PC2 causes specific neurons (those suppressing logit 0 and boosting logit 3) to turn OFF, directly implementing the needed correction via `W_L`.\n",
        "*   **Interpretation:** ReLU is the main source of complex non-linearity. PC2 corrects errors by strategically shifting `z_mlp` relative to the ReLU threshold, thereby controlling which neurons contribute to the final sum via `W_L`. Approximating `a_mlp` itself as `Poly2(PCs)` is functionally quite effective, although mechanistically less precise than `ReLU(Poly2(z_mlp))`.\n",
        "\n",
        "**5. Unembedding Readout (`W_L` & Logits):**\n",
        "\n",
        "*   **Method:** Fourier analysis of final logits and `W_L` columns. PCA on `W_L` columns. Simulation of final output.\n",
        "*   **Findings:**\n",
        "    *   Final logits (e.g., for inputs (0,0), (4,4)) are dominated by **low-frequency Fourier components** (indices 1, 2, 3 most prominent, up to ~6 relevant).\n",
        "    *   The columns of `W_L` are also dominated by these same low frequencies, but typically as **combinations/mixes**, not pure sinusoids.\n",
        "    *   PCA on `W_L` columns revealed an **ultra-low dimensional structure**: the top 2 PCs capture 98.3% of the variance and likely correspond to output frequency k=1 (sin/cos).\n",
        "    *   Simulating the final output using `Logits  W_L @ ReLU(Poly2(PCs))` achieved very high accuracy (Overall R  0.988), closely matching the real model. Simulating using `Logits  W_L @ Poly2(a_mlp)` was slightly less accurate (R  0.966).\n",
        "*   **Interpretation:** `W_L` acts as a readout mechanism structured around a low-dimensional, low-frequency Fourier basis (represented distributively across neurons). It decodes the post-ReLU activation pattern (`a_mlp`) to synthesize the final logit shape.\n",
        "\n",
        "**Synthesized Algorithm:**\n",
        "\n",
        "The network learns to compute `a+b` not directly, but by:\n",
        "1.  Encoding inputs `a, b` using features dominated by PC1 (linear value) and PC2 (quadratic value).\n",
        "2.  Computing pre-ReLU activations `z_mlp` via a highly accurate, symmetric **degree-2 polynomial function** of these PC features (`z_mlp  Poly2(PCs)`). PC2 terms are essential here.\n",
        "3.  Applying **ReLU** (`a_mlp = ReLU(z_mlp)`), which selects active neurons based on the polynomial result (PC2 critically influences this near the threshold for boundary inputs).\n",
        "4.  Performing a linear readout **`Logits = W_L @ a_mlp`**, where `W_L` uses a low-dimensional basis strongly related to **low output frequencies (k=1, 2, 3...)** to combine the activations and synthesize a logit vector peaking at the correct sum `k=a+b`.\n",
        "\n",
        "Functionally, the entire process is well-approximated by `Logit[k]  Poly2'_k(PCs)`, where `Poly2'_k` is a specific quadratic function learned for each output logit index `k`.\n",
        "\n",
        "**Significance:** This investigation fully characterized the learned algorithm, revealing why a quadratic computation (using PC2) emerged as necessary for this network to solve linear addition (boundary correction). It connected geometric (PCA), frequency (Fourier), and functional (polynomial fitting) perspectives, and pinpointed the role of each network component (Embedding, MLP pre/post ReLU, Unembedding).\n",
        "\n",
        "**Next Step: Actual Fibonacci Sequence Prediction**\n",
        "\n",
        "*   **Why Interesting:** Moves beyond simple addition to include sequence processing. The model must learn *which* inputs to add (`F(n-1)` and `F(n-2)`).\n",
        "*   **Questions:** How does the attention mechanism learn to find the correct previous terms? Does it use positional encodings heavily? Is the core addition circuit (MLP/W_L) the same as found here, or does it adapt? How does the network handle the initial terms of the sequence? Analyzing attention patterns and comparing the MLP/W_L structure to the current findings would be key.\n",
        "\n",
        "This provides a compelling narrative arc from a simple problem setup through detailed investigation to a nuanced algorithmic understanding and clear future directions."
      ],
      "metadata": {
        "id": "AevtIMtdfrZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While our analysis of the MLP pre-activations (z_mlp) revealed strong symmetry with respect to inputs a and b, we haven't explicitly verified the role of the attention layer preceding it. To confirm whether attention treats the a and b inputs symmetrically (e.g., by averaging their representations) or introduces asymmetry that the MLP later compensates for, we will analyze the attention patterns. Specifically, we will hook the attention probabilities and examine the attention paid by the query at the final sequence position (where the prediction is made) to the keys corresponding to input a (position 0) and input b (position 1). We will visualize these attention weights across all input pairs (a, b) for each head and calculate the average weights to quantify the attention distribution.\n"
      ],
      "metadata": {
        "id": "Z5bbq3mhVkAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_heads = 4\n",
        "\n",
        "# --- Prepare Inputs (All 100 pairs) ---\n",
        "a_inputs = np.arange(N)\n",
        "b_inputs = np.arange(N)\n",
        "input_pairs = np.array(np.meshgrid(a_inputs, b_inputs)).T.reshape(-1, 2) # Shape [100, 2]\n",
        "num_samples = input_pairs.shape[0]\n",
        "\n",
        "# Create input tokens: [a, b, =] for each pair\n",
        "test_input_tokens_all = torch.zeros((num_samples, 3), dtype=torch.long)\n",
        "for i in range(num_samples):\n",
        "    test_input_tokens_all[i, 0] = input_pairs[i, 0]\n",
        "    test_input_tokens_all[i, 1] = input_pairs[i, 1]\n",
        "    test_input_tokens_all[i, 2] = equals_token_id\n",
        "\n",
        "print(f\"Created input tensor shape: {test_input_tokens_all.shape}\")\n",
        "\n",
        "# --- Hook Setup ---\n",
        "# Hook point for attention probabilities/pattern AFTER softmax\n",
        "attn_pattern_hook_point = utils.get_act_name(\"pattern\", 0) # For layer 0\n",
        "print(f\"Using attention pattern hook point: {attn_pattern_hook_point}\")\n",
        "\n",
        "# Storage for attention patterns [batch, n_heads, query_pos, key_pos]\n",
        "captured_attn_patterns = None\n",
        "\n",
        "def store_attn_pattern_hook(attn_pattern, hook):\n",
        "    global captured_attn_patterns\n",
        "    # Store the whole pattern tensor, detach from graph\n",
        "    captured_attn_patterns = attn_pattern.detach().cpu()\n",
        "    return attn_pattern # Pass through\n",
        "\n",
        "# --- Run Model with Hook ---\n",
        "print(\"Running model to capture attention patterns...\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    try:\n",
        "        _ = model.run_with_hooks(\n",
        "            test_input_tokens_all,\n",
        "            fwd_hooks=[(attn_pattern_hook_point, store_attn_pattern_hook)]\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during model run with hook: {e}\")\n",
        "        print(f\"Please verify the hook point name '{attn_pattern_hook_point}' is correct for attention patterns.\")\n",
        "        captured_attn_patterns = None\n",
        "\n",
        "# --- Analyze Captured Patterns ---\n",
        "if captured_attn_patterns is not None:\n",
        "    print(f\"Captured attention pattern shape: {captured_attn_patterns.shape}\") # Should be [100, n_heads, 3, 3]\n",
        "\n",
        "    # Extract attention FROM final position (query_pos=2) TO positions 0 and 1\n",
        "    # Shape: [batch=100, n_heads]\n",
        "    attn_to_pos0_tensor = captured_attn_patterns[:, :, 2, 0]\n",
        "    attn_to_pos1_tensor = captured_attn_patterns[:, :, 2, 1]\n",
        "    attn_to_pos2_tensor = captured_attn_patterns[:, :, 2, 2] # Attention to '=' token itself\n",
        "\n",
        "    attn_to_pos0 = attn_to_pos0_tensor.numpy()\n",
        "    attn_to_pos1 = attn_to_pos1_tensor.numpy()\n",
        "    attn_to_pos2 = attn_to_pos2_tensor.numpy()\n",
        "\n",
        "\n",
        "    # Reshape for heatmaps (map batch index back to a, b grid)\n",
        "    # attn_heatmaps[head_idx, a, b] will store the attention value\n",
        "    attn_heatmaps_pos0 = np.zeros((n_heads, N, N))\n",
        "    attn_heatmaps_pos1 = np.zeros((n_heads, N, N))\n",
        "    attn_heatmaps_pos2 = np.zeros((n_heads, N, N)) # For checking attention to '='\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        a = input_pairs[i, 0]\n",
        "        b = input_pairs[i, 1]\n",
        "        for head_idx in range(n_heads):\n",
        "            attn_heatmaps_pos0[head_idx, a, b] = attn_to_pos0[i, head_idx]\n",
        "            attn_heatmaps_pos1[head_idx, a, b] = attn_to_pos1[i, head_idx]\n",
        "            attn_heatmaps_pos2[head_idx, a, b] = attn_to_pos2[i, head_idx]\n",
        "\n",
        "    # --- Calculate Averages ---\n",
        "    avg_attn_to_pos0 = np.mean(attn_to_pos0, axis=0) # Average over batch -> shape [n_heads,]\n",
        "    avg_attn_to_pos1 = np.mean(attn_to_pos1, axis=0)\n",
        "    avg_attn_to_pos2 = np.mean(attn_to_pos2, axis=0)\n",
        "\n",
        "    print(\"\\n--- Average Attention Weights (Query='=', Key=Position) ---\")\n",
        "    avg_df = pd.DataFrame({\n",
        "        'Head': np.arange(n_heads),\n",
        "        'Avg Attn to Pos 0 (a)': avg_attn_to_pos0,\n",
        "        'Avg Attn to Pos 1 (b)': avg_attn_to_pos1,\n",
        "        'Avg Attn to Pos 2 (=)': avg_attn_to_pos2,\n",
        "        'Sum Avg Attn': avg_attn_to_pos0 + avg_attn_to_pos1 + avg_attn_to_pos2\n",
        "    })\n",
        "    print(avg_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "    # --- Visualize Heatmaps ---\n",
        "    num_rows = (n_heads + 1) // 2 # Arrange plots in ~2 columns\n",
        "    fig, axes = plt.subplots(num_rows, 4, figsize=(18, 5 * num_rows), squeeze=False)\n",
        "    axes_flat = axes.flatten()\n",
        "\n",
        "    for head_idx in range(n_heads):\n",
        "        # Plot Attn(= -> a)\n",
        "        ax = axes_flat[head_idx*2]\n",
        "        sns.heatmap(attn_heatmaps_pos0[head_idx], annot=False, fmt=\".2f\", cmap=\"viridis\", ax=ax,\n",
        "                    linewidths=.5, linecolor='lightgray', cbar=True, square=True, vmin=0, vmax=1)\n",
        "        ax.set_title(f\"Head {head_idx}: Attn Query='=' -> Key='a' (Pos 0)\")\n",
        "        ax.set_xlabel(\"Input b\")\n",
        "        ax.set_ylabel(\"Input a\")\n",
        "        ax.invert_yaxis()\n",
        "\n",
        "        # Plot Attn(= -> b)\n",
        "        ax = axes_flat[head_idx*2 + 1]\n",
        "        sns.heatmap(attn_heatmaps_pos1[head_idx], annot=False, fmt=\".2f\", cmap=\"viridis\", ax=ax,\n",
        "                    linewidths=.5, linecolor='lightgray', cbar=True, square=True, vmin=0, vmax=1)\n",
        "        ax.set_title(f\"Head {head_idx}: Attn Query='=' -> Key='b' (Pos 1)\")\n",
        "        ax.set_xlabel(\"Input b\")\n",
        "        ax.set_ylabel(\"Input a\")\n",
        "        ax.invert_yaxis()\n",
        "\n",
        "    # Hide any unused axes\n",
        "    for i in range(n_heads * 2, len(axes_flat)):\n",
        "         axes_flat[i].set_visible(False)\n",
        "\n",
        "    fig.suptitle(\"Attention Patterns from Final Position ('=') Query\", fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nAttention pattern analysis skipped due to error during model run.\")\n"
      ],
      "metadata": {
        "id": "1MyQ4F9EVtN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All heads pay roughly the same amount of attention (what does this actually mean? what's the metric) to 'a' and 'b', differing only in the tenths of percent. This was expected, as we found the transformer treats a and b very symmetrically.\n",
        "\n",
        "We find that two heads effectively ignore the '=', with less than 1% attention paid to it. The third one has roughly 2%, and the fourth one ~12%. Further, the plots seem to indicate that the attention varies per input token combination, and it vasries with a different pattern for each of the heads. There are however clear patterns for all of them. Can you explain the plots? I am not sure what I am looking at."
      ],
      "metadata": {
        "id": "hPsLsBSpWlfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gEgi2CCPWgNa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H5rXGKys0GQ"
      },
      "source": [
        "### Looking at Activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbHIVRkas0GQ"
      },
      "source": [
        "Helper variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNdBs3C1s0GQ"
      },
      "outputs": [],
      "source": [
        "pattern_a = cache[\"pattern\", 0, \"attn\"][:, :, -1, 0]\n",
        "pattern_b = cache[\"pattern\", 0, \"attn\"][:, :, -1, 1]\n",
        "neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
        "neuron_pre_acts = cache[\"pre\", 0, \"mlp\"][:, -1, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91s0skkZs0GQ"
      },
      "source": [
        "Get all shapes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdqUdekLs0GQ"
      },
      "outputs": [],
      "source": [
        "for param_name, param in cache.items():\n",
        "    print(param_name, param.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKKPmmvRs0GR"
      },
      "outputs": [],
      "source": [
        "imshow(cache[\"pattern\", 0].mean(dim=0)[:, -1, :], title=\"Average Attention Pattern per Head\", xaxis=\"Source\", yaxis=\"Head\", x=['a', 'b', '='])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePEWnaCts0GR"
      },
      "outputs": [],
      "source": [
        "imshow(cache[\"pattern\", 0][5][:, -1, :], title=\"Average Attention Pattern per Head\", xaxis=\"Source\", yaxis=\"Head\", x=['a', 'b', '='])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XXJM97Fs0GS"
      },
      "outputs": [],
      "source": [
        "dataset[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-xn4mH8s0GS"
      },
      "outputs": [],
      "source": [
        "imshow(cache[\"pattern\", 0][:, 0, -1, 0].reshape(p, p), title=\"Attention for Head 0 from a -> =\", xaxis=\"b\", yaxis=\"a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBrWJ8vLs0GS"
      },
      "outputs": [],
      "source": [
        "imshow(\n",
        "    einops.rearrange(cache[\"pattern\", 0][:, :, -1, 0], \"(a b) head -> head a b\", a=p, b=p),\n",
        "    title=\"Attention for Head 0 from a -> =\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWLpHkZFs0GS"
      },
      "source": [
        "Plotting neuron activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0drBmrps0GS"
      },
      "outputs": [],
      "source": [
        "cache[\"post\", 0, \"mlp\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgQak5VHs0GS"
      },
      "outputs": [],
      "source": [
        "imshow(\n",
        "    einops.rearrange(neuron_acts[:, :5], \"(a b) neuron -> neuron a b\", a=p, b=p),\n",
        "    title=\"First 5 neuron acts\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HnVxne3s0GS"
      },
      "source": [
        "### Singular Value Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U16Z4TJSs0GS"
      },
      "outputs": [],
      "source": [
        "W_E.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiC6n2Fes0GS"
      },
      "outputs": [],
      "source": [
        "U, S, Vh = torch.svd(W_E)\n",
        "line(S, title=\"Singular Values\")\n",
        "imshow(U, title=\"Principal Components on the Input\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvVbeQwRs0GT"
      },
      "outputs": [],
      "source": [
        "# Control - random Gaussian matrix\n",
        "U, S, Vh = torch.svd(torch.randn_like(W_E))\n",
        "line(S, title=\"Singular Values Random\")\n",
        "imshow(U, title=\"Principal Components Random\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSyCHC1Fs0GT"
      },
      "source": [
        "## Explaining Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2hCWMaPs0GT"
      },
      "source": [
        "### Analyse the Embedding - It's a Lookup Table!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVqefI_Ns0GT"
      },
      "outputs": [],
      "source": [
        "U, S, Vh = torch.svd(W_E)\n",
        "line(U[:, :8].T, title=\"Principal Components of the embedding\", xaxis=\"Input Vocabulary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAczKOwOs0GT"
      },
      "outputs": [],
      "source": [
        "fourier_basis = []\n",
        "fourier_basis_names = []\n",
        "fourier_basis.append(torch.ones(p))\n",
        "fourier_basis_names.append(\"Constant\")\n",
        "for freq in range(1, p//2+1):\n",
        "    fourier_basis.append(torch.sin(torch.arange(p)*2 * torch.pi * freq / p))\n",
        "    fourier_basis_names.append(f\"Sin {freq}\")\n",
        "    fourier_basis.append(torch.cos(torch.arange(p)*2 * torch.pi * freq / p))\n",
        "    fourier_basis_names.append(f\"Cos {freq}\")\n",
        "fourier_basis = torch.stack(fourier_basis, dim=0).to(device)\n",
        "fourier_basis = fourier_basis/fourier_basis.norm(dim=-1, keepdim=True)\n",
        "imshow(fourier_basis, xaxis=\"Input\", yaxis=\"Component\", y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTjqVcRfs0GT"
      },
      "outputs": [],
      "source": [
        "line(fourier_basis[:8], xaxis=\"Input\", line_labels=fourier_basis_names[:8], title=\"First 8 Fourier Components\")\n",
        "line(fourier_basis[25:29], xaxis=\"Input\", line_labels=fourier_basis_names[25:29], title=\"Middle Fourier Components\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG-59vX6s0GT"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ fourier_basis.T, title=\"All Fourier Vectors are Orthogonal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22IuwAJIs0GT"
      },
      "source": [
        "### Analyse the Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFVp-W6rs0GT"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ W_E, yaxis=\"Fourier Component\", xaxis=\"Residual Stream\", y=fourier_basis_names, title=\"Embedding in Fourier Basis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmkPoFBis0GT"
      },
      "outputs": [],
      "source": [
        "line((fourier_basis @ W_E).norm(dim=-1), xaxis=\"Fourier Component\", x=fourier_basis_names, title=\"Norms of Embedding in Fourier Basis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAt0F9ers0GT"
      },
      "outputs": [],
      "source": [
        "key_freqs = [17, 25, 32, 47]\n",
        "key_freq_indices = [33, 34, 49, 50, 63, 64, 93, 94]\n",
        "fourier_embed = fourier_basis @ W_E\n",
        "key_fourier_embed = fourier_embed[key_freq_indices]\n",
        "print(\"key_fourier_embed\", key_fourier_embed.shape)\n",
        "imshow(key_fourier_embed @ key_fourier_embed.T, title=\"Dot Product of embedding of key Fourier Terms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlzF0ohGs0GT"
      },
      "source": [
        "### Key Frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj-YO2jas0GT"
      },
      "outputs": [],
      "source": [
        "line(fourier_basis[[34, 50, 64, 94]], title=\"Cos of key freqs\", line_labels=[34, 50, 64, 94])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-rRcCUBs0GT"
      },
      "outputs": [],
      "source": [
        "line(fourier_basis[[34, 50, 64, 94]].mean(0), title=\"Constructive Interference\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxZIrLDXs0GT"
      },
      "source": [
        "## Analyse Neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBPiUgyis0GT"
      },
      "outputs": [],
      "source": [
        "imshow(\n",
        "    einops.rearrange(neuron_acts[:, :5], \"(a b) neuron -> neuron a b\", a=p, b=p),\n",
        "    title=\"First 5 neuron acts\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUsIbU1os0GT"
      },
      "outputs": [],
      "source": [
        "imshow(\n",
        "    einops.rearrange(neuron_acts[:, 0], \"(a b) -> a b\", a=p, b=p),\n",
        "    title=\"First neuron act\", xaxis=\"b\", yaxis=\"a\",)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4vEkIPZs0GT"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis[94][None, :] * fourier_basis[94][:, None], title=\"Cos 47a * cos 47b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5ar09d0s0GU"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis[94][None, :] * fourier_basis[0][:, None], title=\"Cos 47a * const\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eMysImAs0GU"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ neuron_acts[:, 0].reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of neuron 0\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EeLu17xs0GU"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ neuron_acts[:, 5].reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of neuron 5\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cmp_ADus0GU"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ torch.randn_like(neuron_acts[:, 0]).reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of RANDOM\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkEM5FuKs0GU"
      },
      "source": [
        "### Neuron Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKXWKzTvs0GU"
      },
      "outputs": [],
      "source": [
        "fourier_neuron_acts = fourier_basis @ einops.rearrange(neuron_acts, \"(a b) neuron -> neuron a b\", a=p, b=p) @ fourier_basis.T\n",
        "# Center these by removing the mean - doesn't matter!\n",
        "fourier_neuron_acts[:, 0, 0] = 0.\n",
        "print(\"fourier_neuron_acts\", fourier_neuron_acts.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7bVd4uHs0GU"
      },
      "outputs": [],
      "source": [
        "neuron_freq_norm = torch.zeros(p//2, model.cfg.d_mlp).to(device)\n",
        "for freq in range(0, p//2):\n",
        "    for x in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
        "        for y in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
        "            neuron_freq_norm[freq] += fourier_neuron_acts[:, x, y]**2\n",
        "neuron_freq_norm = neuron_freq_norm / fourier_neuron_acts.pow(2).sum(dim=[-1, -2])[None, :]\n",
        "imshow(neuron_freq_norm, xaxis=\"Neuron\", yaxis=\"Freq\", y=torch.arange(1, p//2+1), title=\"Neuron Frac Explained by Freq\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgFc7jTes0GU"
      },
      "outputs": [],
      "source": [
        "line(neuron_freq_norm.max(dim=0).values.sort().values, xaxis=\"Neuron\", title=\"Max Neuron Frac Explained over Freqs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "604eOgv1s0GV"
      },
      "source": [
        "## Read Off the Neuron-Logit Weights to Interpret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-YB2lybs0GV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQUj_Oqts0GV"
      },
      "outputs": [],
      "source": [
        "W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
        "print(\"W_logit\", W_logit.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JeTG5uSs0GV"
      },
      "outputs": [],
      "source": [
        "line((W_logit @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title=\"W_logit in the Fourier Basis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YdSwqDws0GV"
      },
      "outputs": [],
      "source": [
        "neurons_17 = neuron_freq_norm[17-1]>0.85\n",
        "neurons_17.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK18zt9Ys0GV"
      },
      "outputs": [],
      "source": [
        "neurons_17.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmg_8lWMs0GV"
      },
      "outputs": [],
      "source": [
        "line((W_logit[neurons_17] @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title=\"W_logit for freq 17 neurons in the Fourier Basis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyjOvArYs0GV"
      },
      "source": [
        "Study sin 17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_uaGUnVs0GV"
      },
      "outputs": [],
      "source": [
        "freq = 17\n",
        "W_logit_fourier = W_logit @ fourier_basis\n",
        "neurons_sin_17 = W_logit_fourier[:, 2*freq-1]\n",
        "line(neurons_sin_17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FO9TUuNs0GV"
      },
      "outputs": [],
      "source": [
        "neuron_acts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyaLVHtrs0GV"
      },
      "outputs": [],
      "source": [
        "inputs_sin_17c = neuron_acts @ neurons_sin_17\n",
        "imshow(fourier_basis @ inputs_sin_17c.reshape(p, p) @ fourier_basis.T, title=\"Fourier Heatmap over inputs for sin17c\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv0OEDwms0GW"
      },
      "source": [
        "# Black Box Methods + Progress Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ydLen4As0GW"
      },
      "source": [
        "## Setup Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_Rjpunas0GW"
      },
      "source": [
        "Code to plot embedding freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naOtz04Ys0GW"
      },
      "outputs": [],
      "source": [
        "def embed_to_cos_sin(fourier_embed):\n",
        "    if len(fourier_embed.shape) == 1:\n",
        "        return torch.stack([fourier_embed[1::2], fourier_embed[2::2]])\n",
        "    else:\n",
        "        return torch.stack([fourier_embed[:, 1::2], fourier_embed[:, 2::2]], dim=1)\n",
        "\n",
        "from neel_plotly.plot import melt\n",
        "\n",
        "def plot_embed_bars(\n",
        "    fourier_embed,\n",
        "    title=\"Norm of embedding of each Fourier Component\",\n",
        "    return_fig=False,\n",
        "    **kwargs\n",
        "):\n",
        "    cos_sin_embed = embed_to_cos_sin(fourier_embed)\n",
        "    df = melt(cos_sin_embed)\n",
        "    # display(df)\n",
        "    group_labels = {0: \"sin\", 1: \"cos\"}\n",
        "    df[\"Trig\"] = df[\"0\"].map(lambda x: group_labels[x])\n",
        "    fig = px.bar(\n",
        "        df,\n",
        "        barmode=\"group\",\n",
        "        color=\"Trig\",\n",
        "        x=\"1\",\n",
        "        y=\"value\",\n",
        "        labels={\"1\": \"$w_k$\", \"value\": \"Norm\"},\n",
        "        title=title,\n",
        "        **kwargs\n",
        "    )\n",
        "    fig.update_layout(dict(legend_title=\"\"))\n",
        "\n",
        "    if return_fig:\n",
        "        return fig\n",
        "    else:\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evAuHNhvs0GW"
      },
      "source": [
        "Code to test a tensor of edited logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP4qn-ags0GW"
      },
      "outputs": [],
      "source": [
        "def test_logits(logits, bias_correction=False, original_logits=None, mode=\"all\"):\n",
        "    # Calculates cross entropy loss of logits representing a batch of all p^2\n",
        "    # possible inputs\n",
        "    # Batch dimension is assumed to be first\n",
        "    if logits.shape[1] == p * p:\n",
        "        logits = logits.T\n",
        "    if logits.shape == torch.Size([p * p, p + 1]):\n",
        "        logits = logits[:, :-1]\n",
        "    logits = logits.reshape(p * p, p)\n",
        "    if bias_correction:\n",
        "        # Applies bias correction - we correct for any missing bias terms,\n",
        "        # independent of the input, by centering the new logits along the batch\n",
        "        # dimension, and then adding the average original logits across all inputs\n",
        "        logits = (\n",
        "            einops.reduce(original_logits - logits, \"batch ... -> ...\", \"mean\") + logits\n",
        "        )\n",
        "    if mode == \"train\":\n",
        "        return loss_fn(logits[train_indices], labels[train_indices])\n",
        "    elif mode == \"test\":\n",
        "        return loss_fn(logits[test_indices], labels[test_indices])\n",
        "    elif mode == \"all\":\n",
        "        return loss_fn(logits, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2pGSGLZs0GW"
      },
      "source": [
        "Code to run a metric over every checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6B559Xvs0GW"
      },
      "outputs": [],
      "source": [
        "metric_cache = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkX1wnA7s0GW"
      },
      "outputs": [],
      "source": [
        "def get_metrics(model, metric_cache, metric_fn, name, reset=False):\n",
        "    if reset or (name not in metric_cache) or (len(metric_cache[name]) == 0):\n",
        "        metric_cache[name] = []\n",
        "        for c, sd in enumerate(tqdm.tqdm((model_checkpoints))):\n",
        "            model.reset_hooks()\n",
        "            model.load_state_dict(sd)\n",
        "            out = metric_fn(model)\n",
        "            if type(out) == torch.Tensor:\n",
        "                out = utils.to_numpy(out)\n",
        "            metric_cache[name].append(out)\n",
        "        model.load_state_dict(model_checkpoints[-1])\n",
        "        try:\n",
        "            metric_cache[name] = torch.tensor(metric_cache[name])\n",
        "        except:\n",
        "            metric_cache[name] = torch.tensor(np.array(metric_cache[name]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP7HYsw4s0GW"
      },
      "source": [
        "## Defining Progress Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-X6SmHLs0GX"
      },
      "source": [
        "### Loss Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dk8WFsss0GX"
      },
      "outputs": [],
      "source": [
        "memorization_end_epoch = 1500\n",
        "circuit_formation_end_epoch = 13300\n",
        "cleanup_end_epoch = 16600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkOXufDOs0GX"
      },
      "outputs": [],
      "source": [
        "def add_lines(figure):\n",
        "    figure.add_vline(memorization_end_epoch, line_dash=\"dash\", opacity=0.7)\n",
        "    figure.add_vline(circuit_formation_end_epoch, line_dash=\"dash\", opacity=0.7)\n",
        "    figure.add_vline(cleanup_end_epoch, line_dash=\"dash\", opacity=0.7)\n",
        "    return figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBp4jo-ms0GX"
      },
      "outputs": [],
      "source": [
        "fig = line([train_losses[::100], test_losses[::100]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Training Curve for Modular Addition\", line_labels=['train', 'test'], toggle_x=True, toggle_y=True, return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7oWhlUFs0GX"
      },
      "source": [
        "### Logit Periodicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRdbxqLxs0GX"
      },
      "outputs": [],
      "source": [
        "all_logits = original_logits[:, -1, :]\n",
        "print(all_logits.shape)\n",
        "all_logits = einops.rearrange(all_logits, \"(a b) c -> a b c\", a=p, b=p)\n",
        "print(all_logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI27BHros0GX"
      },
      "outputs": [],
      "source": [
        "coses = {}\n",
        "for freq in key_freqs:\n",
        "    print(\"Freq:\", freq)\n",
        "    a = torch.arange(p)[:, None, None]\n",
        "    b = torch.arange(p)[None, :, None]\n",
        "    c = torch.arange(p)[None, None, :]\n",
        "    cube_predicted_logits = torch.cos(freq * 2 * torch.pi / p * (a + b - c)).to(device)\n",
        "    cube_predicted_logits /= cube_predicted_logits.norm()\n",
        "    coses[freq] = cube_predicted_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XCK0pKWs0GX"
      },
      "outputs": [],
      "source": [
        "approximated_logits = torch.zeros_like(all_logits)\n",
        "for freq in key_freqs:\n",
        "    print(\"Freq:\", freq)\n",
        "    coeff = (all_logits * coses[freq]).sum()\n",
        "    print(\"Coeff:\", coeff)\n",
        "    cosine_sim = coeff / all_logits.norm()\n",
        "    print(\"Cosine Sim:\", cosine_sim)\n",
        "    approximated_logits += coeff * coses[freq]\n",
        "residual = all_logits - approximated_logits\n",
        "print(\"Residual size:\", residual.norm())\n",
        "print(\"Residual fraction of norm:\", residual.norm()/all_logits.norm())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1LJbXAJs0GX"
      },
      "outputs": [],
      "source": [
        "random_logit_cube = torch.randn_like(all_logits)\n",
        "print((all_logits * random_logit_cube).sum()/random_logit_cube.norm()/all_logits.norm())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2mNngkks0GX"
      },
      "outputs": [],
      "source": [
        "test_logits(all_logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgJrRka_s0GX"
      },
      "outputs": [],
      "source": [
        "test_logits(approximated_logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWPzR90Us0GX"
      },
      "source": [
        "#### Look During Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrldGN_0s0GX"
      },
      "outputs": [],
      "source": [
        "cos_cube = []\n",
        "for freq in range(1, p//2 + 1):\n",
        "    a = torch.arange(p)[:, None, None]\n",
        "    b = torch.arange(p)[None, :, None]\n",
        "    c = torch.arange(p)[None, None, :]\n",
        "    cube_predicted_logits = torch.cos(freq * 2 * torch.pi / p * (a + b - c)).to(device)\n",
        "    cube_predicted_logits /= cube_predicted_logits.norm()\n",
        "    cos_cube.append(cube_predicted_logits)\n",
        "cos_cube = torch.stack(cos_cube, dim=0)\n",
        "print(cos_cube.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAh-wilSs0GX"
      },
      "outputs": [],
      "source": [
        "def get_cos_coeffs(model):\n",
        "    logits = model(dataset)[:, -1]\n",
        "    logits = einops.rearrange(logits, \"(a b) c -> a b c\", a=p, b=p)\n",
        "    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])\n",
        "    return vals\n",
        "\n",
        "\n",
        "get_metrics(model, metric_cache, get_cos_coeffs, \"cos_coeffs\")\n",
        "print(metric_cache[\"cos_coeffs\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY_4kiNks0GY"
      },
      "outputs": [],
      "source": [
        "fig = line(metric_cache[\"cos_coeffs\"].T, line_labels=[f\"Freq {i}\" for i in range(1, p//2+1)], title=\"Coefficients with Predicted Logits\", xaxis=\"Epoch\", x=checkpoint_epochs, yaxis=\"Coefficient\", return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeI-gBNbs0GY"
      },
      "outputs": [],
      "source": [
        "def get_cos_sim(model):\n",
        "    logits = model(dataset)[:, -1]\n",
        "    logits = einops.rearrange(logits, \"(a b) c -> a b c\", a=p, b=p)\n",
        "    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])\n",
        "    return vals / logits.norm()\n",
        "\n",
        "get_metrics(model, metric_cache, get_cos_sim, \"cos_sim\") # You may need a big GPU. If you don't have one and can't work around this, raise an issue for help!\n",
        "print(metric_cache[\"cos_sim\"].shape)\n",
        "\n",
        "fig = line(metric_cache[\"cos_sim\"].T, line_labels=[f\"Freq {i}\" for i in range(1, p//2+1)], title=\"Cosine Sim with Predicted Logits\", xaxis=\"Epoch\", x=checkpoint_epochs, yaxis=\"Cosine Sim\", return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG0nbFRjs0GY"
      },
      "outputs": [],
      "source": [
        "def get_residual_cos_sim(model):\n",
        "    logits = model(dataset)[:, -1]\n",
        "    logits = einops.rearrange(logits, \"(a b) c -> a b c\", a=p, b=p)\n",
        "    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])\n",
        "    residual = logits - (vals[:, None, None, None] * cos_cube).sum(dim=0)\n",
        "    return residual.norm() / logits.norm()\n",
        "\n",
        "get_metrics(model, metric_cache, get_residual_cos_sim, \"residual_cos_sim\")\n",
        "print(metric_cache[\"residual_cos_sim\"].shape)\n",
        "\n",
        "fig = line([metric_cache[\"cos_sim\"][:, i] for i in range(p//2)]+[metric_cache[\"residual_cos_sim\"]], line_labels=[f\"Freq {i}\" for i in range(1, p//2+1)]+[\"residual\"], title=\"Cosine Sim with Predicted Logits + Residual\", xaxis=\"Epoch\", x=checkpoint_epochs, yaxis=\"Cosine Sim\", return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2K5cIU5s0GY"
      },
      "source": [
        "## Restricted Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvdkOwvis0GY"
      },
      "outputs": [],
      "source": [
        "neuron_acts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUW11uiis0GY"
      },
      "outputs": [],
      "source": [
        "neuron_acts_square = einops.rearrange(neuron_acts, \"(a b) neur -> a b neur\", a=p, b=p).clone()\n",
        "# Center it\n",
        "neuron_acts_square -= einops.reduce(neuron_acts_square, \"a b neur -> 1 1 neur\", \"mean\")\n",
        "neuron_acts_square_fourier = einsum(\"a b neur, fa a, fb b -> fa fb neur\", neuron_acts_square, fourier_basis, fourier_basis)\n",
        "imshow(neuron_acts_square_fourier.norm(dim=-1), xaxis=\"Fourier Component b\", yaxis=\"Fourier Component a\", title=\"Norms of neuron activations by Fourier Component\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHGB6i6Is0GY"
      },
      "outputs": [],
      "source": [
        "original_logits, cache = model.run_with_cache(dataset)\n",
        "print(original_logits.numel())\n",
        "neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rti1zUJ-s0GY"
      },
      "outputs": [],
      "source": [
        "approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
        "approx_neuron_acts += neuron_acts.mean(dim=0)\n",
        "a = torch.arange(p)[:, None]\n",
        "b = torch.arange(p)[None, :]\n",
        "for freq in key_freqs:\n",
        "    cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "    cos_apb_vec /= cos_apb_vec.norm()\n",
        "    cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
        "    approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
        "    sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "    sin_apb_vec /= sin_apb_vec.norm()\n",
        "    sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
        "    approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
        "restricted_logits = approx_neuron_acts @ W_logit\n",
        "print(loss_fn(restricted_logits[test_indices], test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYIXtp_Ys0GY"
      },
      "outputs": [],
      "source": [
        "print(loss_fn(all_logits, labels)) # This bugged on models not fully trained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv05m50Is0GY"
      },
      "source": [
        "### Look During Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IinjgaGWs0GY"
      },
      "outputs": [],
      "source": [
        "def get_restricted_loss(model):\n",
        "    logits, cache = model.run_with_cache(dataset)\n",
        "    logits = logits[:, -1, :]\n",
        "    neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
        "    approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
        "    approx_neuron_acts += neuron_acts.mean(dim=0)\n",
        "    a = torch.arange(p)[:, None]\n",
        "    b = torch.arange(p)[None, :]\n",
        "    for freq in key_freqs:\n",
        "        cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "        cos_apb_vec /= cos_apb_vec.norm()\n",
        "        cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
        "        approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
        "        sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "        sin_apb_vec /= sin_apb_vec.norm()\n",
        "        sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
        "        approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
        "    restricted_logits = approx_neuron_acts @ model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
        "    # Add bias term\n",
        "    restricted_logits += logits.mean(dim=0, keepdim=True) - restricted_logits.mean(dim=0, keepdim=True)\n",
        "    return loss_fn(restricted_logits[test_indices], test_labels)\n",
        "get_restricted_loss(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_qhNrM7s0GY"
      },
      "outputs": [],
      "source": [
        "get_metrics(model, metric_cache, get_restricted_loss, \"restricted_loss\", reset=True)\n",
        "print(metric_cache[\"restricted_loss\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T2erlNUs0GZ"
      },
      "outputs": [],
      "source": [
        "fig = line([train_losses[::100], test_losses[::100], metric_cache[\"restricted_loss\"]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Restricted Loss Curve\", line_labels=['train', 'test', \"restricted_loss\"], toggle_x=True, toggle_y=True, return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bewRWQ8s0GZ"
      },
      "outputs": [],
      "source": [
        "fig = line([torch.tensor(test_losses[::100])/metric_cache[\"restricted_loss\"]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Restricted Loss to Test Loss Ratio\", toggle_x=True, toggle_y=True, return_fig=True)\n",
        "# WARNING: bugged when cancelling training half way thr ough\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybIDyH4gs0GZ"
      },
      "source": [
        "## Excluded Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKd6GtA4s0GZ"
      },
      "outputs": [],
      "source": [
        "approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
        "# approx_neuron_acts += neuron_acts.mean(dim=0)\n",
        "a = torch.arange(p)[:, None]\n",
        "b = torch.arange(p)[None, :]\n",
        "for freq in key_freqs:\n",
        "    cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "    cos_apb_vec /= cos_apb_vec.norm()\n",
        "    cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
        "    approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
        "    sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "    sin_apb_vec /= sin_apb_vec.norm()\n",
        "    sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
        "    approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
        "excluded_neuron_acts = neuron_acts - approx_neuron_acts\n",
        "excluded_logits = excluded_neuron_acts @ W_logit\n",
        "print(loss_fn(excluded_logits[train_indices], train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uoji2678s0GZ"
      },
      "outputs": [],
      "source": [
        "def get_excluded_loss(model):\n",
        "    logits, cache = model.run_with_cache(dataset)\n",
        "    logits = logits[:, -1, :]\n",
        "    neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
        "    approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
        "    # approx_neuron_acts += neuron_acts.mean(dim=0)\n",
        "    a = torch.arange(p)[:, None]\n",
        "    b = torch.arange(p)[None, :]\n",
        "    for freq in key_freqs:\n",
        "        cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "        cos_apb_vec /= cos_apb_vec.norm()\n",
        "        cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
        "        approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
        "        sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "        sin_apb_vec /= sin_apb_vec.norm()\n",
        "        sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
        "        approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
        "    excluded_neuron_acts = neuron_acts - approx_neuron_acts\n",
        "    residual_stream_final = excluded_neuron_acts @ model.blocks[0].mlp.W_out + cache[\"resid_mid\", 0][:, -1, :]\n",
        "    excluded_logits = residual_stream_final @ model.unembed.W_U\n",
        "    return loss_fn(excluded_logits[train_indices], train_labels)\n",
        "get_excluded_loss(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHuD_-Fcs0GZ"
      },
      "outputs": [],
      "source": [
        "get_metrics(model, metric_cache, get_excluded_loss, \"excluded_loss\", reset=True)\n",
        "print(metric_cache[\"excluded_loss\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrzhqw-Xs0GZ"
      },
      "outputs": [],
      "source": [
        "fig = line([train_losses[::100], test_losses[::100], metric_cache[\"excluded_loss\"], metric_cache[\"restricted_loss\"]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Excluded and Restricted Loss Curve\", line_labels=['train', 'test', \"excluded_loss\", \"restricted_loss\"], toggle_x=True, toggle_y=True, return_fig=True)\n",
        "\n",
        "add_lines(fig)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8859a5491331dba93123a91c2831400aced845b502848170e05fcb48b2c144be"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}